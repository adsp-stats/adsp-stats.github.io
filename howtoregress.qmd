# Coding it up in R {#sec-howtoregress}

## How to specify a linear regression in R

Let's run multiple (>1 predictor) OLS regression in R, and learn how to read its summaries and use its output objects.

The `penguins` dataset describes measuresments of three species of Antarctic penguins, taken from three islands from 2007--2009. Let's see if we can predict body mass (in grams) from bill length and flipper length (in millimeters).

```{r}
peng.lm <- lm(body_mass ~ bill_len + flipper_len, data=penguins)
```

Already I need to stop and tell you about several important details in this simple line of code.

 * The **formula**: The syntax I supplied was `Y ~ X1 + X2`, which R understands as "$Y$ is a linear function of the predictors $X_1$ and $X_2$ *and also an intercept*.
 
   - I don't need to specify an intercept (that is, using the column of ones as a predictor), since it's the default. If I wish to make the intercept explicit, I would write `Y ~ 1 + X1 + X2` (the results would be identical.)
   
   - If I don't want an intercept, I could use the syntax `Y ~ 0 + X1 + X2`, or alternatively `Y ~ -1 + X1 + X2` (no mathematical difference). These formulae are not literal equations. They simply help R to understand what model to run. We will discuss the implications of a no-intercept model later.
   
   - Technically, R doesn't recognize the terms `body_mass`, `bill_len`, or `flipper_len`. These aren't defined objects that R is "aware of". By using the `data=` argument, we are telling R "it's okay that you don't recognize these names; you'll find objects with these names in this dataset."
   
   - We can also write the formula *without* using the `data=` argument, and simply provide the objects directly to R. This method works well when your predictors and response variables are in different data frames, or not in any data frame at all (*e.g.* stored as separate vectors): `lm(penguins$body_mass ~ penguins$bill_len + penguins$flipper_len)`

 * The **assignment**: Notice that I didn't ask to *see* the results of the `lm()` call, but instead I stored the results as an R object. This allows me to modify it later, and to retrieve objects like the fitted values or the residuals whenever I like.  Running R modesl *without* storing them into memory is very rare.
 
 * The **function call**: I used `lm()` here since I am running a *linear model*. In the future we will use functions like `glm()`, `glm.nb()`, or `zeroinfl()` to run non-linear models, and their syntax will be very similar.
 
## How to interpret the basic summary of a linear regression in R

The R function `summary()` will provide basic information about whatever you put inside of it: datasets, models, lists, etc. When we apply it to an `lm()` object, we see the following output:

```{r}
summary(peng.lm)
```

Let's learn how to read this from top to bottom:

 * **The call**: Repeats its instructions back to the user. Helps to remind you which model you're looking at, especially if your models are being auto-generated by a function.
 
 * **The residuals**: Not very useful. Can help you to notice a large outlier or a skewed distribution (*e.g.* if $|\mathrm{Q1}| \ll \mathrm{Q3}$), but those should be examined separately.
 
 * **The table of coefficients**: Each row of this table refers to a different $\hat{\beta}$ estimated by the model. `(Intercept)` refers to $\hat{\beta}_0$ and each slope after the intercept is labeled using the name of its corresponding predictor.
 
   The estimates themselves are in the first column, `Estimate`. These are our best guesses, not the truth (which is still unknown). The second column, `Std. Error`, provides standard errors for our estimates which can be used to conduct hypothesis tests and to construct confidence intervals. The third and fourth columns, `t value` and `Pr(>|t|)`, conduct a two-sided hypothesis test that the true value of the parameter is actually 0.
   
   If we can reject this null hypothesis, then we can say with some confidence that the predictor "belongs" in the model.^[In future sections we will add more nuance to this idea.] On the other hand, if we can't reject the null that $\beta_j = 0$, then the predictor has no place in our model.  After all, the following two equations are equivalent:
   
   $$\begin{aligned} Y &= \beta_0 + \beta_1 X_1 + 0 X_2 + \beta_3 X_3 + \varepsilon \\ Y &= \beta_0 + \beta_1 X_1 + \beta_3 X_3 + \varepsilon \end{aligned}$$
   
   Here we estimate that for every additional millimeter of bill length, penguins weigh an additional 6 grams, and that for every additional millimeter of flipper length, penguins weigh an additional 48 grams. Furthermore, a penguin with no bill or flippers would be expected to weigh -5736 grams.^[Later, we shall learn why these estimates are flawed and unrealistic.]
   
 * **The model fit statistics**: At the very bottom we learn more about how well the regression model describes the data. Importantly, we see that two rows of the original dataset were not used due to missing values in the response or the predictors --- regressions cannot use a row in which either $Y$ or any of the $X$ predictors are missing.
 
   We also see that R^2 is about 76%, meaning that 76% of the variation in penguin body mass can be explained by variation in penguin bill and flipper lengths. The remaining model fit metrics must wait until we learn a little more theory.
   
## How to retrieve and use regression model attributes in R

Sometimes all we need to move forward can be found in the model summary. More usually, we will want to *use* objects from the regression. These can be found in two places: *(i)* in the `lm()` object, and *(ii)* in the `summary()` object.^[As a general rule, any information that R prints in an output can be accessed through the code as well.]

Recall that we named the penguin regression `peng.lm`. This object now has many attributes we can select by name:

```{r}
names(peng.lm)
```

 * The **fitted values** (also called $\hat{\boldsymbol{y}}$ or the predictions) are stored as a vector in `fitted.values`:^[Notice that the fourth fitted value is labeled 5 --- a clear sign that the fourth row of the original data could not be used in the regression.]

```{r}
head(peng.lm$fitted.values)
```

 * The **residuals** (also called $\boldsymbol{e}$ or the errors) are stored as a vector in `residuals`.

```{r}
head(peng.lm$residuals)
```

 * The **estimated coefficients** (also called $\hat{\boldsymbol{\beta}}$ or the parameter estimates) are stored as a vector in `coefficients`.

```{r}
peng.lm$coefficients
```

 * If you need **more information about the coefficients**, you can retrieve the matrix of standard errors, *t*-statistics, and *p*-values from the summmary object as `coefficients`. Notice that the estimates are the same as the `lm()` coefficients, this method simply extracts more information.

```{r}
summary(peng.lm)$coefficients
summary(peng.lm)$coefficients[1,4] #p-value for intercept
```

 * The **estimated error variance**, $\hat{\sigma}^2$ can also be found in the summary information. In R, the value which is stored is simply $\hat{\sigma}$, the standard deviation, stored as `sigma`.

```{r}
summary(peng.lm)$sigma
```

 *  And the last item for now will the **R^2^ model metric**, which can be found in the summary information under `r.squared`:

```{r}
summary(peng.lm)$r.squared
```

## How to make predictions for new values using a regression object

The fitted values provide our model's guess as to where the training data "should" have been observed. However, we often want to learn where new $X$-values would be observed. As described in @sec-multiplereg, predictions can take two forms:

* Predictions for where *new values will be observed*, or

* Predictions for the *mean of the new observations*.

Both of these predictions share the same point estimates, they simply differ in their standard errors, *i.e.* in the width of their confidence intervals.

The point estimates for new observations can be computed automatically using the `predict()` function or manually from the estimated coefficients as $\mathbf{X}_{\textrm{new}} \hat{\boldsymbol{\beta}}$. Let's build five new penguins and find their predicted body mass both ways:

```{r}
newpengs <- data.frame(ones=c(1,1,1,1,1),
                       bill_len=c(35,40,45,50,55),
                       flipper_len=c(175,185,195,205,215))
predict(peng.lm,newdata=newpengs)
as.matrix(newpengs)%*%peng.lm$coefficients
```

Note a few details here: we had to change the data frame (which R regards as a type of list) into a matrix so that we could matrix multiply it by the vector of beta-hats using the `%*%` operator. We also built in a vector of ones into the new data --- totally unnecessary for the automatic calculation using `predict()`, but helpful for the manual solution. Finally, note that we had to build a data frame with the same names as the original dataset, so that the regression object `peng.lm` could know which vectors to use in which "roles".

The wider confidence intervals for the possible location of new values can be found as follows:

```{r}
predict.lm(peng.lm,newdata=newpengs,level=0.9,interval='prediction')
```

And the narrower confidence intervals for the location of the *mean* for new values can be found with this change:

```{r}
predict.lm(peng.lm,newdata=newpengs,level=0.9,interval='confidence')
```

## Recovering ANOVA information from a regression using R

Once we fit a regression in R, it's easy to decompose the total variance of $\boldsymbol{y}$ into *(i)* variance explained by the model and *(ii)* the residual variance. That information comes to us from the `anova()` command:

```{r}
anova(peng.lm)
```

Let's review this table piece by piece:

* The first two rows describe variance explained by each individual predictor, and the last row marked `Residuals` describes the residual variance.

* The ANOVA tables in R are *order-dependent* -- the variance explained by each individual predictor is only the *additional* variance explained by adding that predictor to a model with all previously-listed variables in the model.^[Or, for the first row, a model with no other predictors.] ANOVA tables from other software tools may work differently.

* For each predictor, the first column describes the model degrees of freedom used up by that predictor.^[For our simple linear predictors this will always be 1; later, we shall consider categorical predictors which use more degrees of freedom.] In the last `Residuals` row, the first column lists the total residual degrees of freedom $n - k - 1$.

* The second column lists the sum of squares contribution from each predictor toward the SSM, as well as the residual sum of squares/sum of squared errors (SSE).

* The remaining columns are used to conduct an $F$ test which we will study in later sections.

We can access all of this information from the `anova()` object, and we can even compare it to our manual calculations of SST, SSM, and SSE. Starting with the model sum of squares, SSM, notice that the two contributions from `bill_len` and `flipper_len` total about 166 million g^2^. We can recreate total directly from the fitted values:

```{r}
anova(peng.lm)[1,2] + anova(peng.lm)[2,2]
(SSM <- sum((peng.lm$fitted.values-mean(penguins$body_mass,na.rm=TRUE))^2))
```

The SSE or sum of squared errors can be found using the bottom row of the table:

```{r}
anova(peng.lm)[3,2]
(SSE <- sum(peng.lm$residuals^2,na.rm=TRUE))
```

And the total sum of squares can be found either by totaling the `Sum Sq` column of the ANOVA table, or directly from $\boldsymbol{y}$, or by adding SSM and SSE:

```{r}
sum(anova(peng.lm)[,2])
(SST <- sum((penguins$body_mass-mean(penguins$body_mass,na.rm=TRUE))^2,na.rm=TRUE))
SSM + SSE
```