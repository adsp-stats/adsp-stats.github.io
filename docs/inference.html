<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Parametric inference – Statistical Analysis Using R: Online Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./approximations.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-cb117224d55f38be902120787fa86d61.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-749060a0f50e976d42fc24c7262bd810.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-df3f6df530e54c2c2a63ea4d10d2e4ce.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-e37fb8ce781c647c1a6346529af60874.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/quarto-contrib/bookup_fonts_gwf-0.0/fonts-embed.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./inference.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Parametric inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Statistical Analysis Using R: Online Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inference.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Parametric inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./approximations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Useful approximations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./olsregression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">OLS regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./robustness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression robustness</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./features.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feature engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modelselection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Model selection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Generalized linear models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binomialregression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Binomial regressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./countregression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Count regressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multinomialregression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Multinomial regressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Distributions</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_bernoulli.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bernoulli distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_binomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Binomial distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_chisquared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chi-squared distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_exponential.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exponential distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_f.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">F distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_geometric.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Geometric distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_negativebinomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Negative binomial distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_normal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Normal distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Poisson distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_t.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">t distribution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dist_uniform.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Uniform distribution</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proofs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Proofs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./listofsymbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">List of Symbols</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-inferential" id="toc-sec-inferential" class="nav-link active" data-scroll-target="#sec-inferential"><span class="header-section-number">1.1</span> Inferential statistics</a></li>
  <li><a href="#sec-parameters" id="toc-sec-parameters" class="nav-link" data-scroll-target="#sec-parameters"><span class="header-section-number">1.2</span> How all parametric estimation is performed</a></li>
  <li><a href="#sec-statsvsml" id="toc-sec-statsvsml" class="nav-link" data-scroll-target="#sec-statsvsml"><span class="header-section-number">1.3</span> Statistical models and machine learning models</a></li>
  <li><a href="#sec-mle" id="toc-sec-mle" class="nav-link" data-scroll-target="#sec-mle"><span class="header-section-number">1.4</span> Maximum likelihood estimation</a>
  <ul class="collapse">
  <li><a href="#motivation-and-definition" id="toc-motivation-and-definition" class="nav-link" data-scroll-target="#motivation-and-definition"><span class="header-section-number">1.4.1</span> Motivation and definition</a></li>
  <li><a href="#example-1-coin-flips" id="toc-example-1-coin-flips" class="nav-link" data-scroll-target="#example-1-coin-flips"><span class="header-section-number">1.4.2</span> Example 1: Coin flips</a></li>
  <li><a href="#example-2-heights" id="toc-example-2-heights" class="nav-link" data-scroll-target="#example-2-heights"><span class="header-section-number">1.4.3</span> Example 2: Heights</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-parametricinference" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Parametric inference</span></span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p>All of the methods and theory presented in these notes are examples of parametric inference. If you aren’t already familiar with that term, it’s worth exploring now.</p>
<section id="sec-inferential" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sec-inferential"><span class="header-section-number">1.1</span> Inferential statistics</h2>
<p>When I was young, I first heard the term “statistics” used to summarize or highlight the values of a dataset:</p>
<ul>
<li><p>Among Major League Baseball pitchers, Nolan Ryan holds the career strikeout record with a total of 5714.</p></li>
<li><p>Japan has a greater proportion of centenarians (people who are over 100 years of age) than any other country, at 43 per 100,000 residents.</p></li>
<li><p>Roughly 24% of the bullet chess players on [www.chess.com] have an Elo score better than my own.</p></li>
</ul>
<p>These figures are correctly called statistics, but statisticians label them as <strong>descriptive statistics</strong>. They are incontrovertible facts. They are properties of a fixed sample which we can all agree upon. They do not require theory, just a calculation. Once I explain to you how to compute a median, you know that the median of the numbers {1,1,2,3,5,8,13} is 3, without having to argue over assumptions.</p>
<p>By contrast, the field of <strong>inferential statistics</strong> concerns itself with a shadowy world we cannot see and which might not exist. Inferential statistics starts from the assumption that the numbers in our world come to us from unknown, but systematic and guessable generating processes. The data we observe are realizations of random variables, and the random variables are defined by formulae, and the formulae are controlled by parameters. If we can correctly guess the general form of the random variables which created a given dataset, then the same dataset can help us to guess the specific parameters for each random variable. Once we know those parameters, we can usually answer more detailed questions about the generating process, even questions for which we have no direct observational evidence.</p>
<p>For example, our study of the Earth and the broader solar system suggests that meteors with a diameter of more than 1 km strike the Earth roughly every 500,000 years. We believe that such meteor strikes are closely approximated by a Poisson process, and that therefore the time between large meteor strikes could be represented by an exponential distribution with the parameter <span class="math inline">\(\lambda=0.000002\)</span> (here <span class="math inline">\(\lambda\)</span>, or lambda, represents the long-term rate of meteor strikes per year).</p>
<p>If all this is true, then I can use the exponential distribution to calculate that the probability of a large meteor strike in my lifetime (or equivalently, in the next 50 years) is only about 0.01%:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span class="math display">\[P(X \le 50) = 1 - e^{-\lambda \cdot 50} = 1 - e^{-0.0001} \approx 0.00009995\]</span></p>
<p>Of course, I cannot prove my calculation is right or wrong. If the next large meteor struck tomorrow, or just as I died, or two thousand years from now, none of these would validate or invalidate my estimate. But the estimate might still be flawed for several reasons. Further research might change our estimate of how often such meteors strike the Earth — perhaps it’s roughly once every 400,000 years, or every 600,000 years. We may even be wrong about meteor strikes being Poisson processes, and if they are not, then I would need a completely new set of assumptions.</p>
<p>Yet consider my approach:</p>
<ul>
<li><p>I started from a dataset (the geologic record of how many large meteors have struck the Earth during the Cenozoic period)</p></li>
<li><p>I assumed that the data came from a specific probability distribution</p></li>
<li><p>I used the data to make a guess as to which parameters controlled that distribution</p></li>
<li><p>With my guess for the parameters, I was able to answer questions that the data by itself could not answer</p></li>
</ul>
<p>These are the central concepts of inferential statistics. We make assumptions about how the world works, and then use data to estimate various unknown parameters. We are always wrong in our guesses –– and we don’t even know how wrong we are. We are sometimes even wrong about our distributional assumptions. I wouldn’t say we are taking guesses in the dark, but the room can be very dim indeed. However, our reward is to be able to describe things we have not seen, to predict the future, and to better understand the past.</p>
</section>
<section id="sec-parameters" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="sec-parameters"><span class="header-section-number">1.2</span> How all parametric estimation is performed</h2>
<p>I described the steps above in generalities. We have not yet discussed <em>how</em> to make a guess about the parameters of a distribution. Nor have we discussed what makes one guessing method better or worse than other guessing methods. Before I can talk about these subjects, we need to introduce a few definitions.</p>
<p>We will start with some data. Perhaps we have a univariate vector of <span class="math inline">\(n\)</span> observations: <span class="math inline">\(\boldsymbol{x} = \{x_1,x_2,\ldots,x_n\}\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> I will frequently describe <span class="math inline">\(\boldsymbol{x}\)</span> as our <strong>sample</strong>, even if it was collected by non-sampling methods.</p>
<p>Next, let’s introduce a distributional assumption. The data <span class="math inline">\(x\)</span> are realizations of a random variable <span class="math inline">\(X\)</span> with a cumulative distribution function <span class="math inline">\(F_X\)</span> controlled by one or more parameters <span class="math inline">\(\theta\)</span> (theta). The probability or density of each observation <span class="math inline">\(x_i\)</span> depends not only on its value but also on the parameters <span class="math inline">\(\theta\)</span>. We could write,</p>
<p><span class="math display">\[P(X \le x_i) = F_X(x_i;\theta) \qquad \forall i \in \mathbb{N}:\ 1 \le i \le n\]</span></p>
<p>Statistics textbooks sometimes refer to <span class="math inline">\(\theta\)</span> as the <em>estimand</em>. In my experience, very few real-world professionals use this term. From now on, we will simply refer to <span class="math inline">\(\theta\)</span> as the <strong>parameters</strong> or often the <em>unknown parameters</em>, which emphasizes the fact that we rarely know their true value.</p>
<p>Now, let’s make a guess as the value(s) of <span class="math inline">\(\theta\)</span>. There are several methods we could use to make this guess, general systems for guessing that work well for many distributions and many datasets. Right now, the specific method we use is unimportant. What is important is that our guess should be some function of the data in front of us. That is, our data <span class="math inline">\(\boldsymbol{x}\)</span> should inform our guess for the parameters <span class="math inline">\(\theta\)</span>. The calculation which transforms our data into a guess of the parameter is called the <strong>estimator</strong> and written <span class="math inline">\(\hat{\theta}\)</span> (theta-hat):</p>
<p><span class="math display">\[\hat{\theta} = g(\boldsymbol{x})\]</span></p>
<p>Written this way, <span class="math inline">\(\hat{\theta}\)</span> is a calculation, a function <span class="math inline">\(g\)</span> that we apply to each new sample <span class="math inline">\(\boldsymbol{x}\)</span>, producing a different result for different samples. Anytime we use our estimator <span class="math inline">\(\hat{\theta}\)</span> on a specific sample, the result of this calculation is called the <strong>estimate</strong> of <span class="math inline">\(\theta\)</span>. The estimator is the function, and the estimate is its value for a specific sample.</p>
<p>Allow me a metaphor to sum this all up. Inferential statistics is like baking chocolate chip cookies. We each have an idea of what chocolate chip cookies should taste like (please take a moment to imagine your own perfect cookie). This theoretical goal is the <em>unknown parameter</em>. We want to make the best real-world version of this unattainable perfection. We can choose from many recipes, or even make a new recipe of our own. Some recipes lean toward one texture or another, or accentuate some flavors more than others. Each different recipe is a different <em>estimator</em>. We may think one recipe is better than another, but that doesn’t mean that it always produces a perfect batch of cookies. Depending on the materials at hand — the freshness of the ingredients, the specific brand of chocolate, the shape and reliability of the oven, our altitude — even our favorite recipe might produce a bad batch of cookies, or an unloved recipe might produce a surprisingly good batch of cookies. Each individual batch is a different <em>estimate</em>, and those conditions which vary batch-to-batch are the <em>data</em>.</p>
</section>
<section id="sec-statsvsml" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="sec-statsvsml"><span class="header-section-number">1.3</span> Statistical models and machine learning models</h2>
<p>Now that we’ve reviewed the aims of parametric inference, you might wonder how it differs from any other type of data analysis. After all, don’t all quantitative methods use the information in a dataset to answer questions about the world around us?</p>
<p>A <strong>statistical model</strong> makes a strong assumption that the data has a functional form, <em>i.e.</em> that the data are realizations of a random variable with a known distribution type. The only unknowns are the specific values of the parameters which created the data. Finding estimates for these parameters is typically the “finish line” for the analysis: most of the useful findings flow directly from the estimated theoretical distribution.</p>
<p>Statistical models are high-risk and high-reward. Very few datasets are perfectly distributed according to known probability distributions. Even if the generating process is well understood, the parameters which govern the data might change over time, and the dataset we use may give us outdated information. The extremes of the distribution will typically be the least observed, and we may produce catastrophically bad predictions when we naively fit the wrong distributions to these unobserved regions.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>At first, we accepted these risks because we had no other choice. There were few alternative methods, and practitioners were limited by the data and technology of their day: dozens or hundreds of observations, studied without electronic help or with relatively primitive computing resources.</p>
<p>All these drawbacks are also the strengths of statistical modeling. Statistical models can give a wide range of answers about how a generating process will behave in conditions never seen in the data. Statistical models are often very resource-light, and some can even be computed by hand. Statistical models will work with almost any amount of data and can give useful results with very small sample sizes. The parameters that we estimate from statistical models often give us powerful insight into how and why the world works.</p>
<p>A <strong>machine learning (ML) model</strong>, by contrast, does not believe or require that the data were generated by a probability distribution. ML models are generally uninterested in the idea of a single generating process which created all the data. They explore clusters and breakpoints within the data, seeking to find useful rulesets or “views” of the data which preserve as much of the original information as possible.</p>
<p>An ML model might use parametric components which are iteratively tuned in order to minimize an error function. For example, the bins created by a decision tree or the hyperplane classifiers of support vector machines are both defined by parameters. But these parameters are generally not associated with probability distributions.</p>
<p>ML models typically offer less interpretability than statistical models. They are less eager to find hidden “truths” in the world around us, less able to explain how changes in the inputs result in changes to the outputs. They can also be very resource-intensive, requiring both large amounts of data as well as large amounts of computing power.</p>
<p>In return, ML models offer flexibility and robustness in situations which would defy statistical modeling. ML models usually perform better than statistical models on very large datasets, which are more likely the result of many different generating processes rather than a single generating process. ML models thrive on heterogeneity and local differences in behavior, which often confound or mislead statistical models. You will learn about them in other courses. For now, we will focus on statistical models.</p>
</section>
<section id="sec-mle" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="sec-mle"><span class="header-section-number">1.4</span> Maximum likelihood estimation</h2>
<p>Let’s begin studying statistical inference with the simplest of cases: trying to determine the distribution which generated a single column of numbers. I assume your familiarity with some of the most common probability distributions: the discrete Bernoulli, binomial, geometric, and Poisson distributions, along with the continuous uniform, exponential, and normal distributions. If you need a refresher, the appendices contain more detail about each of these distributions and others we will study in later sections.</p>
<p>When we ask our computers today to model our data with set of parameters (such as the slope and intercept of a simple regression), the computer usually solves for the unknown parameters using <em>maximum likelihood estimation</em>, a method which was originally developed by Ronald Fisher over 100 years ago. The basic premise of maximum likelihood is to assume that the data aren’t… weird. Of course, the data could be weird: they could be unrepresentative or contain wild outliers. But <em>usually</em>, and by definition, our data aren’t <em>unusual</em>. So we will suppose that the best guess for the unknown parameters will be the values which make the data seem the most ordinary. There are other estimation methods worth learning, but in this course we will focus heavily on maximum likelihood estimation.</p>
<section id="motivation-and-definition" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="motivation-and-definition"><span class="header-section-number">1.4.1</span> Motivation and definition</h3>
<p>When we conduct statistical inference, we begin with data, and then we proceed to ask what distribution (and which parameters) could have created the data. Notice that this situation reverses the problems you might find in a high school or college class on probability. In those earlier classes, you would often be asked “if the parameters for this distribution are <span class="math inline">\(\theta\)</span>, then what is the probability of observing <span class="math inline">\(\boldsymbol{x}\)</span>?” For example:</p>
<ul>
<li><p>If a fair coin is flipped 50 times, what is the probability of observing exactly 27 heads and 23 tails?</p></li>
<li><p>If the heights of adult women in the United States are normally distributed with a mean of 162 cm and a standard deviation of 7 cm, what is the probability of drawing a sample of ten women who are all shorter than 152 cm?</p></li>
</ul>
<p>Instead, the shoe is now on the other foot:</p>
<ul>
<li><p>We flip a coin 50 times and observe 27 heads and 23 tails. Is it reasonable to believe that the coin is fair, despite these lopsided results?</p></li>
<li><p>We observe the heights of ten women, and we assume they represent a sample from a population which is normally distributed. Based on our sample, what are realistic ranges for the unknown mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>?</p></li>
</ul>
<p>To answer these questions, we will invent a way to “score” each set of possible parameters for a specific dataset. The parameters which seem well fit to the data will receive a higher score. We will consider all parameter choices which score highly as reasonable guesses for the unknown truth, but if we need to make just one guess, then we will choose the parameter or set of parameters with the highest score. We will call this scoring function <em>likelihood</em>.</p>
<p>Imagine a univariate sample <span class="math inline">\(\boldsymbol{x} = x_1,x_2,\ldots,x_n\)</span> which we believe came from a discrete probability distribution <span class="math inline">\(X\)</span> with parameters <span class="math inline">\(\theta\)</span>, and assume for the moment that the observations are independent of each other. Then we could compute the probability of observing the entire sample simply by multiplying together the probabilities of each individual observation:</p>
<p><span class="math display">\[P(\boldsymbol{x}|\theta) = \prod_{i=1}^n P(X=x_i|\theta)\]</span></p>
<p>The equation above treats <span class="math inline">\(\theta\)</span> as a fixed assumption and <span class="math inline">\(\boldsymbol{x}\)</span> as the input variable. Now we will repurpose this same function but treat <span class="math inline">\(\boldsymbol{x}\)</span> as the assumption and <span class="math inline">\(\theta\)</span> as the variable. We will also take the opportunity to extend this idea to a continuous case, using densities rather than probability mass functions:</p>
<div class="{callout-note}">
<p>Let <span class="math inline">\(X\)</span> be a random variable dependent on one or more parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\boldsymbol{x} = x_1,x_2,\ldots,x_n\)</span> be independent sample observations where <span class="math inline">\(x_i \sim X \quad \forall i \in \mathbb{N}: \ 1 \le i \le n.\)</span> Then we write the likelihood of <span class="math inline">\(\theta\)</span> given <span class="math inline">\(\boldsymbol{x}\)</span> as <span class="math inline">\(\mathcal{L}(\theta|\boldsymbol{x})\)</span>. If <span class="math inline">\(X\)</span> is discrete, we define:</p>
<p><span class="math display">\[\mathcal{L}(\theta|\boldsymbol{x})=\prod_{i=1}^n P(X=x_i|\theta)\]</span></p>
<p>And if <span class="math inline">\(X\)</span> is continuous with probability distribution function (PDF) <span class="math inline">\(f_X\)</span>, we define:</p>
<p><span class="math display">\[\mathcal{L}(\theta|\boldsymbol{x})=\prod_{i=1}^n f_X(x_i|\theta)\]</span></p>
<p>If <span class="math inline">\(\theta\)</span> has <span class="math inline">\(k\)</span> parameters, let <span class="math inline">\(\mathcal{S} \in \mathbb{R}^k\)</span> be the set of values <span class="math inline">\(\theta\)</span> can take which are permitted by the distribution of <span class="math inline">\(X\)</span> and the observed values of <span class="math inline">\(\boldsymbol{x}\)</span>. Then the choice for <span class="math inline">\(\theta\)</span> which maximizes <span class="math inline">\(\mathcal{L}(\theta|\boldsymbol{x})\)</span> is referred to as the <strong>maximum likelihood estimator (MLE)</strong> for <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[\hat{\theta}_\textit{MLE} = \mathop{\mathrm{argmax}}_{\theta \in \mathcal{S}} \,\mathcal{L}(\theta|\boldsymbol{x})\]</span></p>
</div>
<p>Notice that likelihood functions are usually products of the individual probabilities/densities of each observation in the sample. It can be difficult to directly maximize a complicated product of many terms. Happily, we can use one or two tricks which greatly simplify maximum likelihood estimation. The first trick is to notice that the log of a function reaches its maximum or minimum at the same input values as the original function (that is, logarithms are a monotonic transformation). Below I plot the likelihood function for the coin data mentioned above: 27 heads and 23 tails. I also will plot the log of the likelihood function:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#generate coin data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>coins <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">27</span>),<span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">23</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">#compute likelihood and log-likelihood functions</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>coin.lik <span class="ot">&lt;-</span> <span class="cf">function</span>(p) p<span class="sc">^</span><span class="fu">sum</span>(coins)<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">^</span><span class="fu">sum</span>(<span class="dv">1</span><span class="sc">-</span>coins)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>coin.ll <span class="ot">&lt;-</span> <span class="cf">function</span>(p) <span class="fu">sum</span>(coins)<span class="sc">*</span><span class="fu">log</span>(p)<span class="sc">+</span><span class="fu">sum</span>(<span class="dv">1</span><span class="sc">-</span>coins)<span class="sc">*</span><span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>p)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#plot coin likelihood and log-likelihood</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="at">bty=</span><span class="st">'n'</span>,<span class="at">cex=</span><span class="fl">0.8</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>((<span class="dv">1</span><span class="sc">:</span><span class="dv">99</span>)<span class="sc">/</span><span class="dv">100</span>, <span class="fu">coin.lik</span>((<span class="dv">1</span><span class="sc">:</span><span class="dv">99</span>)<span class="sc">/</span><span class="dv">100</span>),<span class="at">type=</span><span class="st">'l'</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Likelihood of Coin Parameter 'p'"</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">'p (Prob. of Heads)'</span>,<span class="at">ylab=</span><span class="st">'Likelihood'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">mean</span>(coins),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">'grey50'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span><span class="fu">mean</span>(coins),<span class="at">y=</span><span class="dv">0</span>,<span class="at">pos=</span><span class="dv">4</span>,<span class="at">col=</span><span class="st">'grey50'</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels=</span><span class="fu">paste0</span>(<span class="st">'x='</span>,<span class="fu">round</span>(<span class="fu">mean</span>(coins),<span class="dv">2</span>)))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>((<span class="dv">1</span><span class="sc">:</span><span class="dv">99</span>)<span class="sc">/</span><span class="dv">100</span>, <span class="fu">coin.ll</span>((<span class="dv">1</span><span class="sc">:</span><span class="dv">99</span>)<span class="sc">/</span><span class="dv">100</span>),<span class="at">type=</span><span class="st">'l'</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Log-likelihood of Coin Parameter 'p'"</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">'p (Prob. of Heads)'</span>,<span class="at">ylab=</span><span class="st">'Log-likelihood'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">mean</span>(coins),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">'grey50'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span><span class="fu">mean</span>(coins),<span class="at">y=</span><span class="sc">-</span><span class="dv">120</span>,<span class="at">pos=</span><span class="dv">4</span>,<span class="at">col=</span><span class="st">'grey50'</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels=</span><span class="fu">paste0</span>(<span class="st">'x='</span>,<span class="fu">round</span>(<span class="fu">mean</span>(coins),<span class="dv">2</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-coinlikelihood" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coinlikelihood-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="inference_files/figure-html/fig-coinlikelihood-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coinlikelihood-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Likelihood and log-likelihood of coin flip data
</figcaption>
</figure>
</div>
</div>
</div>
<p>Since the log of a product is simply the sum of logs of each term in the product, we often find log-likelihood proves easier to maximize than the original likelihood function.</p>
<div class="{callout-note}">
<p>Let <span class="math inline">\(\mathcal{L}(\theta|\boldsymbol{x})\)</span> be the likelihood of a parameter <span class="math inline">\(\theta\)</span> given a sample <span class="math inline">\(\boldsymbol{x}\)</span> and a distributional assumption about the random variable <span class="math inline">\(X\)</span> from which the sample was drawn. Then we denote the log-likelihood of <span class="math inline">\(\theta\)</span> given <span class="math inline">\(\boldsymbol{x}\)</span> as <span class="math inline">\(\ell(\theta|\boldsymbol{x})\)</span> and define it as</p>
<p><span class="math display">\[\ell(\theta│\boldsymbol{x})=\log{\mathcal{L}(\theta│\boldsymbol{x})}\]</span></p>
</div>
<p>The second trick which we use to more easily maximize likelihood is calculus. When a likelihood function is continuously differentiable and has a local maximum (as the two graphs do above), then the same parameter value which maximizes the likelihood and log-likelihood will be a root of the first derivative of the log-likelihood:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create first derivative of coin log-likelihood function</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>coin.dll <span class="ot">&lt;-</span> <span class="cf">function</span>(p) <span class="fu">sum</span>(coins)<span class="sc">/</span>p<span class="sc">-</span><span class="fu">sum</span>(<span class="dv">1</span><span class="sc">-</span>coins)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>p)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># graph first derivative of coin log-likelihood function</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">bty=</span><span class="st">'n'</span>,<span class="at">cex=</span><span class="fl">0.8</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>((<span class="dv">1</span><span class="sc">:</span><span class="dv">99</span>)<span class="sc">/</span><span class="dv">100</span>, <span class="fu">coin.dll</span>((<span class="dv">1</span><span class="sc">:</span><span class="dv">99</span>)<span class="sc">/</span><span class="dv">100</span>),<span class="at">type=</span><span class="st">'l'</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Derivative of LL of Coin Parameter 'p'"</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">'p (Prob. of Heads)'</span>,<span class="at">ylab=</span><span class="st">'dLL/dp'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">sum</span>(coins)<span class="sc">/</span><span class="fu">length</span>(coins),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">'grey50'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="at">x=</span><span class="fu">sum</span>(coins)<span class="sc">/</span><span class="fu">length</span>(coins),<span class="at">y=</span><span class="sc">-</span><span class="dv">2000</span>,<span class="at">pos=</span><span class="dv">4</span>,<span class="at">col=</span><span class="st">'grey50'</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels=</span><span class="fu">paste0</span>(<span class="st">'x='</span>,<span class="fu">round</span>(<span class="fu">sum</span>(coins)<span class="sc">/</span><span class="fu">length</span>(coins),<span class="dv">2</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-coindll" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-coindll-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="inference_files/figure-html/fig-coindll-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-coindll-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: First derivative of the log-likelihood of coin flip data
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="example-1-coin-flips" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="example-1-coin-flips"><span class="header-section-number">1.4.2</span> Example 1: Coin flips</h3>
<p>Suppose that we were given a coin and told it was fixed to land on one side more than the other. We flip the coin 50 times and record each ‘heads’ as 1 and each ‘tails’ as 0. The results below show that the coin landed ‘heads’ 27 times and ‘tails’ 23 times.</p>
<p><span class="math display">\[\boldsymbol{x}=\left\{\begin{array}{l}{0,0,1,1,0,1,1,1,1,1,1,1,0,0,1,0,0,1,0,0,0,0,1,1,0,\\ 0,1,1,1,0,0,1,1,1,0,1,1,1,0,1,1,0,1,1,1,1,1,1,0,0}\end{array}\right\}\]</span> Let us make a distributional assumption: the coin data can be modeled by a Bernoulli distribution. We cannot know whether this is correct or not, but it seems reasonable: Bernoulli trials require exactly two outcomes, a fixed probability of success, and independence between trials. While it’s possible that the coin could land on its edge, or that it deforms over time, or that it shows serial correlation, modeling the coin flips as Bernoulli trials seems true enough to be useful.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>What is the probability of observing the data if the coin truly lands heads 60% of the time? You should be able to answer this from your past lessons in probability and statistics. Since we assume trials to be independent, we can write:</p>
<p><span class="math display">\[P(\boldsymbol{x} | p = 0.6) = \prod_{i = 1}^{50} P(x_i | p = 0.6) = 0.4 \cdot 0.4 \cdot 0.6 \cdot \ldots \cdot 0.4 = 0.6^{27} \cdot 0.4^{23} \approx 7.2×10^{-16}\]</span> A very small number… although these results are actually quite unextraordinary, there are simply so many ways that 50 coin flips can occur (1.13 quadrillion ways) that even the most common sequences each have a very, very low probability.</p>
<p>What is the likelihood for any given parameter <span class="math inline">\(p\)</span>, given our dataset of 27 heads in 50 flips?</p>
<p><span class="math display">\[\mathcal{L}(p|\boldsymbol{x}) = \prod_{i=1}^{50} P(x_i|p) = p^{27} (1-p)^{23}\]</span></p>
<p>What is the value of <span class="math inline">\(p\)</span> which maximizes this likelihood? It’s not immediately evident from the equation above, but perhaps the log-likelihood will help us to solve for <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[\ell(p|\boldsymbol{x}) = \log{\mathcal{L}(p|\boldsymbol{x})} = 27\log{⁡p}+23\log⁡(1-p)\]</span></p>
<p>This is still difficult to solve by hand, so let’s bring in the final trick, and instead try to find the root of the first derivative of the log-likelihood:</p>
<p><span class="math display">\[\frac{d}{dp} \ell(p│\boldsymbol{x}) = \frac{27}{p}-\frac{23}{1-p}\]</span></p>
<p>Setting the above equal to zero and solving for p,</p>
<p><span class="math display">\[\frac{27}{p}-\frac{23}{1-p} = 0 \Longrightarrow \frac{27}{p} = \frac{23}{1-p} \Longrightarrow 27-27p = 23p \Longrightarrow 50p=27 \Longrightarrow \hat{p}_\textit{MLE} = \frac{27}{50}=0.54\]</span></p>
<p>In fact, we could abstract a little further here to find the MLE for any Bernoulli-distributed sample. Let <span class="math inline">\(k\)</span> be the number of successes and <span class="math inline">\(n\)</span> be the total number of trials. Using the same math as above, you will find that:</p>
<p><span class="math display">\[\hat{p}_\mathit{MLE} = k/n = \frac{\sum_i x_i}{n} = \bar{x}\]</span> This is a tidy little result. When our data are Bernoulli distributed, then the maximum likelihood estimator for the parameter <span class="math inline">\(p\)</span> is simply the sample average, <em>i.e.</em> the proportion of observations that were successes. I like findings such as these which conform with our intuition: when we have data on a Bernoulli process, our best guess as to how often successes truly happen will simply be how often successes occurred in our data.</p>
</section>
<section id="example-2-heights" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="example-2-heights"><span class="header-section-number">1.4.3</span> Example 2: Heights</h3>
<p>The above example used a very simple discrete distribution with a single parameter. Let’s try again with a more complicated continuous distribution, which uses two parameters. Across the entire world population, heights are not exactly distributed according to any known distribution. However, among otherwise homogeneous populations, we do observe that heights are roughly normally distributed. Let’s pretend that we sampled 10 adult women in the United States and measured their heights. Rounded to the nearest tenth of a centimeter, their heights are listed below:<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p><span class="math display">\[\boldsymbol{x}=\{170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3\}\]</span> Let us assume that these heights are drawn from a normal distribution. What then would be the best guess for the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, which are the mean and variance of the distribution? We will start by finding the likelihood function. Recall that if <span class="math inline">\(X\)</span> is normal,</p>
<p><span class="math display">\[f_X(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\]</span> From this, we can compute the likelihood of any pair of normal parameters for any dataset:</p>
<p><span class="math display">\[\mathcal{L}(\mu,\sigma^2|\boldsymbol{x}) = (2\pi\sigma^2)^{-n/2}\cdot e^{-\frac{1}{2\sigma^2}\sum_{i}(x_i-\mu)^2}\]</span> Then, we can find the log-likelihood:</p>
<p><span class="math display">\[\ell(\mu,\sigma^2|\boldsymbol{x}) = \log{\mathcal{L}(\mu,\sigma^2|\boldsymbol{x})} = -\frac{n}{2}\log{2\pi\sigma^2} - \frac{1}{2\sigma^2} \sum_{i}(x_i-\mu)^2\]</span></p>
<p>Next, we will take the derivative with respect to <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[\frac{\partial\ell}{\partial\mu} = \frac{1}{\sigma^2} \sum_{i}(x_i-μ) \]</span> From here we will solve for the root of the derivative, which will be the value of <span class="math inline">\(\mu\)</span> that maximizes the original likelihood function:</p>
<p><span class="math display">\[\frac{\partial\ell}{\partial\mu} = 0 \Longrightarrow \sum_i (x_i-\mu) = 0 \Longrightarrow \sum_i x_i - n\mu = 0 \Longrightarrow n\mu=\sum_i x_i \Longrightarrow \hat{\mu}_\textit{MLE} = \bar{x}\]</span></p>
<p>What a fantastic bit of luck. The best guess for the true mean of a normal distribution is the sample mean of our data! Let’s finish up by repeating for variance: first we take the partial derivative of the log-likelihood with respect to <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[\frac{\partial\ell}{\partial\sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_i (x_i-\mu)^2\]</span> Then we will solve for the root of the derivative, which will be the value of <span class="math inline">\(\sigma^2\)</span> which maximizes the original likelihood function:</p>
<p><span class="math display">\[\frac{\partial\ell}{\partial\sigma^2} =0 \Longrightarrow \frac{1}{2\sigma^4} \sum_i (x_i-\mu)^2 = \frac{n}{2\sigma^2} \Longrightarrow \frac{1}{\sigma^2} \sum_i (x_i-\mu)^2 = n \Longrightarrow \hat{\sigma}^2_\textit{MLE} = \frac{1}{n} \sum_i (x_i-\mu)^2\]</span></p>
<p>You may recognize this quantity as the biased (uncorrected) sample variance. Although this calculation seems very sensible, we will later show that it systematically underestimates the true variance <span class="math inline">\(\sigma^2\)</span>, which provides our first hint that maximum likelihood estimation is not the final answer for every problem we will encounter.</p>
<p>With these results in hand, we can produce the MLEs for our normal parameters given our height data. We would say that the best guess for the true mean height of adult women in the United States is 163.5 cm and the best guess for their variance would be 50.4 cm<span class="math inline">\({}^2\)</span> (implying a standard deviation of 7.1 cm).<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#generate height data</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>heights <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">170.1</span>,<span class="fl">161.6</span>,<span class="fl">175.2</span>,<span class="fl">166.3</span>,<span class="fl">165.6</span>,<span class="fl">165.8</span>,<span class="fl">152.0</span>,<span class="fl">155.8</span>,<span class="fl">168.6</span>,<span class="fl">154.3</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">#generate height parameter ll contour plot</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>height.ll <span class="ot">&lt;-</span> <span class="cf">function</span>(parms) {<span class="sc">-</span><span class="dv">1</span><span class="sc">*</span><span class="fu">length</span>(heights)<span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>parms[<span class="dv">2</span>])<span class="sc">/</span><span class="dv">2</span><span class="sc">-</span><span class="fu">sum</span>((heights<span class="sc">-</span>parms[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>parms[<span class="dv">2</span>])}</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>xgrid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">140</span>,<span class="dv">180</span>,<span class="fl">0.1</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>ygrid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">25</span>,<span class="dv">80</span>,<span class="fl">0.1</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>zgrid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(<span class="fu">cbind</span>(<span class="fu">rep</span>(xgrid,<span class="at">times=</span><span class="fu">length</span>(ygrid)),<span class="fu">rep</span>(ygrid,<span class="at">each=</span><span class="fu">length</span>(xgrid))),</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>                      <span class="dv">1</span>,height.ll),<span class="at">ncol=</span><span class="fu">length</span>(ygrid))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="fu">filled.contour</span>(xgrid,ygrid,zgrid,<span class="at">levels=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">120</span>,<span class="sc">-</span><span class="dv">100</span>,<span class="sc">-</span><span class="dv">80</span>,<span class="sc">-</span><span class="dv">70</span>,<span class="sc">-</span><span class="dv">60</span>,<span class="sc">-</span><span class="dv">50</span>,<span class="sc">-</span><span class="dv">40</span>,<span class="sc">-</span><span class="dv">35</span>,<span class="sc">-</span><span class="dv">34</span>,<span class="sc">-</span><span class="fl">33.8</span>,<span class="sc">-</span><span class="fl">33.7</span>),</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>               <span class="at">col=</span><span class="fu">paste0</span>(<span class="st">'#000000'</span>,<span class="fu">c</span>(<span class="st">'00'</span>,<span class="st">'20'</span>,<span class="st">'40'</span>,<span class="st">'60'</span>,<span class="st">'80'</span>,<span class="st">'9f'</span>,<span class="st">'af'</span>,<span class="st">'cf'</span>,<span class="st">'ef'</span>,<span class="st">'ff'</span>)),</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>               <span class="at">main=</span><span class="st">'Log-Lik of Parameters for Height Data'</span>,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>               <span class="at">xlab=</span><span class="st">'Mu (mean height, in cm)'</span>,<span class="at">ylab=</span><span class="st">'Sigma^2 (variance)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-heightcontour" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-heightcontour-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="inference_files/figure-html/fig-heightcontour-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-heightcontour-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.3: Contour plot of height data log-likelihoods
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can confirm that these solutions are reasonable by plotting the log-likelihood of various combinations of mean and variance, seen above. Notice that a broad range of possible means and variances have log-likelihoods <em>close</em> to the maximum value of -33.8. Any of these combinations could easily have generated our data. But if we have to make one guess, then the MLE values of (163.5, 50.4) would be our best choice.</p>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Somehow this proved less comforting than I had hoped.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Later on in these notes, we will extend our methods to data which form a matrix of values, where each component <span class="math inline">\(x_i\)</span> is a vector of its own: <span class="math inline">\(\mathbf{X}={\boldsymbol{x_1},\boldsymbol{x_2},\ldots,\boldsymbol{x_n}}\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>This was a major driver of the 2007–08 financial crisis, for example.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>You may recall the statistician George Box’s maxim: all models are wrong, some are useful.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>For the stubborn imperial-unit diehards among us, these heights range from five feet (60”) to five feet nine inches (69”).<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Or for provincial bumpkins like myself: 64 in, 20 in<span class="math inline">\({}^2\)</span>, and 3 in.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduction</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./approximations.html" class="pagination-link" aria-label="Useful approximations">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Useful approximations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>