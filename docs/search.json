[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Analysis Using R: Online Notes",
    "section": "",
    "text": "Introduction\nHello! I’m Jonathan, and I want to teach you statistics. I have designed this website to accompany the University of Chicago course ADSP 31014 “Statistical Models for Data Science”, since I was unable to find a single textbook which covered all of the requisite material, and and did not want my students to buy three expensive textbooks for a ten-week class.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-howtouse",
    "href": "index.html#sec-howtouse",
    "title": "Statistical Analysis Using R: Online Notes",
    "section": "How to use this website",
    "text": "How to use this website\nThe left-side chapter headings and search bar will help you to select different pages. The right-side table of contents will help you to navigate within each page.\nThe appendices contain notes on common probability distributions, a few proofs (which might be of interest but are not necessary to progress through the material), and a table of commonly used symbols and their meanings. For more on how to read these chapters, please see the last section below, A note on notation.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#purpose-and-scope",
    "href": "index.html#purpose-and-scope",
    "title": "Statistical Analysis Using R: Online Notes",
    "section": "Purpose and scope",
    "text": "Purpose and scope\nEleventy years ago, I studied probability and statistics “for statisticians”. Probability theory is elegant and contemplative. Graduate-level work can be completed with just pencil and paper, through proofs and symbolic notation.\nThe tools and methods discussed in these notes were built from such theoretical work. However, the students I teach are headed (generally) to industry, not to research positions or Ph.D programs. They may never need to author a proof, but they will need to draw conclusions about the world from noisy, biased, incomplete, or insufficient data, to perform their analyses within a modern day tech stack, and to communciate their findings to decisionmakers.\nSo I want to teach probability and statistics “for data scientists”. Data science is a tradecraft, not a body of theory, and it allows people and organizations to answer questions, solve problems, and achieve goals. Data science which does not help us to confront real problems or understand real datasets is not truly data science after all.\nI will try to stay focused on that mission in these notes, which should not be confused for a true textbook: they more closely resemble a detailed set of lecture slides, paired with ready-to-use code and interactive workshops. The layout of these notes echoes the syllabus for my class: a fast-paced survey of inferential statistics and parametric modeling methods, using likelihood estimation theory as a throughline for three main sections:\n\nFirst, we’ll review some topics in univariate analysis which may already be familiar to many readers.\nSecond, we’ll examine ordinary least squares (OLS) regression in more detail than readers may have seen at the college level.\nThird, we’ll abstract from OLS regression to the family of models called generalized linear models, which describe non-linear trends among non-normally distributed datasets.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-referencetexts",
    "href": "index.html#sec-referencetexts",
    "title": "Statistical Analysis Using R: Online Notes",
    "section": "Recommended textbooks",
    "text": "Recommended textbooks\nDespite the sentiments above, I retain both fondness and respect for true textbooks, and I highly suggest that readers of these notes pair them with one or more proper reference tomes. A few recommendations follow:\n\nIntroduction to Probability 2nd edition, by Joseph Blitzstein and Jessica Hwang\nThis book is freely available and would help you prepare if you need to spend more time with the foundational topics which precede this course. The authors cover probability and random variables in depth, but they do not cover inference/estimation or regression topics.\nPractical Statistics for Data Scientists 2nd edition, by Peter Bruce, Andrew Bruce, and Peter Gedeck\nThis might be the most comprehensive of the books in this list. It covers many of our topics, and contains extra sections on experimental design, classification models, and machine learning models which might be useful to readers of these notes.\nFoundations and Applications of Statistics: An Introduction using R 2nd edition, by Randall Pruim\nThis book covers most of our material, but skimps a little on generalized linear models. It focuses mostly on univariate analysis and linear regression, with some basic probability as well as a brief section on logistic regression. It also contains a lot of R code.\nFoundations of Linear and Generalized Linear Models, by Alan Agresti\nThis book aligns well with the second and third sections of these notes (linear regression and GLMs), but does not cover basic probability or univariate inference.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-these-notes-were-made",
    "href": "index.html#how-these-notes-were-made",
    "title": "Statistical Analysis Using R: Online Notes",
    "section": "How these notes were made",
    "text": "How these notes were made\nI assembled these notes using Quarto, a publishing system built around the Pandoc markdown language. I wrote all the code backing these notes in R, and alongside every figure or table you can find the corresponding R code.\nNeither the text nor the R code in these notes were generated by AI tools: for better and worse the opinions expressed here are my own, and the I’ve described these concepts in my own voice.1 Complaints can be submitted here.\nThese notes began as a (clunky) Word document shared with my students across several academic quarters. Their questions, requests for clarification, and occasional corrections have all vastly improved my content, and I thank them for their help.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-notation",
    "href": "index.html#sec-notation",
    "title": "Statistical Analysis Using R: Online Notes",
    "section": "A note on notation",
    "text": "A note on notation\nUnfortunately, no two statistical sources use exactly the same notation. Any choice I make will inevitably differ from other sources you consult. I will follow the common conventions when I can, and beg your understanding when I cannot. I will occasionally use two different options to represent the same concept, not because I wish to confuse you, but because sometimes complete consistency creates impossible formatting challenges or even greater ambiguity.\n\nObservations from a sample and realizations of a random variable will use lowercase Latin letters, with subscripts as needed:\n\n\\[x_1,x_2,...,x_n\\]\n\nRandom variables themselves will use uppercase Latin letters.2 Subscripts on random variables suggest relations between them, such as two predictors for the same response:\n\n\\[Y,Z;\\quad X_1,X_2\\]\n\nTrue parameters of a model will often use Greek lowercase letters:3\n\n\\[\\mu,\\sigma^2,\\beta_0,\\beta_1\\]\n\nEstimates of random variables will use a ‘hat’ accent above the original symbol or use the matching lowercase Latin alphabet letter:4\n\n\\[\\hat{\\mu},\\hat{\\beta}_0;\\quad s^2,b_1\\]\n\nVectors (including univariate samples) will use an arrow accent or boldface lowercase letters. Matrices will use bold uppercase letters (not italic):\n\n\\[\\vec{u},\\boldsymbol{y};\\quad \\mathbf{X}\\]\n\nElements of a matrix or data with multiple indices will use a double subscript:\n\n\\[\\mathbf{X} = \\begin{bmatrix} x_{1,1} & x_{1,2} \\\\ x_{2,1} & x_{2,2} \\end{bmatrix};\\qquad y_{i \\bullet} = \\frac{1}{n_i}\\sum_j y_{i,j}\\]\n\nBy convention some terms are stylized using blackboard letters, including the set of reals and the set of integers, as well as expectation and variance.\n\n\\[\\mathbb{R},\\mathbb{Z}; \\quad \\mathbb{V}[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\]\n\nScript typefaces may be used for other sets, or to “re-use” a letter with more established meanings:\n\n\\[\\mathcal{S},\\mathcal{L},\\ell \\qquad (\\textrm{compare with}\\ S, L, l)\\]",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Statistical Analysis Using R: Online Notes",
    "section": "",
    "text": "AI assistance was used to brainstorm case studies and examples, and to help with the layout and coding of the website itself.↩︎\nThe most common exception will be my use of the Greek lowercase epsilon \\((\\varepsilon)\\) for an error term, per tradition.↩︎\nWith many exceptions when parameters are traditionally named otherwise, such as using ‘a’ and ‘b’ for the parameters of a Uniform distribution or ‘df’ for degrees of freedom in a t-distribution.↩︎\nThe hat accent is properly called a circumflex, but statisticians say ‘hat’, e.g. the estimate for ‘mu’ is ‘mu-hat’.↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface: Inference",
    "section": "",
    "text": "Inferential statistics\nAll of the methods and theory presented in these notes are examples of parametric inference. If you aren’t already familiar with that term, it’s worth exploring now.\nWhen I was young, I first heard the term “statistics” used to summarize or highlight the values of a dataset:\nThese figures are correctly called statistics, but statisticians label them as descriptive statistics. They are incontrovertible facts. They are properties of a fixed sample which we can all agree upon. They do not require theory, just a calculation. Once I explain to you how to compute a median, you know that the median of the numbers \\(\\{1,1,2,3,5,8,13\\}\\) is 3, without having to argue over assumptions.\nBy contrast, the field of inferential statistics concerns itself with a shadowy world we cannot see and which might not exist. Inferential statistics starts from the assumption that the numbers in our world come to us from generating processes which are unknown — but systematic and guessable. The data we observe are realizations of random variables, and the random variables are defined by formulae, and the formulae are controlled by parameters. Once we know those parameters, we can usually answer more detailed questions about the generating process, even questions for which we have no direct observational evidence.\nFor example, our study of the Earth and the broader solar system suggests that meteors with a diameter of more than 1 km strike the Earth roughly every 500,000 years. We believe that such meteor strikes are closely approximated by a Poisson process, and that therefore the time between large meteor strikes could be represented by an exponential distribution with the parameter \\(\\lambda=0.000002\\) (here \\(\\lambda\\), or lambda, represents the long-term rate of meteor strikes per year).\nIf all this is true, then I can use the exponential distribution to calculate that the probability of a large meteor strike in my lifetime (or equivalently, in the next 50 years) is only about 0.01%:1\n\\[P(X \\le 50) = 1 - e^{-\\lambda \\cdot 50} = 1 - e^{-0.0001} \\approx 0.00009995\\]\nOf course, I cannot prove my calculation is right or wrong. If the next large meteor struck tomorrow, or just as I died, or two thousand years from now, none of these would validate or invalidate my estimate. But the estimate might still be flawed for several reasons. Further research might change our estimate of how often such meteors strike the Earth — perhaps it’s roughly once every 400,000 years, or every 600,000 years. We may even be wrong about meteor strikes being Poisson processes, and if they are not, then I would need a completely new set of assumptions.\nYet consider my approach:\nThese are the central concepts of inferential statistics. We make assumptions about how the world works, and then use data to estimate various unknown parameters. We are always wrong in our guesses –– and we don’t even know how wrong we are. We are sometimes even wrong about our distributional assumptions. I wouldn’t say we are taking guesses in the dark, but the room can be very dim indeed. However, our reward is to be able to describe things we have not seen, to predict the future, and to better understand the past.",
    "crumbs": [
      "Preface: Inference"
    ]
  },
  {
    "objectID": "preface.html#sec-inferentialstatistics",
    "href": "preface.html#sec-inferentialstatistics",
    "title": "Preface: Inference",
    "section": "",
    "text": "Among Major League Baseball pitchers, Nolan Ryan holds the career strikeout record with a total of 5714.\nJapan has a greater proportion of centenarians (people who are over 100 years of age) than any other country, at 43 per 100,000 residents.\nRoughly 24% of the bullet chess players on &lt;www.chess.com&gt; have an Elo score better than my own.\n\n\n\n\n\n\n\n\n\nI started from a dataset (the geologic record of how many large meteors have struck the Earth during the Cenozoic period)\nI assumed that the data came from a specific probability distribution\nI used the data to make a guess as to which parameters controlled that distribution\nWith my guess for the parameters, I was able to answer questions that the data by itself could not answer",
    "crumbs": [
      "Preface: Inference"
    ]
  },
  {
    "objectID": "preface.html#sec-parameters",
    "href": "preface.html#sec-parameters",
    "title": "Preface: Inference",
    "section": "How all parametric estimation is performed",
    "text": "How all parametric estimation is performed\nI described the steps above in generalities. We have not yet discussed how to make a guess about the parameters of a distribution. Nor have we discussed what makes one guessing method better or worse than other guessing methods. Before I can talk about these subjects, we need to introduce a few definitions.\nWe will start with some data. Perhaps we have a univariate vector of \\(n\\) observations: \\(\\boldsymbol{x} = \\{x_1,x_2,\\ldots,x_n\\}\\).2 I will frequently describe \\(\\boldsymbol{x}\\) as our sample, even if it was collected by non-sampling methods.\nNext, let’s introduce a distributional assumption. The data \\(\\boldsymbol{x}\\) are realizations of a random variable \\(X\\) with a cumulative distribution function \\(F_X\\) controlled by one or more parameters \\(\\theta\\) (theta). The probability or density of each observation \\(x_i\\) depends not only on its value but also on the parameters \\(\\theta\\). We could write,\n\\[P(X \\le x_i) = F_X(x_i;\\theta) \\qquad \\forall i \\in \\mathbb{N}:\\ 1 \\le i \\le n\\]\nStatistics textbooks sometimes refer to \\(\\theta\\) as the estimand. In my experience, very few real-world professionals use this term. From now on, we will simply refer to \\(\\theta\\) as the parameters or often the unknown parameters, which emphasizes the fact that we rarely know their true value.\nNow, let’s make a guess as the value(s) of \\(\\theta\\). There are several methods we could use to make this guess, general systems for guessing that work well for many distributions and many datasets. Right now, the specific method we use is unimportant. What is important is that our guess should be some function of the data in front of us. That is, our data \\(\\boldsymbol{x}\\) should inform our guess for the parameters \\(\\theta\\). The calculation which transforms our data into a guess of the parameter is called the estimator and written \\(\\hat{\\theta}\\) (theta-hat):\n\\[\\hat{\\theta} = g(\\boldsymbol{x})\\]\nWritten this way, \\(\\hat{\\theta}\\) is a calculation, a function \\(g\\) that we apply to each new sample \\(\\boldsymbol{x}\\), producing a different result for different samples. Anytime we use our estimator \\(\\hat{\\theta}\\) on a specific sample, the result of this calculation is called the estimate of \\(\\theta\\). The estimator is the function, and the estimate is its value for a specific sample.\nAllow me a metaphor to sum this all up. Inferential statistics is like baking chocolate chip cookies. We each have an idea of what chocolate chip cookies should taste like (please take a moment to imagine your own perfect cookie). This theoretical goal is the unknown parameter. We want to make the best real-world version of this unattainable perfection. We can choose from many recipes, or even make a new recipe of our own. Some recipes lean toward one texture or another, or accentuate some flavors more than others. Each different recipe is a different estimator. We may think one recipe is better than another, but that doesn’t mean that it always produces a perfect batch of cookies. Depending on the materials at hand — the freshness of the ingredients, the specific brand of chocolate, the shape and reliability of the oven, our altitude — even our favorite recipe might produce a bad batch of cookies, or an unloved recipe might produce a surprisingly good batch of cookies. Each individual batch is a different estimate, and those conditions which vary batch-to-batch are the data.",
    "crumbs": [
      "Preface: Inference"
    ]
  },
  {
    "objectID": "preface.html#sec-statsvsml",
    "href": "preface.html#sec-statsvsml",
    "title": "Preface: Inference",
    "section": "Statistical models and machine learning models",
    "text": "Statistical models and machine learning models\nNow that we’ve reviewed the aims of parametric inference, you might wonder how it differs from any other type of data analysis. After all, don’t all quantitative methods use the information in a dataset to answer questions about the world around us?\nA statistical model makes a strong assumption that the data has a functional form, i.e. that the data are realizations of a random variable with a known distribution type. The only unknowns are the specific values of the parameters which created the data. Finding estimates for these parameters is typically the “finish line” for the analysis: most of the useful findings flow directly from the estimated theoretical distribution.\nStatistical models are high-risk and high-reward. Very few datasets are perfectly distributed according to known probability distributions. Even if the generating process is well understood, the parameters which govern the data might change over time, and the dataset we use may give us outdated information. The extremes of the distribution will typically be the least observed, and we may produce catastrophically bad predictions when we naively fit the wrong distributions to these unobserved regions.3\nAt first, we accepted these risks because we had no other choice. There were few alternative methods, and practitioners were limited by the data and technology of their day: dozens or hundreds of observations, studied without electronic help or with relatively primitive computing resources.\nAll these drawbacks are also the strengths of statistical modeling. Statistical models can give a wide range of answers about how a generating process will behave in conditions never seen in the data. Statistical models are often very resource-light, and some can even be computed by hand. Statistical models will work with almost any amount of data and can give useful results with very small sample sizes. The parameters that we estimate from statistical models often give us powerful insight into how and why the world works.\nA machine learning (ML) model, by contrast, does not believe or require that the data were generated by a probability distribution. ML models are generally uninterested in the idea of a single generating process which created all the data. They explore clusters and breakpoints within the data, seeking to find useful rulesets or “views” of the data which preserve as much of the original information as possible.\nAn ML model might use parametric components which are iteratively tuned in order to minimize an error function. For example, the bins created by a decision tree or the hyperplane classifiers of support vector machines are both defined by parameters. But these parameters are generally not associated with probability distributions.\nML models typically offer less interpretability than statistical models. They are less eager to find hidden “truths” in the world around us, less able to explain how changes in the inputs result in changes to the outputs. They can also be very resource-intensive, requiring both large amounts of data as well as large amounts of computing power.\nIn return, ML models offer flexibility and robustness in situations which would defy statistical modeling. ML models usually perform better than statistical models on very large datasets, which are more likely the result of many different generating processes rather than a single generating process. ML models thrive on heterogeneity and local differences in behavior, which often confound or mislead statistical models. You will learn about them in other courses. For now, we will focus on statistical models.",
    "crumbs": [
      "Preface: Inference"
    ]
  },
  {
    "objectID": "preface.html#footnotes",
    "href": "preface.html#footnotes",
    "title": "Preface: Inference",
    "section": "",
    "text": "Somehow this proved less comforting than I had hoped.↩︎\nLater on in these notes, we will extend our methods to data which form a matrix of values, where each component \\(x_i\\) is a vector of its own: \\(\\mathbf{X}={\\boldsymbol{x_1},\\boldsymbol{x_2},\\ldots,\\boldsymbol{x_k}}\\).↩︎\nThis was a major driver of the 2007–08 financial crisis, for example.↩︎",
    "crumbs": [
      "Preface: Inference"
    ]
  },
  {
    "objectID": "likelihood.html",
    "href": "likelihood.html",
    "title": "Univariate likelihood",
    "section": "",
    "text": "Motivation and definition\nLet’s begin studying statistical inference with the simplest of cases: trying to determine the distribution which generated a single column of numbers. I assume your familiarity with some of the most common probability distributions: the discrete Bernoulli, binomial, geometric, and Poisson distributions, along with the continuous uniform, exponential, and normal distributions. If you need a refresher, the appendices contain more detail about each of these distributions and others we will study in later sections.\nWhen we ask our computers today to model our data with set of parameters (such as the slope and intercept of a simple regression), the computer usually solves for the unknown parameters using maximum likelihood estimation, a method developed by Ronald Fisher over 100 years ago. The basic premise of maximum likelihood is to assume that the data aren’t… weird. Of course, the data could be weird: they could be unrepresentative or contain wild outliers. But usually, and by definition, our data aren’t unusual. So we will suppose that the best guess for the unknown parameters will be the values which make the data seem the most ordinary. There are other estimation methods worth learning, but in this course we will focus heavily on maximum likelihood estimation.\nWhen we conduct statistical inference, we begin with data, and then we proceed to ask what distribution (and which parameters) could have created the data. Notice that this situation reverses the problems you might find in a high school or college class on probability. In those earlier classes, you would often be asked “if the parameters for this distribution are \\(\\theta\\), then what is the probability of observing \\(\\boldsymbol{x}\\)?” For example:\nInstead, the shoe is now on the other foot:\nTo answer these questions, we will invent a way to “score” each set of possible parameters for a specific dataset. The parameters which seem well fit to the data will receive a higher score. We will consider all parameter choices which score highly as reasonable guesses for the unknown truth, but if we need to make just one guess, then we will choose the parameter or set of parameters with the highest score. We will call this scoring function likelihood.\nImagine a univariate sample \\(\\boldsymbol{x} = x_1,x_2,\\ldots,x_n\\) which we believe came from a discrete probability distribution \\(X\\) with parameters \\(\\theta\\), and assume for the moment that the observations are independent of each other. Then we could compute the probability of observing the entire sample simply by multiplying together the probabilities of each individual observation:\n\\[P(\\boldsymbol{x}|\\theta) = \\prod_{i=1}^n P(X=x_i|\\theta)\\]\nThe equation above treats \\(\\theta\\) as a fixed assumption and \\(\\boldsymbol{x}\\) as the input variable. Now we will repurpose this same function but treat \\(\\boldsymbol{x}\\) as the assumption and \\(\\theta\\) as the variable. We will also take the opportunity to extend this idea to a continuous case, using densities rather than probability mass functions:\nNotice that likelihood functions are usually products of the individual probabilities/densities of each observation in the sample. It can be difficult to directly maximize a complicated product of many terms. Happily, we can use one or two tricks which greatly simplify maximum likelihood estimation. The first trick is to notice that the log of a function reaches its maximum or minimum at the same input values as the original function (that is, logarithms are a monotonic transformation). Below I plot the likelihood function for the coin data mentioned above: 27 heads and 23 tails. I also will plot the log of the likelihood function:\nCode\n#generate coin data\ncoins &lt;- c(rep(1,27),rep(0,23))\n\n#compute likelihood and log-likelihood functions\ncoin.lik &lt;- function(p) p^sum(coins)*(1-p)^sum(1-coins)\ncoin.ll &lt;- function(p) sum(coins)*log(p)+sum(1-coins)*log(1-p)\n\n#plot coin likelihood and log-likelihood\npar(mfrow=c(1,2),bty='n',cex=0.8)\nplot((1:99)/100, coin.lik((1:99)/100),type='l',lwd=2,\n     main=\"Likelihood of Coin Parameter 'p'\",\n     xlab='p (Prob. of Heads)',ylab='Likelihood')\nabline(v=mean(coins),lwd=2,lty=2,col='grey50')\ntext(x=mean(coins),y=0,pos=4,col='grey50',\n     labels=paste0('x=',round(mean(coins),2)))\n\nplot((1:99)/100, coin.ll((1:99)/100),type='l',lwd=2,\n     main=\"Log-likelihood of Coin Parameter 'p'\",\n     xlab='p (Prob. of Heads)',ylab='Log-likelihood')\nabline(v=mean(coins),lwd=2,lty=2,col='grey50')\ntext(x=mean(coins),y=-120,pos=4,col='grey50',\n     labels=paste0('x=',round(mean(coins),2)))\n\n\n\n\n\n\n\n\nFigure 1.1: Likelihood and log-likelihood of coin flip data\nSince the log of a product is simply the sum of logs of each term in the product, we often find log-likelihood proves easier to maximize than the original likelihood function.\nThe second trick which we use to more easily maximize likelihood is calculus. When a likelihood function is continuously differentiable and has a local maximum (as the two graphs do above), then the same parameter value which maximizes the likelihood and log-likelihood will be a root of the first derivative of the log-likelihood:\nCode\n# create first derivative of coin log-likelihood function\ncoin.dll &lt;- function(p) sum(coins)/p-sum(1-coins)/(1-p)\n\n# graph first derivative of coin log-likelihood function\npar(mfrow=c(1,1),bty='n',cex=0.8)\nplot((1:99)/100, coin.dll((1:99)/100),type='l',lwd=2,\n     main=\"Derivative of LL of Coin Parameter 'p'\",\n     xlab='p (Prob. of Heads)',ylab='dLL/dp')\nabline(h=0)\nabline(v=sum(coins)/length(coins),lwd=2,lty=2,col='grey50')\ntext(x=sum(coins)/length(coins),y=-2000,pos=4,col='grey50',\n     labels=paste0('x=',round(sum(coins)/length(coins),2)))\n\n\n\n\n\n\n\n\nFigure 1.2: First derivative of the log-likelihood of coin flip data",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#motivation-and-definition",
    "href": "likelihood.html#motivation-and-definition",
    "title": "Univariate likelihood",
    "section": "",
    "text": "If a fair coin is flipped 50 times, what is the probability of observing exactly 27 heads and 23 tails?\nIf the heights of adult women in the United States are normally distributed with a mean of 162 cm and a standard deviation of 7 cm, what is the probability of drawing a sample of ten women who are all shorter than 152 cm?\n\n\n\nWe flip a coin 50 times and observe 27 heads and 23 tails. Is it reasonable to believe that the coin is fair, despite these lopsided results?\nWe observe the heights of ten women, and we assume they represent a sample from a population which is normally distributed. Based on our sample, what are realistic ranges for the unknown mean \\(\\mu\\) and variance \\(\\sigma^2\\)?\n\n\n\n\n\n\nLet \\(X\\) be a random variable dependent on one or more parameters \\(\\theta\\) and \\(\\boldsymbol{x} = x_1,x_2,\\ldots,x_n\\) be independent sample observations where \\(x_i \\sim X \\quad \\forall i \\in \\mathbb{N}: \\ 1 \\le i \\le n.\\) Then we write the likelihood of \\(\\theta\\) given \\(\\boldsymbol{x}\\) as \\(\\mathcal{L}(\\theta|\\boldsymbol{x})\\). If \\(X\\) is discrete, we define:\n\\[\\mathcal{L}(\\theta|\\boldsymbol{x})=\\prod_{i=1}^n P(X=x_i|\\theta)\\]\nAnd if \\(X\\) is continuous with probability distribution function (PDF) \\(f_X\\), we define:\n\\[\\mathcal{L}(\\theta|\\boldsymbol{x})=\\prod_{i=1}^n f_X(x_i|\\theta)\\]\nIf \\(\\theta\\) has \\(k\\) parameters, let \\(\\mathcal{S} \\in \\mathbb{R}^k\\) be the set of values \\(\\theta\\) can take which are permitted by the distribution of \\(X\\) and the observed values of \\(\\boldsymbol{x}\\). Then the choice for \\(\\theta\\) which maximizes \\(\\mathcal{L}(\\theta|\\boldsymbol{x})\\) is referred to as the maximum likelihood estimator (MLE) for \\(\\theta\\):\n\\[\\hat{\\theta}_\\textit{MLE} = \\mathop{\\mathrm{argmax}}_{\\theta \\in \\mathcal{S}} \\,\\mathcal{L}(\\theta|\\boldsymbol{x})\\]\n\n\n\n\n\nLet \\(\\mathcal{L}(\\theta|\\boldsymbol{x})\\) be the likelihood of a parameter \\(\\theta\\) given a sample \\(\\boldsymbol{x}\\) and a distributional assumption about the random variable \\(X\\) from which the sample was drawn. Then we denote the log-likelihood of \\(\\theta\\) given \\(\\boldsymbol{x}\\) as \\(\\ell(\\theta|\\boldsymbol{x})\\) and define it as\n\\[\\ell(\\theta│\\boldsymbol{x})=\\log{\\mathcal{L}(\\theta│\\boldsymbol{x})}\\]\n\n\n\n\nExample 1: Coin flips\nSuppose that we were given a coin and told it was fixed to land on one side more than the other. We flip the coin 50 times and record each ‘heads’ as 1 and each ‘tails’ as 0. The results below show that the coin landed ‘heads’ 27 times and ‘tails’ 23 times.\n\\[\\boldsymbol{x}=\\left\\{ \\begin{aligned} 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, \\\\ 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0 \\; \\end{aligned} \\right\\}\\]\nLet us make a distributional assumption: the coin data can be modeled by a Bernoulli distribution. We cannot know whether this is correct or not, but it seems reasonable: Bernoulli trials require exactly two outcomes, a fixed probability of success, and independence between trials. While it’s possible that the coin could land on its edge, or that it deforms over time, or that it shows serial correlation, modeling the coin flips as Bernoulli trials seems true enough to be useful.1\nWhat is the probability of observing the data if the coin truly lands heads 60% of the time? You should be able to answer this from your past lessons in probability and statistics. Since we assume trials to be independent, we can write:\n\\[\\begin{align}P(\\boldsymbol{x} | p = 0.6) &= \\prod_{i = 1}^{50} P(x_i | p = 0.6) = 0.4 \\cdot 0.4 \\cdot 0.6 \\cdot \\ldots \\cdot 0.4 \\\\ &= 0.6^{27} \\cdot 0.4^{23} \\approx 7.2×10^{-16}\\end{align}\\]\nA very small number… although these results are actually quite unextraordinary, there are simply so many ways that 50 coin flips can occur (1.13 quadrillion ways) that even the most common sequences each have a very, very low probability.\nWhat is the likelihood for any given parameter \\(p\\), given our dataset of 27 heads in 50 flips?\n\\[\\mathcal{L}(p|\\boldsymbol{x}) = \\prod_{i=1}^{50} P(x_i|p) = p^{27} (1-p)^{23}\\]\nWhat is the value of \\(p\\) which maximizes this likelihood? It’s not immediately evident from the equation above, but perhaps the log-likelihood will help us to solve for \\(p\\):\n\\[\\ell(p|\\boldsymbol{x}) = \\log{\\mathcal{L}(p|\\boldsymbol{x})} = 27\\log{⁡p}+23\\log⁡(1-p)\\]\nThis is still difficult to solve by hand, so let’s bring in the final trick, and instead try to find the root of the first derivative of the log-likelihood:\n\\[\\frac{d}{dp} \\ell(p│\\boldsymbol{x}) = \\frac{27}{p}-\\frac{23}{1-p}\\]\nSetting the above equal to zero and solving for p,\n\\[\\begin{align} \\frac{27}{p}-\\frac{23}{1-p} = 0 & \\Longrightarrow \\frac{27}{p} = \\frac{23}{1-p} \\Longrightarrow 27-27p = 23p \\\\ & \\Longrightarrow 50p=27 \\Longrightarrow \\hat{p}_\\textit{MLE} = \\frac{27}{50}=0.54 \\end{align}\\]\nIn fact, we could abstract a little further here to find the MLE for any Bernoulli-distributed sample. Let \\(k\\) be the number of successes and \\(n\\) be the total number of trials. Using the same math as above, you will find that:\n\\[\\hat{p}_\\mathit{MLE} = k/n = \\frac{\\sum_i x_i}{n} = \\bar{x}\\] This is a tidy little result. When our data are Bernoulli distributed, then the maximum likelihood estimator for the parameter \\(p\\) is simply the sample average, i.e. the proportion of observations that were successes. I like findings such as these which conform with our intuition: when we have data on a Bernoulli process, our best guess as to how often successes truly happen will simply be how often successes occurred in our data.\n\n\nExample 2: Heights\nThe above example used a very simple discrete distribution with a single parameter. Let’s try again with a more complicated continuous distribution, which uses two parameters. Across the entire world population, heights are not exactly distributed according to any known distribution. However, among otherwise homogeneous populations, we do observe that heights are roughly normally distributed. Let’s pretend that we sampled 10 adult women in the United States and measured their heights. Rounded to the nearest tenth of a centimeter, their heights are listed below:2\n\\[\\boldsymbol{x}=\\{170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3\\}\\] Let us assume that these heights are drawn from a normal distribution. What then would be the best guess for the parameters \\(\\mu\\) and \\(\\sigma^2\\), which are the mean and variance of the distribution? We will start by finding the likelihood function. Recall that if \\(X\\) is normal,\n\\[f_X(x|\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\\] From this, we can compute the likelihood of any pair of normal parameters for any dataset:\n\\[\\mathcal{L}(\\mu,\\sigma^2|\\boldsymbol{x}) = (2\\pi\\sigma^2)^{-n/2}\\cdot e^{-\\frac{1}{2\\sigma^2}\\sum_{i}(x_i-\\mu)^2}\\] Then, we can find the log-likelihood:\n\\[\\ell(\\mu,\\sigma^2|\\boldsymbol{x}) = \\log{\\mathcal{L}(\\mu,\\sigma^2|\\boldsymbol{x})} = -\\frac{n}{2}\\log{2\\pi\\sigma^2} - \\frac{1}{2\\sigma^2} \\sum_{i}(x_i-\\mu)^2\\]\nNext, we will take the derivative with respect to \\(\\mu\\):\n\\[\\frac{\\partial\\ell}{\\partial\\mu} = \\frac{1}{\\sigma^2} \\sum_{i}(x_i-μ) \\] From here we will solve for the root of the derivative, which will be the value of \\(\\mu\\) that maximizes the original likelihood function:\n\\[\\begin{align} \\frac{\\partial\\ell}{\\partial\\mu} = 0 & \\Longrightarrow \\sum_i (x_i-\\mu) = 0 \\Longrightarrow \\sum_i x_i - n\\mu = 0 \\Longrightarrow n\\mu=\\sum_i x_i \\\\ & \\Longrightarrow \\hat{\\mu}_\\textit{MLE} = \\bar{x} \\end{align}\\]\nWhat a fantastic bit of luck. The best guess for the true mean of a normal distribution is the sample mean of our data! Let’s finish up by repeating for variance: first we take the partial derivative of the log-likelihood with respect to \\(\\sigma^2\\):\n\\[\\frac{\\partial\\ell}{\\partial\\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_i (x_i-\\mu)^2\\] Then we will solve for the root of the derivative, which will be the value of \\(\\sigma^2\\) which maximizes the original likelihood function:\n\\[\\begin{align} \\frac{\\partial\\ell}{\\partial\\sigma^2} =0 & \\Longrightarrow \\frac{1}{2\\sigma^4} \\sum_i (x_i-\\mu)^2 = \\frac{n}{2\\sigma^2} \\Longrightarrow \\frac{1}{\\sigma^2} \\sum_i (x_i-\\mu)^2 = n \\\\ & \\Longrightarrow \\hat{\\sigma}^2_\\textit{MLE} = \\frac{1}{n} \\sum_i (x_i-\\mu)^2 \\end{align}\\]\nYou may recognize this quantity as the biased (uncorrected) sample variance. Although this calculation seems very sensible, we will later show that it systematically underestimates the true variance \\(\\sigma^2\\), which provides our first hint that maximum likelihood estimation is not the final answer for every problem we will encounter.\nWith these results in hand, we can produce the MLEs for our normal parameters given our height data. We would say that the best guess for the true mean height of adult women in the United States is 163.5 cm and the best guess for their variance would be 50.4 cm\\({}^2\\) (implying a standard deviation of 7.1 cm).3\n\n\nCode\n#generate height data\nheights &lt;- c(170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3)\n\n#generate height parameter ll contour plot\nheight.ll &lt;- function(parms) {-1*length(heights)*log(2*pi*parms[2])/2-sum((heights-parms[1])^2)/(2*parms[2])}\nxgrid &lt;- seq(140,180,0.1)\nygrid &lt;- seq(25,80,0.1)\nzgrid &lt;- matrix(apply(cbind(rep(xgrid,times=length(ygrid)),rep(ygrid,each=length(xgrid))),\n                      1,height.ll),ncol=length(ygrid))\nfilled.contour(xgrid,ygrid,zgrid,levels=c(-120,-100,-80,-70,-60,-50,-40,-35,-34,-33.8,-33.7),\n               col=paste0('#000000',c('00','20','40','60','80','9f','af','cf','ef','ff')),\n               main='Log-Lik of Parameters for Height Data',\n               xlab='Mu (mean height, in cm)',ylab='Sigma^2 (variance)')\n\n\n\n\n\n\n\n\nFigure 1.3: Contour plot of height data log-likelihoods\n\n\n\n\n\nWe can confirm that these solutions are reasonable by plotting the log-likelihood of various combinations of mean and variance, seen above. Notice that a broad range of possible means and variances have log-likelihoods close to the maximum value of -33.8. Any of these combinations could easily have generated our data. But if we have to make one guess, then the MLE values of (163.5, 50.4) would be our best choice.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#visualizer",
    "href": "likelihood.html#visualizer",
    "title": "Univariate likelihood",
    "section": "Visualizer",
    "text": "Visualizer\nThis visualizer shows an exponential distribution, which is often used to model the waiting times between events. You can create a sample by adjusting the rate parameter \\(\\lambda\\) (e.g. \\(\\lambda=5\\) might mean an average of five events per hour), and the total sample size. Then the visualizer will plot the raw data (each waiting time) as well as the likelihood, log-likelihood, and derivative of the log-likelihood function for the parameter \\(\\lambda\\).\nNotice how the parameter value made most likely by your data will never quite “right” (i.e. equal to the true parameter.) But as sample size increases, the estimates usually get closer to the true value, and the likelihood function develops a very narrow “peak” around our estimate, meaning that the other values are not made very likely by our data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nlibrary(shiny)\nlibrary(bslib)\n\nll &lt;- function(lambda,x) length(x)*log(lambda) - lambda*sum(x)\nl &lt;- function(lambda,x) exp(ll(lambda,x))\ndll &lt;-function(lambda,x) length(x)/lambda - sum(x)\nx.axis &lt;- seq(0.05,6.0,0.05)\n\nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Likelihood for an Exponential sample\",\n  fluidRow(column(width=6,sliderInput(\"lambda\", \"Lambda (rate)\", min=1, max=5, value=3)),\n           column(width=6,sliderInput(\"nsamp\", \"N (sample size)\", min=10, max=1000, value=100))),\n  fluidRow(column(width=6,plotOutput(\"distPlot1\")),\n           column(width=6,plotOutput(\"distPlot2\"))),\n  fluidRow(column(width=6,plotOutput(\"distPlot3\")),\n           column(width=6,plotOutput(\"distPlot4\"))))\n\nserver &lt;- function(input, output) {\n  x &lt;- reactive({rexp(n=input$nsamp,rate=input$lambda)})\n  output$distPlot1 &lt;- renderPlot(hist(x(),main='Histogram of Data',xlab='Waiting time',ylab='Frequency'))\n  output$distPlot2 &lt;- renderPlot(plot(x.axis,l(x.axis,x()),main='Likelihoods for Lambda',xlab='Lambda',ylab='Likelihood',type='l',xlim=c(0,6)))\n  output$distPlot3 &lt;- renderPlot(plot(x.axis,ll(x.axis,x()),main='Log-likelihoods for Lambda',xlab='Lambda',ylab='Log-likelihood',type='l',xlim=c(0,6)))\n  output$distPlot4 &lt;- renderPlot({plot(x.axis,dll(x.axis,x()),main='First Derivative of LL for Lambda',xlab='Lambda',ylab='dLL/dLambda',type='l',xlim=c(0,6)); abline(v=0,lty=2)})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#footnotes",
    "href": "likelihood.html#footnotes",
    "title": "Univariate likelihood",
    "section": "",
    "text": "You may recall the statistician George Box’s maxim: all models are wrong, some are useful.↩︎\nFor the stubborn imperial-unit diehards among us, these heights range from five feet (60”) to five feet nine inches (69”).↩︎\nOr for provincial bumpkins like myself: 64 in, 20 in\\({}^2\\), and 3 in.↩︎",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate likelihood</span>"
    ]
  },
  {
    "objectID": "hypothesistest.html",
    "href": "hypothesistest.html",
    "title": "Hypothesis tests",
    "section": "",
    "text": "Two-tailed hypothesis tests\nMaximum likelihood estimation computes the best guess for the true location of an unknown parameter, but unfortunately these guesses are almost never correct, and guesses for the same parameter usually fluctuate by small amounts from sample to sample. Put another way, each sample usually misleads us by a little bit — even carefully gathered samples which meet all our assumptions. Therefore, we should not only learn methods of identifying the best estimate for a parameter, but methods for identifying whether a specific estimate is reasonable, or indeed, methods which help us to identify all reasonable estimates for the parameter.\nWe will first consider the case of asking whether one specific estimate could be a reasonable guess for the true parameter(s).\nConsider again the coin data from before: 27 heads and 23 tails from 50 trials. Maximum likelihood estimation would say that the best guess for the true probability of heads would be the sample average, 54%. But I believe that if you had a real coin in your hands, and you flipped it 50 times and observed 27 heads, you would not say to yourself, “Surely, this coin must slightly favor heads.” You probably assumed it was a fair 50-50 coin before you flipped it, and you probably didn’t change your mind after you flipped it. Intuitively, we know that even fair coins produce unfair sample averages.1 Yes, it’s true that an unfair 55-45 coin might often produce 27 heads from 50 trials. But so would a fair 50-50 coin, and so our data do not rule out the possibility of a fair coin.\nWe can even compute the exact probability that a fair 50-50 coin would create results like our data. When I say “like our data”, I will include all results that are as far-from-fair as our data, or even farther-from-fair. I want to show you that fair coins can not only produce our exact data, but can sometimes produce even stranger results.2 These extreme results would include 27 heads (like our data), but also 28, 29, …, up to 50 heads. They also include 23 heads, since 23 heads (and 27 tails) from 50 flips of a fair coin would be just as unusual as 23 tails (and 27 heads). Likewise, 22 heads, 21, …, down to 0 heads. If we assume that the number of heads is binomially distributed, we can write:\n\\[\\begin{align} P(X \\le 23 \\textit{ or } X \\ge 27) & = 2 \\cdot P(X \\le 23) = 2 \\cdot \\sum_{i=0}^{23} \\left( \\begin{array}{c} 50 \\\\ i \\end{array} \\right) 0.5^{50} \\\\ & \\approx 0.672 \\end{align}\\]\nSo you can see that perfectly fair coins will behave at least as “unfairly” as our data more than two times out of three!3\nCode\n#coin hypothesis test\nplot(0:50,dbinom(0:50,50,0.5),type='h',lwd=4,xlim=c(10,40),\n     main='PMF of Fair Coin Flipped 50 Times',col='blue',\n     xlab='Number of heads',ylab='Probability')\npoints(24:26,dbinom(24:26,50,0.5),type='h',lwd=4)\ntext(27,dbinom(27,50,0.5),'x',pos=3)\nlegend(x='topright',legend=expression('Total Mass'%~~%0.672),\n       fill='blue')\n\n\n\n\n\n\n\n\nFigure 2.1: Two-tailed hypothesis test on the coin data, assuming a fair coin\nThe chain of reasoning we just followed is known as a hypothesis test. Above, you can see a figure which illustrates the same logic through a graph: the probability mass function (PMF) of a binomial variable where \\(N=50\\) and \\(p=0.5\\). This corresponds to our calculation of how the heads and tails will be distributed, under our assumption that the coin is fair. The maroon-colored lines are the probabilities for every outcome as extreme as our data (27 heads, marked ‘x’), or more extreme than our data. They sum to 0.672.\nIn truth, I did not strictly follow the steps of a formal hypothesis test, which I present below:\nEach of these steps involve many nuanced considerations, more than can be easily expressed in this short document. For the moment, I might simply say that hypothesis testing is incredibly misunderstood and misused throughout not just the broader scientific community (including data science), but even within the statistical community, which you might presume to know better. What I have presented here is a woefully incomplete tale, but we all must start somewhere. With those caveats, allow me a few further remarks:",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis tests</span>"
    ]
  },
  {
    "objectID": "hypothesistest.html#two-tailed-hypothesis-tests",
    "href": "hypothesistest.html#two-tailed-hypothesis-tests",
    "title": "Hypothesis tests",
    "section": "",
    "text": "Let \\(X\\) be a random variable from a distribution but with one or more unknown parameters \\(\\theta\\), and let \\(\\boldsymbol{x}\\) be data sampled from \\(X\\). To perform a two-tailed hypothesis test, perform the following steps:\n\nDefine the null hypothesis, \\(H_0: \\theta = \\theta_0\\)\nDefine the alternative hypothesis, \\(H_1: \\theta \\ne \\theta_0\\)\nDetermine a significance threshold \\(\\alpha\\) which represents the maximum acceptable risk of falsely rejecting the null hypothesis in cases where it was actually true\nDerive a test statistic \\(T=f(\\cdot | \\theta_0)\\), a function of the sample which has a known probability distribution when the null hypothesis is true, and calculate \\(t_\\boldsymbol{x}=f(\\boldsymbol{x}|\\theta_0)\\)\nCompute the p-value of the test, which is the probability of observing a test statistic at least as extreme as observed in \\(\\boldsymbol{x}\\) — typically, \\(P(T \\gt t_\\boldsymbol{x})\\) or \\(P(T \\lt t_\\boldsymbol{x})\\)\nIf the p-value is less than or equal to \\(\\alpha\\) then reject \\(H_0\\), since the data make it too unlikely; otherwise, we do not have enough evidence to reject \\(H_0\\)\n\n\n\n\nSometimes we choose a null hypothesis \\(H_0\\) which we genuinely wish to evaluate as a possible truth about the world. Other times we choose a “straw man” null hypothesis which we do not seriously believe, in order to establish a more realistic alternative. These two cases are both legitimate avenues for scientific inquiry.\nNotice that the alternative hypothesis will frequently be a composite hypothesis, containing an infinitude of possibilities. Therefore, we only say that we “reject the null hypothesis”, never that we “accept the alternative hypothesis” — there are many alternative hypotheses, and some of them are even less likely to be true than our null hypothesis. We have done nothing to identify the ‘right’ alternative hypothesis, we have only ruled out the null hypothesis.\nThe significance threshold should be chosen before calculating your p-value, or else all this scientific rigor evaporates, leaving us with nothing more than a gut check. The most common significance threshold is \\(\\alpha = 0.05\\), but it’s best practice to choose a threshold that is actually appropriate to the context of your problem. I have often used thresholds as low as \\(\\alpha = 0.001\\) and as high as \\(\\alpha = 0.2\\) in my career.\nConsider the costs of wrongly rejecting the null hypothesis very carefully, and weigh them against the costs of wrongly failing to reject the null hypothesis. For example, a diagnostic test for cancer creates very different problems from a false positive versus a false negative.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis tests</span>"
    ]
  },
  {
    "objectID": "hypothesistest.html#one-tailed-hypothesis-tests",
    "href": "hypothesistest.html#one-tailed-hypothesis-tests",
    "title": "Hypothesis tests",
    "section": "One-tailed hypothesis tests",
    "text": "One-tailed hypothesis tests\nThe steps listed above document a two-sided hypothesis test, but sometimes we have an asymmetric problem to consider which suggests a one-tailed hypothesis test. As an example, let’s re-use the same coin data (27 heads, 23 tails), but invent a different story: Pretend that you have bought a “trick” coin from a magician, who promises that it lands tails much more often than heads. The magician isn’t sure of the precise probabilities, but guarantees that the coin lands heads no more than 40% of the time. Skeptical of the magician’s claims, you flip the coin 50 times and observe 27 heads. Could the magician be telling the truth? Let’s test this hypothesis through the following steps:\n\nWe define the null hypothesis, \\(H_0:p \\le 0.4\\). Notice that unlike the two-sided test, our null hypothesis is now composite, including many different possible truths.\nWe define the alternative hypothesis, \\(H_a:p \\gt 0.4\\). Just like the two-sided test, our null and alternative hypotheses are exhaustive and mutually exclusive: exactly one of them will always be true.\nWe set a significance threshold; here I will use the classic \\(\\alpha=0.05\\) since this example is made up and there are no real consequences to being wrong.\nWe derive a test statistic under the null hypothesis — here, simply the number of heads, which follows the binomial distribution. Our null contains multiple candidates for \\(p\\), but as the data suggest \\(p \\gt 0.4\\), we will assume \\(p=0.4\\), since every other parameter choice within the null hypothesis will result in even lower \\(p\\)-values.\nWe compute a \\(p\\)-value, that is, the probability of observing a test statistic at least as extreme as our data:\n\n\\[P(x \\ge 27 | p=0.4) = \\sum_{i=27}^{50} \\left(\\begin{array}{c} 50 \\\\ i \\end{array} \\right) 0.4^i\\ 0.6^{50-i} \\approx 0.031\\]\n\nSince our p-value of 0.031 is less than our significance threshold of 0.05, we reject the null hypothesis and conclude that we were swindled by the magician. We cannot be certain, but we know that “trick” coins like the one described create datasets like our own only 3% of the time.\n\nThe figure below illustrates this test, and shows why the two variants are called “two-tailed” and “one-tailed”. Two-tailed test evaluates all possible data samples which are at least as extreme as our actual data — these samples might support parameters less than or greater than \\(\\theta_0\\), and they fall in both the left and right “tails” of the test distribution. One-tailed tests evaluate only one side of those extreme samples:\n\n\nCode\n#coin hypothesis test\nplot(0:50,dbinom(0:50,50,0.4),type='h',lwd=4,xlim=c(10,40),\n     main='PMF of True p=0.4 Coin Flipped 50 Times',col='blue',\n     xlab='Number of heads',ylab='Probability')\npoints(0:26,dbinom(0:26,50,0.4),type='h',lwd=4)\ntext(27,dbinom(27,50,0.4),'x',pos=3)\nlegend(x='topright',legend=expression('Total Mass'%~~%0.031),\n       fill='blue')\n\n\n\n\n\n\n\n\nFigure 2.2: One-tailed hypothesis test of the coin data, assuming a trick coin",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis tests</span>"
    ]
  },
  {
    "objectID": "hypothesistest.html#footnotes",
    "href": "hypothesistest.html#footnotes",
    "title": "Hypothesis tests",
    "section": "",
    "text": "If you disagree, I wonder what you think would happen if we flipped a fair coin five times…↩︎\nThis reasoning is very important for continuous distributions, where we may never observe the same data twice. But it also applies to discrete distributions: when we have many observations, the probability of any specific sequence of data can be very, very small.↩︎\nThe formula above makes use of the ‘combination’ operator, familiar to some readers but perhaps not others. In short, \\(({\\scriptstyle \\begin{array}{cc} N \\\\ k \\end{array}}) = N!/(N-k)!k!\\), with “!” representing the factorial symbol, whereby \\(k! = k \\cdot (k-1) \\cdot \\ldots \\cdot 2 \\cdot 1\\). The classic use case of combinations is to count the ways of selecting unordered sets of \\(k\\) objects from \\(N\\) total possibilities.↩︎",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis tests</span>"
    ]
  },
  {
    "objectID": "biasvariance.html",
    "href": "biasvariance.html",
    "title": "Estimator bias and variance",
    "section": "",
    "text": "Visualizer: Normal estimation\nNot all estimators are created equal. Some estimators are worse than others, and they can be better or worse in different ways. Let’s pretend we have a normally distributed population (such as people’s heights, or stock returns), and we would like to estimate the true mean of the population \\(\\mu\\) from a sample of 100 observations, \\(\\boldsymbol{x}\\). We have already learned that the maximum likelihood estimator for the true mean is \\(\\bar{x}\\), the sample mean. What if I forced you to pick one of two alternative estimators:\n\\[\\begin{array}{ll} \\hat{\\mu}_1 = & \\frac{\\min(\\boldsymbol{x}) + \\max(\\boldsymbol{x})}{2} \\\\ \\hat{\\mu}_2 = & x_{(52)},\\, \\textrm{the }52^{nd}\\text{ observation (from smallest to largest)}\\end{array}\\]\nYou may not yet be an expert statistician, but you can probably guess something about the second estimator: \\(\\hat{\\mu}_2\\) will generally overestimate the true mean. The 50th or 51st observation out of 100 would generally be more central.\nIn the figure above, I simulated 100,000 different samples of 100 observations each, and plotted a histogram of how far from the true mean these two estimators were each time. You can see that while the 52nd ordered observation tends to slightly overestimate the true mean, it usually produces a much closer estimate than the other estimator (the straight average of the sample min and max). In fact, in almost three-quarters of the simulations, \\(\\hat{\\mu}_2\\) produced a closer estimate of the true mean than \\(\\hat{\\mu}_1\\). We call these concepts the bias and the variance of an estimator. Bias measures how much the estimator systematically overestimates or underestimates the true parameter. Variance measures how much its estimates change sample to sample.\nThe sport of archery provides a useful analogy for illustrating bias and variance. Imagine different archers (estimators) firing at the target. They are all aiming for the same goal (the true value of the parameter), but they can be off in different ways. One might have a tight grouping of shots that are all off-target in the same way (bias). Another might be hitting every corner of the target but, on average, is neither too high nor too low (variance).\nThere are other properties, such as consistency or efficiency, which can make an estimator desirable or undesirable, but the variance and bias of an estimator are generally the two most-discussed properties.1 They relate to a third metric which may be familiar from a machine learning perspective: mean squared error (MSE).\n\\[\\begin{array}{rl} \\mathrm{MSE}(\\hat{\\theta}) = & \\mathbb{E}[(\\hat{\\theta} - \\theta)^2] \\\\ = & \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] + \\mathbb{E}[\\hat{\\theta}] - \\theta)^2] \\\\ = & \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2 + 2 (\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}]) (\\mathbb{E}[\\hat{\\theta}] - \\theta) + (\\mathbb{E}[\\hat{\\theta}] - \\theta)^2] \\\\ = & \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])]^2 + 2\\, \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}]) (\\mathbb{E}[\\hat{\\theta}] - \\theta)] + \\mathbb{E}[(\\mathbb{E}[\\hat{\\theta}] - \\theta)^2] \\\\ = & \\mathbb{V}[\\hat{\\theta}] + 2\\, \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}]) (\\mathbb{E}[\\hat{\\theta}] - \\theta)] + \\mathrm{Bias}(\\hat{\\theta})^2 \\\\ = & \\mathbb{V}[\\hat{\\theta}] + 2\\,(\\mathbb{E}[\\hat{\\theta}] - \\theta)\\, \\mathbb{E}[\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}]] + \\mathrm{Bias}(\\hat{\\theta})^2 \\\\ = & \\mathbb{V}[\\hat{\\theta}] + \\mathrm{Bias}(\\hat{\\theta})^2 \\end{array}\\]\nEssentially, the mean squared error of an estimator can always be decomposed into a systematic bias component and an idiosyncratic variance component. Between two models with the same MSE, a lower variance implies a higher bias and vice versa. This property is called the bias-variance tradeoff and will affect our modeling work in later sections.\nThere are times we prefer maximum likelihood estimators (MLEs) and times we do not. Consider estimating the mean \\((\\mu)\\) and standard deviation \\((\\sigma^2)\\) of a normal distribution from a sample. I will set true values of \\(\\mu = 100\\) and \\(\\sigma = 20\\). You can choose a sample size, and I will show you how various estimators perform among 10,000 simulated samples of that size.\nWe have many choices for an estimator of the mean, \\(\\mu\\). The MLE is the sample average, \\(\\bar{x}\\), but we could also choose the median of \\(\\boldsymbol{x}\\). Note that both estimators are unbiased, but that the sample average has lower variance, and that the effect grows for large sample sizes.\nWe have fewer sensible estimators for the standard deviation, \\(\\sigma\\). The MLE is the ‘uncorrected’ sample standard deviation, \\(\\sqrt{\\sum_i (x_i - \\bar{x})^2 / n}\\). However, you can see that when sample sizes are small, this estimator is biased; it underestimates the true standard deviation. A slight bias remains even at large sample sizes, but would be hard for the eye to detect. The ‘corrected’ sample standard deviation, \\(\\sqrt{\\sum_i (x_i - \\bar{x})^2 / (n - 1)}\\), remains unbiased and we usually choose this estimator even though it is not the MLE solution.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>*Estimator bias and variance*</span>"
    ]
  },
  {
    "objectID": "biasvariance.html#footnotes",
    "href": "biasvariance.html#footnotes",
    "title": "Estimator bias and variance",
    "section": "",
    "text": "Briefly, a consistent estimator produces better and better estimates as your sample size increases, while an efficient estimator makes the best possible use of the sample’s information.↩︎",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>*Estimator bias and variance*</span>"
    ]
  },
  {
    "objectID": "confidenceintervals.html",
    "href": "confidenceintervals.html",
    "title": "Exact confidence intervals",
    "section": "",
    "text": "Motivation\nWe have learned how to identify the best guess for an unknown parameter from the data. We have also learned how to check if a specific alternative is a reasonable guess, or if the data make it too unlikely to be true. Now we shall learn how to find the range of all reasonable guesses for an unknown parameter. This range is called a confidence interval, because the size of the range depends on how confident we are that it will contain the true parameter:\nYou are the one who decides how confident to be. Combining your own risk tolerance with the precise nature of the problem at hand, you choose a confidence level, usually a number close to 100%. Then you find the upper and lower bounds for your interval.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exact confidence intervals</span>"
    ]
  },
  {
    "objectID": "confidenceintervals.html#footnotes",
    "href": "confidenceintervals.html#footnotes",
    "title": "Exact confidence intervals",
    "section": "",
    "text": "This particular method is known as the Clopper-Pearson interval. But as if to illustrate the confusion on this topic, it is neither the only “exact” calculation nor the one with the best performance!↩︎\nIn practice it’s incredibly rare to know the variance without knowing the mean. This will be addressed in The Student’s t distribution.↩︎\nE.g. if we wanted a 95% confidence interval, then we would set \\(\\alpha = 0.05\\).↩︎\nE.g. if we are computing a 95% confidence interval, samples so large would only be seen 2.5% of the time.↩︎\nPulsars are star-like objects which emit energy at extremely regular intervals, making them very valuable for mapping the universe, terrestrial timekeeping, and interstellar navigation.↩︎",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exact confidence intervals</span>"
    ]
  },
  {
    "objectID": "identification.html",
    "href": "identification.html",
    "title": "Identifying distributions",
    "section": "",
    "text": "Using what you and others know about the data\nThe prior sections have all assumed that our data comes to us from a known distribution, such as the normal or the binomial, and that all we need to do is find the specific parameters of the distribution. But in the real world, data are rarely labeled with their distribution name. Often, the data don’t even come from a named distribution at all. How can we be sure that we have chosen the right distribution with which to model our data?\nThe first step is to narrow down your list of candidate distributions. Ideally you will be in one of two scenarios:\nHowever the world is not an ideal place, and so sometimes we will choose a distribution not because it is “correct” but because it is “good enough” for our data. Remember also that your data may not be distributed neatly. Your data may include different subsamples, each with their own distribution. Your data might also be distributed a certain way only after accounting for other variables, which we will consider in future chapters.\nIf you are unsure of where to start, try asking the following questions:",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Identifying distributions</span>"
    ]
  },
  {
    "objectID": "identification.html#using-what-you-and-others-know-about-the-data",
    "href": "identification.html#using-what-you-and-others-know-about-the-data",
    "title": "Identifying distributions",
    "section": "",
    "text": "The data you’re examining has been studied before, and other researchers have described it as being distributed a certain way. For example, daily stock returns are often modeled as being normal or lognormal. Lightbulb lifetimes are often modeled as exponentially distributed. (Stay cautious: there may be important differences between the earlier research and the data in front of you.)\nThe data you’re examining comes from a generating process with known physical properties suggesting certain distributions. For example, even if we are not familiar with astronomical literature, we can reason for ourselves that large meteors (\\(\\gt\\) 1 km diameter) might strike the Earth according to a Poisson process.\n\n\n\n\nDoes your data only take integer values (or is it easily transformed to take only integer values)? If so, consider a discrete distribution.\n\nBut what if the integers are all very large, like stadium attendance figures? Depending on your task, continuous distributions might work just as well.\nWhat if the general scale for the integers is different in each observation, such as smoking deaths by state?1 Perhaps it would be better to model the rate (e.g. deaths per 100,000 residents) as a continuous distribution, rather than modeling the count as a discrete distribution.\nCan the integers be negative? If so, you might need to transform the data, since many discrete distributions don’t permit negative values.\n\nAre the data naturally bounded above and/or below? Some distributions like the uniform or exponential might have matching bounds, others like the normal distribution do not have theoretical bounds, just practical limits where it would be rare to see any data.\nAre your data spread out over many degrees of magnitude? If so, a logarithmic transformation or the lognormal distribution might be useful, or again finding a way to express these figures relative to some baseline which varies by observation.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Identifying distributions</span>"
    ]
  },
  {
    "objectID": "identification.html#empirical-distribution-functions-and-the-kolmogorov-smirnov-test",
    "href": "identification.html#empirical-distribution-functions-and-the-kolmogorov-smirnov-test",
    "title": "Identifying distributions",
    "section": "Empirical distribution functions and the Kolmogorov-Smirnov test",
    "text": "Empirical distribution functions and the Kolmogorov-Smirnov test\nProbability distributions are often plotted with very precise and elegant curves, brimming with mathematical truths and delicate nuances. And then there is your data: shaped like a mess, coarsely finite, and probably full of unhelpful outliers and misleading observations. But we can still plot the data as though it were a probability distribution, and learn from it:\n\nLet \\(\\boldsymbol{x}\\) be a sample of \\(n\\) observations. Define the indicator function \\(1\\!\\!1\\{x_i \\le t\\}\\) as 1 when an observation from \\(\\boldsymbol{x}\\) is less than or equal to some threshold value \\(t\\), and 0 otherwise. Then the empirical cumulative distribution function of \\(\\boldsymbol{x}\\) on a support of all \\(t \\in \\mathbb{R}\\) is:\n\\[\\hat{F}(t)=\\frac{1}{n} \\sum_{i} 1\\!\\!1\\{x_i \\le t\\}\\]\n\nThis definition provides us with a sample-based analog to the theoretical CDF of a probability distribution — which is why I use the notation \\(\\hat{F}(t)\\), to emphasize that it estimates some unknown cumulative distribution \\(F(t)\\). In essence, the empirical distribution function simply plots the ranks of data, or the quantiles of the data if you prefer that term. Below, I plot the empirical distribution function for 50 simulated heights next to a theoretical normal distribution which may or may not have generated the sample:\n\n\nCode\n#empirical cdf\npar(mfrow=c(1,2))\nset.seed(1977)\nhgt50 &lt;- round(rnorm(50,165,7),1)\nplot(ecdf(hgt50),do.points=FALSE,main=expression('Empirical CDF of 50 Heights'),\n     xlab='Height (cm)',ylab='ECDF',xlim=c(146,184))\nplot(seq(146,184,0.1),pnorm(seq(146,184,0.1),162,7),type='l',\n     col='#0000ff',main=expression(paste('Theoretical CDF of Norm(162,',7^2,')')),\n     ylab='Cumul. Prob.',xlab='Height (cm)',xlim=c(146,184))\n\n\n\n\n\n\n\n\nFigure 5.1: Empirical and Theoretical CDFs\n\n\n\n\n\nYou can see these two graphs resemble each other. We may be tempted to say that the sample came from the normal distribution, or at least that it could be normal. But we need not settle for visual approximations: we can use a hypothesis test specifically developed for this situation, which will more precisely compare these two graphs. The Kolmogorov-Smirnov test, or K-S test, was developed to help statisticians answer two similar questions:\n\nCould a sample \\(\\boldsymbol{x}\\) have been generated according to a theoretical distribution \\(X\\)?\nCould two samples, \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\), be generated by the same unknown distribution?\n\nIt works by comparing the ECDF of one sample against either the theoretical CDF of a known distribution or against the ECDF of the second sample, and finding the maximum vertical difference between the two functions. Andrey Kolmogorov showed that under the null hypothesis that the two distributions are equal, this maximum discrepancy follows a novel probability distribution.\n\n\nCode\n#k-s test\nkst &lt;- suppressWarnings(ks.test(hgt50,pnorm,mean=162,sd=7))\nFhgt &lt;- sapply(seq(146,184,0.1),function(z){mean(hgt50&lt;=z)})\nFnorm &lt;- pnorm(seq(146,184,0.1),162,7)\nplot(ecdf(hgt50),do.points=FALSE,main=\n       expression(paste('K-S test of heights against Norm(162,',7^2,')')),\n     xlab='Height (cm)',ylab='Cumul. Prob.',xlim=c(146,184))\nlines(seq(146,184,0.1),Fnorm,col='#0000ff')\nk_max &lt;- order(abs(Fnorm-Fhgt),decreasing=TRUE)[1]\nx_max &lt;- seq(146,184,0.1)[152]\nsegments(x0=x_max,x1=x_max,y0=Fhgt[k_max],y1=Fnorm[k_max],lwd=3)\nlegend(x='topleft',bty='n',lwd=1,col=c('#000000','#0000ff'),\n       legend=c('ECDF of heights','Norm CDF'))\ntext(x=x_max,y=Fhgt[k_max],label='Max diff = 0.115, p-value=0.529',pos=4)\n\n\n\n\n\n\n\n\nFigure 5.2: Graphical illustration of a Kolmogorov-Smirnov test\n\n\n\n\n\nIn the figure above, we see that while the empirical CDF of our 50 heights does not always perfectly match the theoretical CDF of the normal distribution with mean 162 and variance 49, that same theoretical distribution would generate samples of 50 observations that look even stranger over 52% of the time, more than half of all samples. So we cannot rule out that the height data were generated from that theoretical distribution.2\nIn practice, we can use maximum likelihood estimation and the Kolmogorov-Smirnov test as an effective “toggle” to help rapidly propose and confirm possible distributions for our data. Consider using this set of steps whenever you are faced with new univariate data:\n\nIdentify a candidate distribution type, such as normal or binomial\nUse maximum likelihood to find the best possible parameters for the data, given your guess of the distribution type\nUse a K-S test to determine whether that distribution and those parameters are actually a good fit to the data (the best fit is not necessarily a good fit)\nIf so, proceed under the assumption that your data could be distributed that way. If not, go back to step 1, identify a new distribution type, and repeat.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Identifying distributions</span>"
    ]
  },
  {
    "objectID": "identification.html#footnotes",
    "href": "identification.html#footnotes",
    "title": "Identifying distributions",
    "section": "",
    "text": "Keep in mind that California has 80x more residents than Wyoming.↩︎\nSince I generated the data, I will tell you: they are not from that distribution! They are from another!↩︎\nI am adding the ‘prime’ mark next to them to distinguish these log-normal parameters from the normal parameters above.↩︎\nE.g. interpretability, ease of communication, simplicity in cumulating stopping distance with other braking factors, etc.↩︎",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Identifying distributions</span>"
    ]
  },
  {
    "objectID": "dist_normal.html",
    "href": "dist_normal.html",
    "title": "Appendix B — Normal distribution",
    "section": "",
    "text": "Assumptions\nAlthough a few generating processes are provably normal, we mostly use the normal distribution in contexts where it is “close enough”, and do not require any particular assumptions.\nHowever, keep in mind that the normal distribution, at least in theory, is:\nIf the process you are modeling is bounded, discrete, or asymmetrical, then the normal distribution may be a poor fit. Two common exceptions would be:",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "dist_normal.html#assumptions",
    "href": "dist_normal.html#assumptions",
    "title": "Appendix B — Normal distribution",
    "section": "",
    "text": "Unbounded\nContinuous\nSymmetrical\n\n\n\nWhen the distribution is naturally bounded, but most values are observed very far from the bounds (such as the weights of passenger jets, bounded below by 0, or the returns on a stock index, bounded below at -100%)\nWhen the distribution is discrete, but most values are very large or very finely subdivided (such as stadium attendance, or the current value of your bank account)",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "dist_normal.html#definition",
    "href": "dist_normal.html#definition",
    "title": "Appendix B — Normal distribution",
    "section": "Definition",
    "text": "Definition\n\\[\\begin{array}{ll}\n  \\text{Support:} & \\mathbb{R} \\\\\n  \\text{Parameter(s):} & \\mu,\\text{ the mean }(\\mu \\in \\mathbb{R}) \\\\\n  & \\sigma,\\text{ the standard deviation }(\\sigma \\gt 0) \\\\\n  \\text{PDF:} & f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2} \\\\\n  \\text{CDF:} & F_X(x) = \\Phi(\\frac{x-\\mu}{\\sigma})\\quad (\\text{No closed form expression}) \\\\\n  \\text{Mean:} & \\mathbb{E}[X]=\\mu \\\\\n  \\text{Variance:} & \\mathbb{V}[X]=\\sigma^2 \\\\\n\\end{array}\\]",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "dist_normal.html#visualizer",
    "href": "dist_normal.html#visualizer",
    "title": "Appendix B — Normal distribution",
    "section": "Visualizer",
    "text": "Visualizer\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n      tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Normal distribution PDF\",\n  fluidRow(plotOutput(\"distPlot\")),\n  fluidRow(column(width=6,sliderInput(\"mu\", \"Mean (mu)\", min=-10, max=10, value=0)),\n           column(width=6,sliderInput(\"sigma\", \"Std Dev (sigma)\", min=0.01, max=10, value=1))))\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- seq(input$mu-3*input$sigma,input$mu+3*input$sigma,input$sigma/100)\n    y &lt;- dnorm(x,input$mu,input$sigma)\n    xlims &lt;- c(mean(c(-3,x[1])),mean(c(3,x[601])))\n    ylims &lt;- c(0,mean(c(dnorm(0),y[301])))\n    plot(x=x,y=y,main=NULL,xlab='x',ylab='Density',type='l',lwd=2,\n         xlim=xlims,ylim=ylims)\n  })\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "dist_normal.html#properties",
    "href": "dist_normal.html#properties",
    "title": "Appendix B — Normal distribution",
    "section": "Properties",
    "text": "Properties\n\nThe normal distribution with mean \\(\\mu=0\\) and variance \\(\\sigma^2=1\\) is said to be the standard normal distribution and often written as \\(Z \\sim \\mathrm{Norm}(0,1)\\). The CDF of the standard normal distribution and its inverse are often abbreviated as \\(F_X(x)=\\Phi(x)\\) and \\(F_X^{-1}(x)=\\Phi^{-1}(x)\\), respectively.\nIf \\(X\\) is a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then for any constants \\(a,b \\in \\mathbb{R}\\) the transformation \\(aX + b\\) is also a normal random variable with mean \\(a\\mu + b\\) and variance \\(a^2\\sigma^2\\).\nIf \\(X\\) and \\(Y\\) are two independent normal random variables with means \\(\\mu_X, \\mu_Y\\) and variances \\(\\sigma^2_X, \\sigma^2_Y\\), then their sum \\(X+Y\\) is also a normal random variable with mean \\(\\mu_X+\\mu_Y\\) and variance \\(\\sigma^2_X+\\sigma^2_Y\\).\nMore generally, any linear combination of any number of independent normal random variables is itself a normal random variable!",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "dist_normal.html#relations-to-other-distributions",
    "href": "dist_normal.html#relations-to-other-distributions",
    "title": "Appendix B — Normal distribution",
    "section": "Relations to other distributions",
    "text": "Relations to other distributions\n\nThe sum of the squares of \\(n\\) independent standard normal variates is chi-squared distributed with \\(df=n\\): \\[\\sum_{i=1}^n Z_i^2 \\sim \\chi_{(n)}^2\\]\nThe ratio of two standard normal variates has the standard Cauchy distribution, i.e. \\[\\mathrm{For\\ }Z_1,Z_2 \\sim \\mathrm{Norm}(0,1),\\quad \\frac{Z_1}{Z_2} \\sim \\mathrm{Cauchy}(0,1)\\]\nThe standard normal distribution is the limit case for the Student’s t-distribution (as \\(df \\rightarrow \\infty\\)). The standard normal can be used in place of the t-distribution with little loss of accuracy for large \\(df\\).\nThe normal distribution with mean \\(df\\) and standard deviation \\(\\sqrt{2df}\\) closely approximates the chi-squared distribution for large \\(df\\).\nThe Poisson distribution and binomial distribution both form discrete approximations to the normal distribution when either \\(\\lambda\\) is very large (Poisson) or \\(np\\) is very large and \\(p\\) is not near 0 or 1.",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "likelihood.html#example-1-coin-flips",
    "href": "likelihood.html#example-1-coin-flips",
    "title": "Univariate likelihood",
    "section": "Example 1: Coin flips",
    "text": "Example 1: Coin flips\nSuppose that we were given a coin and told it was fixed to land on one side more than the other. We flip the coin 50 times and record each ‘heads’ as 1 and each ‘tails’ as 0. The results below show that the coin landed ‘heads’ 27 times and ‘tails’ 23 times.\n\\[\\boldsymbol{x}=\\left\\{ \\begin{aligned} 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, \\\\ 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0 \\; \\end{aligned} \\right\\}\\]\nLet us make a distributional assumption: the coin data can be modeled by a Bernoulli distribution. We cannot know whether this is correct or not, but it seems reasonable: Bernoulli trials require exactly two outcomes, a fixed probability of success, and independence between trials. While it’s possible that the coin could land on its edge, or that it deforms over time, or that it shows serial correlation, modeling the coin flips as Bernoulli trials seems true enough to be useful.1\nWhat is the probability of observing the data if the coin truly lands heads 60% of the time? You should be able to answer this from your past lessons in probability and statistics. Since we assume trials to be independent, we can write:\n\\[\\begin{align}P(\\boldsymbol{x} | p = 0.6) &= \\prod_{i = 1}^{50} P(x_i | p = 0.6) = 0.4 \\cdot 0.4 \\cdot 0.6 \\cdot \\ldots \\cdot 0.4 \\\\ &= 0.6^{27} \\cdot 0.4^{23} \\approx 7.2×10^{-16}\\end{align}\\]\nA very small number… although these results are actually quite unextraordinary, there are simply so many ways that 50 coin flips can occur (1.13 quadrillion ways) that even the most common sequences each have a very, very low probability.\nWhat is the likelihood for any given parameter \\(p\\), given our dataset of 27 heads in 50 flips?\n\\[\\mathcal{L}(p|\\boldsymbol{x}) = \\prod_{i=1}^{50} P(x_i|p) = p^{27} (1-p)^{23}\\]\nWhat is the value of \\(p\\) which maximizes this likelihood? It’s not immediately evident from the equation above, but perhaps the log-likelihood will help us to solve for \\(p\\):\n\\[\\ell(p|\\boldsymbol{x}) = \\log{\\mathcal{L}(p|\\boldsymbol{x})} = 27\\log{⁡p}+23\\log⁡(1-p)\\]\nThis is still difficult to solve by hand, so let’s bring in the final trick, and instead try to find the root of the first derivative of the log-likelihood:\n\\[\\frac{d}{dp} \\ell(p│\\boldsymbol{x}) = \\frac{27}{p}-\\frac{23}{1-p}\\]\nSetting the above equal to zero and solving for p,\n\\[\\begin{align} \\frac{27}{p}-\\frac{23}{1-p} = 0 & \\Longrightarrow \\frac{27}{p} = \\frac{23}{1-p} \\Longrightarrow 27-27p = 23p \\\\ & \\Longrightarrow 50p=27 \\Longrightarrow \\hat{p}_\\textit{MLE} = \\frac{27}{50}=0.54 \\end{align}\\]\nIn fact, we could abstract a little further here to find the MLE for any Bernoulli-distributed sample. Let \\(k\\) be the number of successes and \\(n\\) be the total number of trials. Using the same math as above, you will find that:\n\\[\\hat{p}_\\mathit{MLE} = k/n = \\frac{\\sum_i x_i}{n} = \\bar{x}\\] This is a tidy little result. When our data are Bernoulli distributed, then the maximum likelihood estimator for the parameter \\(p\\) is simply the sample average, i.e. the proportion of observations that were successes. I like findings such as these which conform with our intuition: when we have data on a Bernoulli process, our best guess as to how often successes truly happen will simply be how often successes occurred in our data.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#example-2-heights",
    "href": "likelihood.html#example-2-heights",
    "title": "Univariate likelihood",
    "section": "Example 2: Heights",
    "text": "Example 2: Heights\nThe above example used a very simple discrete distribution with a single parameter. Let’s try again with a more complicated continuous distribution, which uses two parameters. Across the entire world population, heights are not exactly distributed according to any known distribution. However, among otherwise homogeneous populations, we do observe that heights are roughly normally distributed. Let’s pretend that we sampled 10 adult women in the United States and measured their heights. Rounded to the nearest tenth of a centimeter, their heights are listed below:2\n\\[\\boldsymbol{x}=\\{170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3\\}\\] Let us assume that these heights are drawn from a normal distribution. What then would be the best guess for the parameters \\(\\mu\\) and \\(\\sigma^2\\), which are the mean and variance of the distribution? We will start by finding the likelihood function. Recall that if \\(X\\) is normal,\n\\[f_X(x|\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\\] From this, we can compute the likelihood of any pair of normal parameters for any dataset:\n\\[\\mathcal{L}(\\mu,\\sigma^2|\\boldsymbol{x}) = (2\\pi\\sigma^2)^{-n/2}\\cdot e^{-\\frac{1}{2\\sigma^2}\\sum_{i}(x_i-\\mu)^2}\\] Then, we can find the log-likelihood:\n\\[\\ell(\\mu,\\sigma^2|\\boldsymbol{x}) = \\log{\\mathcal{L}(\\mu,\\sigma^2|\\boldsymbol{x})} = -\\frac{n}{2}\\log{2\\pi\\sigma^2} - \\frac{1}{2\\sigma^2} \\sum_{i}(x_i-\\mu)^2\\]\nNext, we will take the derivative with respect to \\(\\mu\\):\n\\[\\frac{\\partial\\ell}{\\partial\\mu} = \\frac{1}{\\sigma^2} \\sum_{i}(x_i-μ) \\] From here we will solve for the root of the derivative, which will be the value of \\(\\mu\\) that maximizes the original likelihood function:\n\\[\\begin{align} \\frac{\\partial\\ell}{\\partial\\mu} = 0 & \\Longrightarrow \\sum_i (x_i-\\mu) = 0 \\Longrightarrow \\sum_i x_i - n\\mu = 0 \\Longrightarrow n\\mu=\\sum_i x_i \\\\ & \\Longrightarrow \\hat{\\mu}_\\textit{MLE} = \\bar{x} \\end{align}\\]\nWhat a fantastic bit of luck. The best guess for the true mean of a normal distribution is the sample mean of our data! Let’s finish up by repeating for variance: first we take the partial derivative of the log-likelihood with respect to \\(\\sigma^2\\):\n\\[\\frac{\\partial\\ell}{\\partial\\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_i (x_i-\\mu)^2\\] Then we will solve for the root of the derivative, which will be the value of \\(\\sigma^2\\) which maximizes the original likelihood function:\n\\[\\begin{align} \\frac{\\partial\\ell}{\\partial\\sigma^2} =0 & \\Longrightarrow \\frac{1}{2\\sigma^4} \\sum_i (x_i-\\mu)^2 = \\frac{n}{2\\sigma^2} \\Longrightarrow \\frac{1}{\\sigma^2} \\sum_i (x_i-\\mu)^2 = n \\\\ & \\Longrightarrow \\hat{\\sigma}^2_\\textit{MLE} = \\frac{1}{n} \\sum_i (x_i-\\mu)^2 \\end{align}\\]\nYou may recognize this quantity as the biased (uncorrected) sample variance. Although this calculation seems very sensible, we will later show that it systematically underestimates the true variance \\(\\sigma^2\\), which provides our first hint that maximum likelihood estimation is not the final answer for every problem we will encounter.\nWith these results in hand, we can produce the MLEs for our normal parameters given our height data. We would say that the best guess for the true mean height of adult women in the United States is 163.5 cm and the best guess for their variance would be 50.4 cm\\({}^2\\) (implying a standard deviation of 7.1 cm).3\n\n\nCode\n#generate height data\nheights &lt;- c(170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3)\n\n#generate height parameter ll contour plot\nheight.ll &lt;- function(parms) {-1*length(heights)*log(2*pi*parms[2])/2-sum((heights-parms[1])^2)/(2*parms[2])}\nxgrid &lt;- seq(140,180,0.1)\nygrid &lt;- seq(25,80,0.1)\nzgrid &lt;- matrix(apply(cbind(rep(xgrid,times=length(ygrid)),rep(ygrid,each=length(xgrid))),\n                      1,height.ll),ncol=length(ygrid))\nfilled.contour(xgrid,ygrid,zgrid,levels=c(-120,-100,-80,-70,-60,-50,-40,-35,-34,-33.8,-33.7),\n               col=paste0('#000000',c('00','20','40','60','80','9f','af','cf','ef','ff')),\n               main='Log-Lik of Parameters for Height Data',\n               xlab='Mu (mean height, in cm)',ylab='Sigma^2 (variance)')\n\n\n\n\n\n\n\n\nFigure 1.3: Contour plot of height data log-likelihoods\n\n\n\n\n\nWe can confirm that these solutions are reasonable by plotting the log-likelihood of various combinations of mean and variance, seen above. Notice that a broad range of possible means and variances have log-likelihoods close to the maximum value of -33.8. Any of these combinations could easily have generated our data. But if we have to make one guess, then the MLE values of (163.5, 50.4) would be our best choice.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate likelihood</span>"
    ]
  },
  {
    "objectID": "likelihood.html#visualizer-exponential-likelihoods",
    "href": "likelihood.html#visualizer-exponential-likelihoods",
    "title": "Univariate likelihood",
    "section": "Visualizer: Exponential likelihoods",
    "text": "Visualizer: Exponential likelihoods\nThis visualizer shows an exponential distribution, which is often used to model the waiting times between events. You can create a sample by adjusting the rate parameter \\(\\lambda\\) (e.g. \\(\\lambda=5\\) might mean an average of five events per hour), and the total sample size. Then the visualizer will plot the raw data (each waiting time) as well as the likelihood, log-likelihood, and derivative of the log-likelihood function for the parameter \\(\\lambda\\).\nNotice how the parameter value made most likely by your data will never quite “right” (i.e. equal to the true parameter.) But as sample size increases, the estimates usually get closer to the true value, and the likelihood function develops a very narrow “peak” around our estimate, meaning that the other values are not made very likely by our data.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 960\n\nlibrary(shiny)\nlibrary(bslib)\n\nll &lt;- function(lambda,x) length(x)*log(lambda) - lambda*sum(x)\nl &lt;- function(lambda,x) exp(ll(lambda,x))\ndll &lt;-function(lambda,x) length(x)/lambda - sum(x)\nx.axis &lt;- seq(0.05,6.0,0.05)\n\nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Likelihood for an Exponential sample\",\n  fluidRow(column(width=6,sliderInput(\"lambda\", \"Lambda (rate)\", min=1, max=5, value=3)),\n           column(width=6,sliderInput(\"nsamp\", \"N (sample size)\", min=10, max=1000, value=100))),\n  fluidRow(column(width=6,plotOutput(\"distPlot1\")),\n           column(width=6,plotOutput(\"distPlot2\"))),\n  fluidRow(column(width=6,plotOutput(\"distPlot3\")),\n           column(width=6,plotOutput(\"distPlot4\"))))\n\nserver &lt;- function(input, output) {\n  x &lt;- reactive({rexp(n=input$nsamp,rate=input$lambda)})\n  output$distPlot1 &lt;- renderPlot(hist(x(),main='Histogram of Data',xlab='Waiting time',ylab='Frequency'))\n  output$distPlot2 &lt;- renderPlot({plot(x.axis,l(x.axis,x()),main='Likelihoods for Lambda',xlab='Lambda',ylab='Likelihood',type='l',xlim=c(0,6)); abline(v=1/mean(x()),col='#0000ff')})\n  output$distPlot3 &lt;- renderPlot({plot(x.axis,ll(x.axis,x()),main='Log-likelihoods for Lambda',xlab='Lambda',ylab='Log-likelihood',type='l',xlim=c(0,6)); abline(v=1/mean(x()),col='#0000ff')})\n  output$distPlot4 &lt;- renderPlot({plot(x.axis,dll(x.axis,x()),main='First Derivative of LL for Lambda',xlab='Lambda',ylab='dLL/dLambda',type='l',xlim=c(0,6),ylim=c(-100,500)); abline(h=0,lty=2); abline(v=1/mean(x()),col='#0000ff')})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate likelihood</span>"
    ]
  },
  {
    "objectID": "hypothesistest.html#visualizer-poisson-estimation",
    "href": "hypothesistest.html#visualizer-poisson-estimation",
    "title": "Hypothesis tests",
    "section": "Visualizer: Poisson estimation",
    "text": "Visualizer: Poisson estimation\nThis visualizer shows data generated from a Poisson distribution, which is often used to model the number of events observed in a fixed period. You can set the true rate (lambda) wherever you like, and also set a sample size. For the purpose of this visualization, assume that you are asked if the true rate could be 5, i.e. \\(H_0: \\lambda = 5\\). On the left you can see a plot of your sample, and on the right you can see how often samples that extreme are generated from a true Poisson(\\(\\lambda\\)=5) process. The results of the two-tailed hypothesis test (which may draw the wrong conclusion!) are at the bottom.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Hypothesis test for a Poisson sample\",\n  fluidRow(column(width=4,sliderInput(\"lambda\", \"True lambda (rate)\", min=3, max=7, value=5, step=0.05)),\n           column(width=4,sliderInput(\"nsamp\", \"N (sample size)\", min=10, max=500, value=50)),\n           column(width=4,sliderInput(\"alpha\", \"Alpha\", min=0.01, max=0.2, value=0.05))),\n  fluidRow(column(width=6,plotOutput(\"distPlot1\")),\n           column(width=6,plotOutput(\"distPlot2\"))),\n  fluidRow(column(width=12,textOutput(\"textString1\"))))\n\nserver &lt;- function(input, output) {\n  x &lt;- reactive({rpois(n=input$nsamp,lambda=input$lambda)})\n  y &lt;- reactive({input$nsamp*dpois(0:max(x()),5)})\n  z &lt;- reactive({(0:2714)[dpois(0:2714,5*input$nsamp)&gt;1e-6]})\n  w &lt;- reactive({(0:2714)[dpois(0:2714,5*input$nsamp)&lt;=dpois(sum(x()),5*input$nsamp)]})\n  text1 &lt;- reactive({if (sum(dpois(0:2714,5*input$nsamp)[dpois(0:2714,5*input$nsamp)&lt;=dpois(sum(x()),5*input$nsamp)])&lt;=input$alpha) 'we *reject*' else 'we *cannot* reject'})\n  output$distPlot1 &lt;- renderPlot({plot(table(x()),main='Histogram of Data',xlab='Number of events',ylab='Frequency'); points(0:max(x()),y(),col='#0000ff'); legend(x='topright',legend=expression(paste('Expected if ', lambda == 5)),col='#0000ff',pch=1)})\n  output$distPlot2 &lt;- renderPlot({plot(z(),dpois(z(),5*length(x())),type='h',main='Distribution Under H0',xlab='Sum of sample',ylab='Probability',lwd=3/log(length(x()),10)); points(w(),dpois(w(),5*length(x())),type='h',col='#0000ff',lwd=3/log(length(x()),10))})\n  output$textString1 &lt;- renderText({paste('Under H0, data as extreme as our sample are observed ', round(100*sum(dpois(w(),5*length(x()))),1), '% of the time. Since alpha=', input$alpha, text1(), ' the null hypothesis that lambda=5.')})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis tests</span>"
    ]
  },
  {
    "objectID": "biasvariance.html#visualizer-normal-estimation",
    "href": "biasvariance.html#visualizer-normal-estimation",
    "title": "Estimator bias and variance",
    "section": "",
    "text": "#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 900\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Variance and bias of Normal estimators\",\n  fluidRow(column(width=3,\"\"),column(width=6,sliderInput(\"nsamp\", \"N (sample size)\", min=5, max=200, value=25)),column(width=3,\"\")),\n  fluidRow(column(width=12,plotOutput(\"distPlot1\"))),\n  fluidRow(column(width=12,plotOutput(\"distPlot2\"))))\n\nserver &lt;- function(input, output) {\n  x &lt;- reactive({matrix(rnorm(10000*input$nsamp,100,20),nrow=10000)})\n  xbar &lt;- reactive({apply(x(),1,mean)})\n  xmed &lt;- reactive({apply(x(),1,median)})\n  xsd &lt;- reactive({apply(x(),1,sd)})\n  xrmse &lt;- reactive({sqrt(xsd()^2*(input$nsamp-1)/input$nsamp)})\n  output$distPlot1 &lt;- renderPlot({hist(xbar(), breaks=seq(min(xbar(),xmed()),max(xbar(),xmed()),length.out=31), main='10,000 estimates for the mean', xlab='Mu-hat', ylab='Frequency', col='#ff000080'); hist(xmed(), breaks=seq(min(xbar(),xmed()),max(xbar(),xmed()),length.out=31), col='#0000ff80', add=TRUE); legend(x='topright', legend=c('Sample mean','Sample median'), fill=c('#ff000080','#0000ff80'), bty='n')})\n  output$distPlot2 &lt;- renderPlot({hist(xrmse(), breaks=seq(min(xrmse(),xsd()),max(xrmse(),xsd()),length.out=31), main='10,000 estimates for the std dev', xlab='Sigma-hat', ylab='Frequency', col='#ff000080'); hist(xsd(), breaks=seq(min(xrmse(),xsd()),max(xrmse(),xsd()),length.out=31), col='#0000ff80', add=TRUE); legend(x='topright', legend=c('Uncorrected: /(n)','Corrected: /(n-1)'), fill=c('#ff000080','#0000ff80'), bty='n')})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>*Estimator bias and variance*</span>"
    ]
  },
  {
    "objectID": "confidenceintervals.html#motivation",
    "href": "confidenceintervals.html#motivation",
    "title": "Exact confidence intervals",
    "section": "",
    "text": "If we pick wide ranges, we can be confident that they will actually include the true parameter, but the range might be so wide that we don’t gain useful or actionable information.\nIf we pick narrow ranges, we seem much more precise in our estimates, but we run a greater risk of being wrong about where the true parameter is located.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exact confidence intervals</span>"
    ]
  },
  {
    "objectID": "confidenceintervals.html#definition-well-more-or-less",
    "href": "confidenceintervals.html#definition-well-more-or-less",
    "title": "Exact confidence intervals",
    "section": "Definition… well, more or less",
    "text": "Definition… well, more or less\nI wish I could now show you how to construct a confidence interval. Many other textbooks do, at this point. But the truth is, confidence intervals are very poorly defined(?!) and statisticians do not agree on how best to build them(!!). For the moment, I will leave you with an unsatisfying half-answer: what a confidence interval should do.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\theta \\in \\mathcal{S}\\) be a parameter from a random variable \\(X\\) and let \\(\\boldsymbol{x}\\) be an as-yet unobserved sample from \\(X\\). Suppose that \\(a(\\boldsymbol{x})\\) and \\(b(\\boldsymbol{x})\\) are also random variables that are functions of the sample (that is, their values will vary sample to sample). If,\n\\[P(a(\\boldsymbol{x}) \\le \\theta \\le b(\\boldsymbol{x})) = 1 - \\alpha\\]\nfor all samples and all parameter values $, then \\([a(\\boldsymbol{x}),b(\\boldsymbol{x})]\\) is a \\(1 - \\alpha\\) confidence interval for the true value of the parameter \\(\\theta\\).",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exact confidence intervals</span>"
    ]
  },
  {
    "objectID": "confidenceintervals.html#example-1-exact-binomial-confidence-interval",
    "href": "confidenceintervals.html#example-1-exact-binomial-confidence-interval",
    "title": "Exact confidence intervals",
    "section": "Example 1: Exact binomial confidence interval",
    "text": "Example 1: Exact binomial confidence interval\nI will show you one example of an exact confidence interval below, and in the next chapter I will show you a method for constructing approximate confidence intervals which are much easier to calculate and which are very accurate for large sample sizes.\nLet’s return to the coin data: 27 heads from 50 trials. We’ve already shown that a fair 50-50 coin could reasonably have produced this sample. We’ve also shown that an unfair 40-60 coin would not likely have produced this sample. Using a computer’s help and the CDF of the binomial distribution, I can find the exact lower and upper bounds \\(p^*_{l,0.05}\\) and \\(p^*_{u,0.05}\\) such that:\n\nIf \\(p\\) were any lower than \\(p^*_{l,0.05}\\), then observing so many heads (27 or more) would happen less than 5% of the time\nIf \\(p\\) were any higher than \\(p^*_{u,0.05}\\), then observing so few heads (27 or fewer) would happen less than 5% of the time\n\nWhen I use this method to build a confidence interval, I know that 10% of the time my data is going to “trick” me into constructing an interval which does not include the true parameter. This will happen when — by freak chance — the data contain so many heads (or so many tails) that the true parameter seems like a bad fit to the data. But 90% of the time the interval I create will contain the true parameter. Even when the data have a few more heads or a few more tails than expected, the true parameter will be among the values that could plausibly create such data. Therefore we say that this method produces an “exact” 90% confidence interval for \\(p\\).1\n\n\nCode\n#exact 90CI for binomial data\npl &lt;- uniroot(function(z){pbinom(26,50,z)-0.95},interval=c(0.3,0.7))$root\npu &lt;- uniroot(function(z){pbinom(27,50,z)-0.05},interval=c(0.3,0.7))$root\nplot(seq(0.2,0.8,0.005),1-pbinom(26,50,seq(0.2,0.8,0.005)),\n     type='l',col='#0000ff',lwd=2,ylim=c(0,0.2),\n     xlab='Proportion of heads (p)',ylab='Cumul. Prob.',\n     main='90% CI for location of coin parameter')\nlines(seq(0.2,0.8,0.005),pbinom(27,50,seq(0.2,0.8,0.005)),\n     type='l',col='#000000',lwd=2)\nabline(h=0.05,lty=2)\nlines(x=c(pl,pu),y=c(0.05,0.05),type='b',col='#00000080',lwd=4)\nlegend(x='topleft',legend=c('P(X &gt;= 27 | p)','P(X &lt;= 27 | p)'),\n       col=c('#0000ff','#000000'),lwd=2)\ntext(x=c(pl+0.02,pu-0.02),y=c(0.05,0.05),pos=1,labels=round(c(pl,pu),3))\n\n\n\n\n\n\n\n\nFigure 4.1: Construction of an exact binomial confidence interval",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exact confidence intervals</span>"
    ]
  },
  {
    "objectID": "confidenceintervals.html#exact-normal-confidence-intervals-two-sided",
    "href": "confidenceintervals.html#exact-normal-confidence-intervals-two-sided",
    "title": "Exact confidence intervals",
    "section": "Exact normal confidence intervals (two-sided)",
    "text": "Exact normal confidence intervals (two-sided)\nDiscrete and asymmetric distributions both pose particular challenges for defining an exact confidence interval. The normal distribution provides a somewhat more convenient case. Let us consider the most general case first, so that we can try to find a method which will work for all normally-distributed samples.\nAssume that we have a sample \\(\\boldsymbol{x} = \\{x_1, \\ldots, x_n\\}\\) which observes a normally-distributed random variable \\(X\\) with unknown mean \\(\\mu_X\\) and known variance \\(\\sigma^2_X\\).2 How might we compute a \\((1-\\alpha)\\) confidence interval for the mean of \\(X\\)?3\n\nLower bound\nThe lower bound would be a potential true mean \\(x^*_{l,\\alpha/2}\\) for which these observations would be unusually large; in fact, if \\(\\mu = x^*_{l,\\alpha/2}\\) then samples as large as \\(\\boldsymbol{x}\\) would be seen with probability \\(\\alpha/2\\)].4\nSince we our sample has many observations, what precisely does it mean to be “unusually large”? Recall that the average of independent and identically-distributed normal variates is also normally distributed:\n\n\n\n\n\n\nNote\n\n\n\nIf \\(X_1, \\ldots, X_n\\) are each normally distributed random variables with mean \\(\\mu_X\\) and variance \\(\\sigma_X^2\\), then:\n\\[ \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim \\mathrm{Normal}(\\mu_X,\\frac{\\sigma_X^2}{n})\\]\n\n\nFrom this handy result, we can define a more “extreme” or “unusual” sample as one with a sample average further from the mean.\nNow we can back out the critical value of \\(\\mu^*_{l,0.05}\\). Let \\(\\bar{X}\\) be our normally distributed sample mean, centered on our unknown \\(x^*_{l,\\alpha/2}\\) and with standard deviation \\(\\sigma_X/\\sqrt{n}\\). Notice that we can standardize this normal variable without losing any information: if we define \\(Z = (X - x^*_{l,\\alpha/2})/(\\sigma_X/\\sqrt{n})\\), then \\(Z \\sim \\mathrm{Normal}(0,1)\\).\n\\[\\begin{aligned} P(\\bar{X} \\ge \\bar{x}) = \\frac{\\alpha}{2} & \\Longrightarrow P(\\bar{X} \\le \\bar{x}) = 1 - \\frac{\\alpha}{2} \\\\ & \\Longrightarrow P(Z \\le \\frac{\\bar{x} - x^*_{l,\\alpha/2}}{\\sigma_X/\\sqrt{n}}) = 1 - \\frac{\\alpha}{2} \\\\ & \\Longrightarrow \\frac{\\bar{x} - x^*_{l,\\alpha/2}}{\\sigma_X/\\sqrt{n}} = \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right) = z^*_{1 - \\alpha/2} \\\\ & \\Longrightarrow x^*_{l,\\alpha/2} = \\bar{x} - z^*_{1 - \\alpha/2} \\cdot \\sigma_X/\\sqrt{n} \\end{aligned}\\]\n\n\nUpper bound\nWith complete symmetry, we define the upper bound of our interval as the hypothetical mean for which our data seem unusually and improbably small; in fact, that such “small” samples would only occur with probability \\(\\alpha/2\\). Using the same methods shown above, we conclude,\n\\[x^*_{u,\\alpha/2} = \\bar{x} + z^*_{1 - \\alpha/2} \\cdot \\sigma_X/\\sqrt{n}\\]\nWith the general solution now in hand, let’s try computing a specific example.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exact confidence intervals</span>"
    ]
  },
  {
    "objectID": "confidenceintervals.html#example-2-pulsar-distances",
    "href": "confidenceintervals.html#example-2-pulsar-distances",
    "title": "Exact confidence intervals",
    "section": "Example 2: Pulsar distances",
    "text": "Example 2: Pulsar distances\nImagine that astronomers discover a new pulsar in deep space.5 To use the new pulsar’s data effectively, we need to know how far away it is, but our measurements of deep-space distance are very imprecise. We believe each measurement is normally distributed around the true distance with standard deviation of 7.5 light years. Astronomers take ten measurements:\n\\[\\boldsymbol{x} = \\{412.7, 419.4, 420.6, 414.0, 399.5, 417.6, 421.4, 407.5, 408.2, 407.5\\}\\]\nWhat would be a 90% confidence interval for the true distance of the pulsar?\n\\[\\begin{aligned} \\bar{x} &= 412.84 \\\\ n &= 10 \\\\ \\sigma_X &= 7.5 \\\\ z^*_{1 - \\alpha/2} &= \\Phi^{-1}(0.95) \\approx 1.645 \\end{aligned}\\]\n\\[\\begin{aligned} \\mathrm{CI}_{\\mu_X,95\\%} &= \\bar{x} \\pm z^*_{1 - \\alpha/2} \\cdot \\sigma_X / \\sqrt{n} \\\\ &\\approx 412.84 \\pm 1.645 \\cdot 7.5 / \\sqrt{10} \\\\ &\\approx (408.9, 416.7)\\end{aligned}\\]\nFrom which we may conclude that the pulsar is truly between 408.9 and 416.7 light years away. Of course, we could be wrong! We have no guarantee of this conclusion. But the method we used is right 95% of the time, at least when we are correct in our assumptions about the data being independent and identically normally distributed with a known standard deviation.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exact confidence intervals</span>"
    ]
  },
  {
    "objectID": "confidenceintervals.html#misconceptions-about-confidence-intervals",
    "href": "confidenceintervals.html#misconceptions-about-confidence-intervals",
    "title": "Exact confidence intervals",
    "section": "Misconceptions about confidence intervals",
    "text": "Misconceptions about confidence intervals\nWe will learn more about confidence intervals soon. But while we are discussing theory, I want to emphasize a few common misunderstandings about their concept and purpose. For the sake of argument, let’s say that you have a calculated a 95% confidence interval.\n\nFALSE: “95% of the values of the distribution are inside the interval”\n\nWe are not trying to define or bound the overall distribution. We are only talking about a property of the distribution: a parameter like its mean, or its endpoints, or the slope in a regression.\n\nFALSE: “The true parameter has a distribution described by our interval.”\n\nThe true parameter isn’t randomly distributed, or distributed at all: it’s just unknown. If our data only misleads us a little, it will be inside our interval. If our data misleads us a lot, the true parameter will be outside the interval. We don’t know how much our data is misleading us.\n\nFALSE: “There’s a 95% chance that this interval contains the true parameter.”\n\nAgain, we are not talking about a probabilistic event. Once the interval has been calculated, there are no varying components. Our endpoints are fixed. The truth of where the parameter is was fixed before we woke up this morning, possibly before we were born. We are either right or wrong, and we don’t know which.\n\nTRUE: “The process which created this interval successfully covers the true parameter in 95% of its applications.”\n\nWe don’t know if this specific interval is right or wrong, but we know the process usually results in success.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exact confidence intervals</span>"
    ]
  },
  {
    "objectID": "confidenceintervals.html#visualizer-confidence-interval-success-and-failure",
    "href": "confidenceintervals.html#visualizer-confidence-interval-success-and-failure",
    "title": "Exact confidence intervals",
    "section": "Visualizer: Confidence interval success and failure",
    "text": "Visualizer: Confidence interval success and failure\nThe visualizer below allows you to construct many confidence intervals for normally-distributed data with variance 1. The true data is standard normal, that is, \\(\\mathrm{Normal}(0,1)\\). However, individual samples may mislead us — if the sample average is much higher or lower than zero, we may not believe that 0 could be the true mean. Intervals containing the true mean of zero are colored blue, intervals which wrongly exclude the true mean will be colored red.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Confidence interval builder for normal data\",\n  fluidRow(column(width=5,sliderInput(\"nsamp\", \"N (sample size)\", min=5, max=200, value=25)),\n           column(width=5,sliderInput(\"conf\", \"Confidence\", min=80, max=99, post='%', value=95)),\n           column(width=2,actionButton(\"go\",\"Generate!\"))),\n  fluidRow(column(width=12,plotOutput(\"distPlot1\"))))\n\nserver &lt;- function(input, output) {\n  output$distPlot1 &lt;- renderPlot({\n    input$go\n    x &lt;- isolate(rnorm(input$nsamp))\n    p &lt;- isolate(1-(100-input$conf)/200)\n    xlo &lt;- mean(x) - qnorm(p)/sqrt(length(x))\n    xhi &lt;- mean(x) + qnorm(p)/sqrt(length(x))\n    good &lt;- (xlo &lt; 0) & (xhi &gt; 0)\n    plot(x=x,y=rep(-1,length(x)),pch=4,cex=2,xlim=c(-3.5,3.5),ylim=c(-2,2),axes=FALSE,ylab=NA,xlab='Values of X')\n    axis(1,-3:3,-3:3,pos=0)\n    lines(x=c(xlo,xhi),y=c(1,1),lwd=5,type='b',col=c('red','blue')[1*good+1])\n    text(x=c(xlo,xhi),y=c(1,1),pos=c(2,4),labels=round(c(xlo,xhi),2))\n    abline(v=mean(x),lty=2)})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exact confidence intervals</span>"
    ]
  },
  {
    "objectID": "identification.html#example-car-stopping-distances",
    "href": "identification.html#example-car-stopping-distances",
    "title": "Identifying distributions",
    "section": "Example: Car stopping distances",
    "text": "Example: Car stopping distances\nLet’s say that we are brake systems engineers and our job is to evaluate the safety of a new car model. We wish to know the real-life distribution of stopping times (in seconds) when the car is traveling at 50 kph ($$30 mph) under ideal road conditions. We gather 20 test drivers, put them on a safe course, and observe the following stopping times:\n\\[\\boldsymbol{x}=\\left\\{ \\begin{aligned} 3.68, 2.86, 3.40, 3.82, 4.32, 3.40, 3.82, 3.73, 4.33, 3.56, \\\\ 3.27, 3.41, 3.25, 4.04, 3.34, 3.88, 3.85, 3.07, 3.58, 3.88 \\; \\end{aligned} \\right\\}\\] Consulting our company’s research library, we find that prior experiments have modeled stopping times as normally distributed, but we believe that the log-normal distribution might be a better choice. Can we test these ideas?\nFirst, we fit the best normal distribution we can, retrieving the best maximum-likelihood estimates for \\(\\mu\\) and \\(\\sigma\\):\n\nlibrary(MASS)\nstoptimes &lt;- c(3.68, 2.86, 3.40, 3.82, 4.32, 3.40, 3.82, 3.73, 4.33, 3.56, 3.27, 3.41, 3.25, 4.04, 3.34, 3.88, 3.85, 3.07, 3.58, 3.88)\n(norm.ests &lt;- fitdistr(stoptimes,'normal')$estimate)\n\n     mean        sd \n3.6245000 0.3753595 \n\n\nThen we use the K-S test to see if these ‘best’ estimates are any good:\n\nsuppressWarnings(ks.test(stoptimes,pnorm,mean=norm.ests[1],sd=norm.ests[2]))\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  stoptimes\nD = 0.11615, p-value = 0.9501\nalternative hypothesis: two-sided\n\n\nWe see no reason to reject the earlier guidance that stopping times are normally distributed. There is a normal distribution out there which generates data as non-normal as our own 95% of the time! That’s almost all of the time!\nHowever, we can still see if the log-normal distribution is a good fit as well. Just like before, we find the best parameters for \\(\\mu'\\) and \\(\\sigma'\\):3\n\n(lognorm.ests &lt;- fitdistr(stoptimes,'log-normal')$estimate)\n\n  meanlog     sdlog \n1.2822967 0.1044904 \n\n\nThen we use the K-S test to see if these ‘best’ estimates are any good:\n\nsuppressWarnings(ks.test(stoptimes,plnorm,meanlog=lognorm.ests[1],sdlog=lognorm.ests[2]))\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  stoptimes\nD = 0.11043, p-value = 0.9677\nalternative hypothesis: two-sided\n\n\nAs it turns out, this sample of stopping times also supports the idea that they are log-normally distributed. We can find a log-normal distribution that generates data similar to this (or more extreme) 97% of the time.\nWe do not have enough evidence to reject the earlier literature’s assumption that stopping times are normal. However, we also find a close fit to the log-normal distribution. We might choose to use either, or both, in our research, or collect more data, or make an argument for one or the other that does not depend on relative likelihood.4",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Identifying distributions</span>"
    ]
  },
  {
    "objectID": "identification.html#visualizer-confidence-interval-success-and-failure",
    "href": "identification.html#visualizer-confidence-interval-success-and-failure",
    "title": "Identifying distributions",
    "section": "Visualizer: Confidence interval success and failure",
    "text": "Visualizer: Confidence interval success and failure\nWhen sample sizes are small, it can be difficult to tell one distribution from another. I will generate some data from a triangular distribution, which looks exactly as you might expect: a continuous distribution bounded by its minimum \\(a\\) and maximum \\(b\\), where the density at either endpoint scales linearly to a modal maximum of \\(c: a \\le c \\le b\\).\n\n\nCode\n#theoretical pdf\npar(mfrow=c(1,2))\nplot(x=c(45,55,100,145,155),y=c(0,0,1/45,0,0),type='l',lwd=2,xlab='x',ylab='Density',\n     main='Triangular PDF',xlim=c(45,155))\nxseq &lt;- seq(45,155,0.1)\nyseq &lt;- c(rep(0,101),(seq(55.1,100,0.1)-55)^2/(90*45),\n          1-(145-seq(100.1,145,0.1))^2/(90*45),rep(1,100))\nplot(x=xseq,y=yseq,type='l',lwd=2,xlab='x',ylab='Cumul. Prob.',\n     main='Triangular CDF',xlim=c(45,155))\n\n\n\n\n\n\n\n\nFigure 5.3: Triangular(55,145,100) distribution\n\n\n\n\n\nThe tool below simulates data from this exact distribution, and then tests whether that data could have come from a uniform distribution (specifically, the uniform distribution which best fits each new sample). Notice that our ability to rule out the uniform distribution is closely tied to sample size: at small sample sizes, we rarely reject the null.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Univariate estimation from a triangular sample\",\n  fluidRow(column(width=12,sliderInput(\"nsamp\", \"N (sample size)\", min=20, max=200, value=50))),\n  fluidRow(column(width=6,plotOutput(\"distPlot1\")),\n           column(width=6,plotOutput(\"distPlot2\"))),\n  fluidRow(textOutput(\"testText1\")))\n\nqtri &lt;- function(p){\n  x &lt;- vector(mode='numeric',length=length(p))\n  x[p&lt;0.5] &lt;- 55 + sqrt(p[p&lt;0.5]*90*45)\n  x[p&gt;=0.5] &lt;- 145 - sqrt((1-p)[p&gt;=0.5]*90*45)\n  return(x)}\n\nserver &lt;- function(input, output){\n  x &lt;- reactive({qtri(p=runif(input$nsamp))})\n  yseq &lt;- c((seq(55,100,0.5)-55)^2/(90*45),\n            1-(145-seq(100.5,145,0.5))^2/(90*45))\n  pval &lt;- reactive({ks.test(x(),punif,min=min(x()),max=max(x()))$p.value})\n  output$distPlot1 &lt;- renderPlot({\n    hist(x(),xlab='Values',ylab='Frequency',main='Histogram of Sample')})\n  output$distPlot2 &lt;- renderPlot({\n    plot(ecdf(x()),xlab='x',ylab='F(x)',main='ECDF of Sample',pch=NA)\n    lines(x=seq(55,145,0.5),y=punif(seq(55,145,0.5),min(x()),max(x())),col='#0000ff')\n    lines(x=seq(55,145,0.5),y=yseq,col='green')\n    legend(x='topleft',legend=c('Best Uniform fit','Actual distribution'),lty=1,col=c('blue','green'))})\n  output$testText1 &lt;- renderText({c('Could sample be Uniform?',ifelse(pval()&gt;0.05,'Likely so: p = ','Likely not: p = '),round(pval(),4))})\n  }\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Identifying distributions</span>"
    ]
  },
  {
    "objectID": "newton.html",
    "href": "newton.html",
    "title": "The Newton-Raphson method",
    "section": "",
    "text": "Motivation\nIn Univariate likelihood, we learned to maximize likelihood functions using arithmetic and calculus. However, sometimes the likelihood function does not allow a closed-form solution, and of course modern computers do not solve problems with pencil and paper. How then would we find the maximum of an unsolvable likelihood equation?\nThe short answer is that we can use any of several algorithmic optimization methods. These methods were not developed specifically for statistical use, but they work quite well for our purposes, and we can easily express them in machine-readable code. One example of these optimization algorithms is the Newton-Raphson method, which has been used for hundreds of years.1",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Newton-Raphson method</span>"
    ]
  },
  {
    "objectID": "newton.html#definition",
    "href": "newton.html#definition",
    "title": "The Newton-Raphson method",
    "section": "Definition",
    "text": "Definition\nThe Newton-Raphson method specifically finds the root of an equation, i.e. the value of \\(x\\) for which the function’s value \\(f(x)\\) is zero. We can use this to find where the first derivative of a log-likelihood function is zero, which in turn will (usually) be the maximum of the original likelihood function:2\n\nLet \\(f\\) be a continuous and differentiable function with derivative \\(f'\\) and at least one root, i.e. there exists some \\(x^*\\) such that \\(f(x^*) = 0\\). The root \\(x^*\\) can sometimes be approximated, depending on the nature of the function \\(f\\) and the choice of an initial guess \\(x_0\\), by successively iterating the following series:\n\\[x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}\\]",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Newton-Raphson method</span>"
    ]
  },
  {
    "objectID": "newton.html#footnotes",
    "href": "newton.html#footnotes",
    "title": "The Newton-Raphson method",
    "section": "",
    "text": "At least four mathematicians could lay claim to this method, and neither Isaac Newton nor Joseph Raphson published first or completed the version we use today. Stigler’s Law of Eponymy strikes again!↩︎\nDo be cautious, since the root of a derivative may instead match an inflection point or local minimum of the original function, and the true maximum of a likelihood function may not be a local maximum.↩︎",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Newton-Raphson method</span>"
    ]
  },
  {
    "objectID": "newton.html#modifying-newton-raphson-with-secant-approximations",
    "href": "newton.html#modifying-newton-raphson-with-secant-approximations",
    "title": "The Newton-Raphson method",
    "section": "Modifying Newton-Raphson with secant approximations",
    "text": "Modifying Newton-Raphson with secant approximations\nWhen we have a perfect algebraic expression for the derivative \\(f^{'}\\), then we should by all means use it. But if we don’t have a closed-form solution, or if we are using computers to speed our work, we may approximate the derivative with a local linearization. If you were taught calculus through the concept of infinitesimals, this formula may be familiar. Rather than compute the slope of an exact tangent line, we can compute the slope of a secant line between two very nearby points, as shown in the equation and figure below:\n\\[ \\mathrm{for}\\; \\delta \\approx0, \\quad f'(x) \\approx \\frac{f(x + \\delta) - f(x - \\delta)}{2 \\delta}\\]\n\n\nCode\n#secant approximation to the derivative\npar(mfrow=c(1,2),mar=c(4,2,4,2)+0.1)\nsec.x &lt;- seq(0.5,2.5,0.05)\nfsec &lt;- function(x) 2*log(x)-x+3\nplot(sec.x,fsec(sec.x),type='l',lwd=2,ylim=c(1,2.8),\n     main='Exact derivative of a function',xlab='x',ylab='f(x)')\nabline(fsec(1.5)-0.5,1/3,col='#800000',lwd=2)\nlines(x=c(1.5,1.5),y=c(0,fsec(1.5)))\ntext(0.8,2.3,\"f'(1.5)=0.333...\",col='#800000',cex=0.8)\n\nplot(sec.x,fsec(sec.x),type='l',lwd=2,ylim=c(1,2.8),\n     main='Approximation using secant line',xlab='x',ylab='f(x)')\nlines(x=c(1.3,1.3),y=c(0,fsec(1.3)),lty=2)\nlines(x=c(1.7,1.7),y=c(0,fsec(1.3)),lty=2)\nlines(x=c(1.3,1.7),y=c(fsec(1.3),fsec(1.3)))\nlines(x=c(1.7,1.7),y=c(fsec(1.3),fsec(1.7)))\nlines(x=c(1.5,1.5),y=c(0,1))\nsec.b &lt;- (fsec(1.7)-fsec(1.3))/0.4\nabline(fsec(1.3)-1.3*sec.b,sec.b,1/3,col='#800000',lwd=2)\ntext(0.8,2.3,expression(\"f'(1.5)\"%~~%0.341),col='#800000',cex=0.8)\ntext(1.5,1.1,expression(x),cex=0.8)\ntext(1.2,1.1,expression(x-epsilon),cex=0.8)\ntext(1.8,1.1,expression(x+epsilon),cex=0.8)\n\n\n\n\n\n\n\n\nFigure 6.1: Approximating the derivative with a secant line",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Newton-Raphson method</span>"
    ]
  },
  {
    "objectID": "newton.html#estimating-derivatives-with-secant-approximations",
    "href": "newton.html#estimating-derivatives-with-secant-approximations",
    "title": "The Newton-Raphson method",
    "section": "Estimating derivatives with secant approximations",
    "text": "Estimating derivatives with secant approximations\nWhen we have a perfect algebraic expression for the derivative \\(f^{'}\\), then we should by all means use it. But if we don’t have a closed-form solution, or if we are using computers to speed our work, we may approximate the derivative with a local linearization. If you were taught calculus through the concept of infinitesimals, this formula may be familiar. Rather than compute the slope of an exact tangent line, we can compute the slope of a secant line between two very nearby points, as shown in the equation and figure below:\n\\[ \\mathrm{for}\\; \\delta \\approx0, \\quad f'(x) \\approx \\frac{f(x + \\delta) - f(x - \\delta)}{2 \\delta}\\]\n\n\nCode\n#secant approximation to the derivative\npar(mfrow=c(1,2),mar=c(4,2,4,2)+0.1)\nsec.x &lt;- seq(0.5,2.5,0.05)\nfsec &lt;- function(x) 2*log(x)-x+3\nplot(sec.x,fsec(sec.x),type='l',lwd=2,ylim=c(1,2.8),\n     main='Exact derivative of a function',xlab='x',ylab='f(x)')\nabline(fsec(1.5)-0.5,1/3,col='#800000',lwd=2)\nlines(x=c(1.5,1.5),y=c(0,fsec(1.5)))\ntext(0.8,2.3,\"f'(1.5)=0.333...\",col='#800000',cex=0.8)\n\nplot(sec.x,fsec(sec.x),type='l',lwd=2,ylim=c(1,2.8),\n     main='Approximation using secant line',xlab='x',ylab='f(x)')\nlines(x=c(1.3,1.3),y=c(0,fsec(1.3)),lty=2)\nlines(x=c(1.7,1.7),y=c(0,fsec(1.3)),lty=2)\nlines(x=c(1.3,1.7),y=c(fsec(1.3),fsec(1.3)))\nlines(x=c(1.7,1.7),y=c(fsec(1.3),fsec(1.7)))\nlines(x=c(1.5,1.5),y=c(0,1))\nsec.b &lt;- (fsec(1.7)-fsec(1.3))/0.4\nabline(fsec(1.3)-1.3*sec.b,sec.b,1/3,col='#800000',lwd=2)\ntext(0.8,2.3,expression(\"f'(1.5)\"%~~%0.341),col='#800000',cex=0.8)\ntext(1.5,1.1,expression(x),cex=0.8)\ntext(1.2,1.1,expression(x-epsilon),cex=0.8)\ntext(1.8,1.1,expression(x+epsilon),cex=0.8)\n\n\n\n\n\n\n\n\nFigure 6.1: Approximating the derivative with a secant line\n\n\n\n\n\nWhether we use an exact algebraic solution for the derivative or approximate it with a secant line, we are now ready to use the Newton-Raphson method to produce a MLE:\n\nCompute a log-likelihood function for your parameter\nFind the derivative of the log-likelihood function, or estimate it using the secant approximation. You will be trying to find a root for this equation; it is the function \\(f\\) described in the equations above.\nFind the second derivative of the log-likelihood function, or estimate it using the second approximation. You will use this second derivative to iterate the algorithm; it is the function \\(f'\\) in the equations above.\nChoose an initial guess for the root (i.e. the best parameter value.)\nChoose a tolerance which will help you know when to stop iterating the algorithm. This tolerance is sometimes expressed as an maximum change in \\(x\\), or other times as a maximum increment in \\(f(x)\\) (i.e. the first derivative of the log-likelihood).\nIterate until the new values of \\(x\\) and \\(f(x)\\) are within tolerance, or stop the algorithm if your search does not converge upon a root.\n\nNotice that the method I have outlined here only works for functions of a single variable, meaning that it will only work to identify distributions defined by a single parameter. Distributions with multiple parameters are more typically solved through gradient descent, which will be outlined in the next section.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Newton-Raphson method</span>"
    ]
  },
  {
    "objectID": "newton.html#programming-example",
    "href": "newton.html#programming-example",
    "title": "The Newton-Raphson method",
    "section": "Programming example",
    "text": "Programming example\n\n#coin data generated with true p=0.6\n1set.seed(2061)\n2coins &lt;- sample(0:1,50,TRUE,c(0.4,0.6))\n\n# derivative of log-likelihood for coin parameter\n3coin.dll &lt;- function(p) sum(coins)/p-sum(1-coins)/(1-p)\n\n# helper function to approximate derivatives\n4ddx &lt;- function(value, func, delta=1e-6){\n  (func(value+delta)-func(value-delta))/(2*delta)}\n\n# core newton-raphson algorithm, plus some output\n5newton &lt;- function(f, x0, tol=1e-6, max.iter=20){\n  x &lt;- x0\n  i &lt;- 0\n6  results &lt;- data.frame(iter=i,x=x,fx=f(x))\n7  while (abs(f(x))&gt;tol & i&lt;max.iter) {\n    i &lt;- i + 1\n    x &lt;- x - f(x)/ddx(x,f)\n    results &lt;- rbind(results,data.frame(iter=i,x=x,fx=f(x)))}\n  return(results)}\n\n\n1\n\nSpecifying the seed forces the same “random” numbers to occur each time\n\n2\n\nEven though \\(p=0.6\\), only \\(27/50 = 54\\%\\) of these 50 flips are heads\n\n3\n\nWe derived this formula earlier, in Univariate likelihood\n\n4\n\nThis implements the secant approximation method shown above\n\n5\n\nThe inputs are f (the function whose root we are finding), x0 (an initial guess), tol (tolerance; we will stop the algorithm when we get this close to a root), and a maximum number of iterations (in case we don’t converge)\n\n6\n\nTo show my work, I’m outputting results to a table\n\n7\n\nThe algorithm will stop when \\(f(x)\\) is within tolerance of 0 or when it hits the maximum iteration count without converging",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Newton-Raphson method</span>"
    ]
  },
  {
    "objectID": "gradientdescent.html",
    "href": "gradientdescent.html",
    "title": "Gradient descent",
    "section": "",
    "text": "Motivation\nIn The Newton-Raphson method, we learned a simple algorithm for finding the root of functions of a single variable. Although we can (and do) use the Newton-Raphson method to perform maximum likelihood estimation, it suffers from some drawbacks:\nLet us introduce a new algorithm which might solve these problems, called gradient descent.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "gradientdescent.html#motivation",
    "href": "gradientdescent.html#motivation",
    "title": "Gradient descent",
    "section": "",
    "text": "It does not easily extend to functions of multiple variables, and so we cannot use it to fit distributions with two or more parameters, such as the Normal distribution (\\(\\mu\\) and \\(\\sigma^2\\)).\nIt requires us to compute or estimate the first and second derivative of the likelihood (or log-likelihood) function, which may prove a mild inconvenience or in rare cases a mathematical impossibility.\nIts searches inefficiently, since it picks a starting point, determines the direction of the slope, and “races ahead” toward what it expects will be the finish line. Only when it hits the x-axis does it stop to check how far off-course the original tangent line has led it.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "gradientdescent.html#concept-and-definition",
    "href": "gradientdescent.html#concept-and-definition",
    "title": "Gradient descent",
    "section": "Concept and definition",
    "text": "Concept and definition\nSuppose we are walking on the surface of a multi-dimensional log-likelihood function, trying to find the joint set of parameters which form the MLEs. Given a starting point (an initial guess for all parameters), we will use calculus to determine the slope of the “hill” under our feet and then take only a few short steps up or down the steepest slope of the hill. At this new waypoint, we will measure the slope again, to see if it has changed much, and walk in whichever direction now seems steepest. In real life this method would work well to find a hilltop or valley bottom, and in mathematics it works well to find maxima and minima.\nUp until now, I have shown you ways to maximize the likelihood function, and if we were to use the principles above to find a hilltop, we would call this technique “gradient ascent”. However, when we work with computers to implement this algorithm, it is often easier to minimize the negative of the log-likelihood function, that is, to find the valley bottom.1 For this rather trivial reason, the technique is more often called “gradient descent”.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(f\\) be a scalar-valued continuous and differentiable function of a vector of parameters \\(\\vec{x}=(x_1, \\ldots, x_p) \\in \\mathbb{R}^p\\). Define the gradient of \\(f\\) to be the vector-valued function which gives the direction and rate at which \\(f\\) increases most quickly:\n\\[\\nabla f(\\vec{x})=\\left[\\begin{array}{c}\\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_p}\\end{array}\\right]\\]\nStarting from an initial guess \\(\\vec{x}_0\\), the method of gradient ascent allows us to iteratively approximate a nearby local maximum or minimum through the following series:\n\\[\\vec{x}_n = \\vec{x}_{n-1} - \\gamma_n \\cdot \\nabla f(\\vec{x}_{n-1})\\]\nWhere each new step size \\(\\gamma_n\\) may be chosen according to several considerations.\n\n\nThis new method doesn’t find a root of the derivative, like the Newton-Raphson method, but rather directly solves for a local minimum. In practice there are important and very finicky considerations of (a) how big the step sizes should be and (b) whether you should necessarily proceed in the steepest possible direction, or if it might be smarter to approach the minimum obliquely.\nI will show one method of choosing a step size, called backtracking line search. The following algorithm chooses the step size \\(\\gamma_n\\) for a single iteration of the main gradient descent algorithm (iteration \\(n\\)):\nIn essence, a backtracking line search takes large steps when the function is being steadily minimized, meaning that all is going well, but shrinks the step size if the algorithm would overshoot or otherwise yield unexpectedly poor results.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(f\\), \\(\\vec{x}\\), and \\(\\nabla f(\\vec{x})\\) be defined as above, and let \\(\\vec{x}_{n-1}\\) be the previous guess for the value of \\(\\vec{x}\\) which maximizes \\(f\\). A backtracking line search will set the next step size, \\(\\gamma_n\\), as follows. First, choose a fixed \\(\\beta \\in (0,1)\\). Then, iterating over the subscript \\(i\\),\n\\[\\begin{aligned} \\gamma_{n,0} &= 1 \\\\\n\\gamma_{n,i} &= \\left\\{\\begin{array}{ll}\\beta \\cdot \\gamma_{n,i-1} & \\textrm{if }  f(\\vec{x}_{n-1} - \\gamma_{n,i-1} \\cdot \\nabla f(\\vec{x}_{n-1})) &gt; f(\\vec{x}_{n-1}) \\\\ \\gamma_{n,i-1} & \\textrm{otherwise} \\end{array}\\right\\} \\end{aligned}\\]\nStop this search when \\(\\gamma_{n,i} = \\gamma_{n,i-1}\\), and define that to be \\(\\gamma_n\\), the step size for the next iteration of the gradient ascent algorithm.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "gradientdescent.html#footnotes",
    "href": "gradientdescent.html#footnotes",
    "title": "Gradient descent",
    "section": "",
    "text": "Of course, the same parameters which maximize the likelihood will minimize the negative of the likelihood, so we will recover the same MLEs either way.↩︎",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "gradientdescent.html#programming-example-height-data",
    "href": "gradientdescent.html#programming-example-height-data",
    "title": "Gradient descent",
    "section": "Programming example: height data",
    "text": "Programming example: height data\nConsider the height data from ten women discussed earlier. If we assume these heights are drawn from a Normal distribution, then we need estimates for the mean \\(\\mu\\) and the variance \\(\\sigma^2\\). We can solve for the MLEs exactly using algebra and calculus, but we can also recover the best estimates using gradient descent.\nFirst let’s code up a generalized gradient descent algorithm, with a backtracking line search. This code would work to solve many cases, but it’s not perfect and doesn’t handle every edge case: treat this as illustrative and not production-ready:\n\n# gradient descent function with backtracking line search\n1graddesc &lt;- function(f, g, x0, b=0.8, tol=1e-2, max.iter=40){\n  i &lt;- 0\n  x &lt;- x0\n  results &lt;- t(c(0,x,-f(x)))\n2  while (sqrt(g(x)%*%g(x))&gt;tol & i&lt;max.iter) {\n    i &lt;- i + 1\n    t &lt;- 1\n3    while ((f(x+t*g(x))&lt;f(x)) & t&gt;tol) {\n      t &lt;- t*b}\n    x &lt;- x + t*g(x)                                           \n4    results &lt;- rbind(results,t(c(i,x,-f(x))))}\n  colnames(results) &lt;- c('iter',paste0('x',1:length(x)),'f')\n  return(results)}\n\n\n1\n\nInputs are \\(f\\) (the likelihood function), \\(g\\) (the gradient of \\(f\\)), \\(x_0\\) (a starting guess), b/\\(\\beta\\) (the backtracking constant), tol (a convergence tolerance, also used to create a minimum step size), and a maximum iteration count.\n\n2\n\nThe outer loop completes each iteration of gradient descent algorithm.\n\n3\n\nThe inner loop completes each choice of step size for the outer loop, using the backtracking line search algorithm.\n\n4\n\nResults are output to a table, including each iteration’s location in parameter space and the negative of the log-likelihood at that location.\n\n\n\n\nNow we can add the height data, the likelihood function, and the gradient function:\n\n#generate height data\nheights &lt;- c(170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3)\n\n#height log-likelihood for any parameter choice\n1height.ll &lt;- function(x){\n2  -1*length(heights)*log(2*pi*x[2])/2 -\n    sum((heights-x[1])^2)/(2*x[2])}\n\n#gradient of height log-likelihood for any parameter choice\nheight.grad &lt;- function(x){\n3  c(sum(heights-x[1])/x[2],\n    -1*length(heights)/(2*x[2]) +\n      sum((heights-x[1])^2)/(2*x[2]^2))}\n\n\n1\n\nIn both functions, \\(\\vec{x} = (\\hat{\\mu},\\hat{\\sigma}^2)\\) is a vector of normal parameters.\n\n2\n\n\\(\\partial \\ell/\\partial \\mu\\)\n\n3\n\n\\(\\partial \\ell/\\partial \\sigma^2\\)\n\n\n\n\nWith these in hand, we are ready to solve. Starting from a poor initial guess, the backtracking line search eventually finds the normal distribution MLEs of our data, reaching stability at around 40 iterations. In practice, the data, the distribution, your tuning parameters, and the initial guess will all affect the convergence speed of this algorithm.\n\ngd &lt;- graddesc(f=height.ll,g=height.grad,x0=c(150,40))\ngd[c(1:5,41),]\n\n     iter       x1       x2        f\n[1,]    0 150.0000 40.00000 56.81366\n[2,]    1 153.3825 40.60450 46.59214\n[3,]    2 155.8816 40.94641 41.04564\n[4,]    3 157.7495 41.14899 37.95694\n[5,]    4 159.1543 41.27492 36.21285\n[6,]   40 163.5298 42.28546 33.86851\n\n\n\n\nCode\nxgrid &lt;- seq(140,180,0.1)\nygrid &lt;- seq(25,80,0.1)\nzgrid &lt;- -1*matrix(apply(cbind(rep(xgrid,times=length(ygrid)),rep(ygrid,each=length(xgrid))),1,height.ll),ncol=length(ygrid))\n\ncontour(xgrid,ygrid,zgrid,levels=c(120,100,80,70,60,50,40,35,34,33.8,33.7),\n               col=paste0('#0000ff',c('00','20','40','60','80','9f','af','cf','ef','ff')),\n               main='Gradient Descent Solution for Height Parameters',\n               xlab='Mu (mean height, in cm)',ylab='Sigma^2 (variance)')\nlines(gd[,2],gd[,3])\npoints(gd[c(1:5,40),2:3],pch=1)\ntext(gd[c(1:5,40),2:3],paste0('x',c(0:4,40)),pos=c(1,1,1,1,1,4),cex=0.8)\n\n\n\n\n\n\n\n\nFigure 8.1: Gradient descent approximation of height MLEs",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gradient descent</span>"
    ]
  },
  {
    "objectID": "clt.html",
    "href": "clt.html",
    "title": "The Central Limit Theorem",
    "section": "",
    "text": "The sample mean as a random variable\nBy now, you are hopefully used to thinking of different populations as following known probability distributions. Perhaps bus waiting times are exponentially distributed, or the number people in Chicago using a payphone right now could be Poisson distributed.\nA sample drawn from a population will change each time a new sample is drawn, and therefore the properties of that sample will also change, meaning that those properties are random variables. However, the distribution of the sample property is not the same as the population distribution.\nAnd so on. Perhaps the most important property of any sample is the mean. We would love to know if a sample mean is so small or so large that it casts doubt on our null hypothesis or our distributional assumptions, but we cannot make these calculations if we do not know the supposed distribution of the sample mean.\n** Estimating the sample mean distribution as Normal\nThe true distribution of the sample mean will depend upon both the distribution of the population and the sample size and is often very,very hard to compute. The good news is, we don’t have to compute the exact distribution, because the sample mean of a sample from almost any distribution grows more and more normally distributed as the sample size increases:\nCode\n#normalization of sample means\npar(mfrow=c(2,2))\nset.seed(1985)\nrolls10 &lt;- matrix(sample(1:6,500000,TRUE),ncol=10)\navgs10 &lt;- apply(rolls10,1,mean)\nrolls50 &lt;- matrix(sample(1:6,5000000,TRUE),ncol=50)\navgs50 &lt;- apply(rolls50,1,mean)\nplot(1:6,rep(1/6,6),type='h',lwd=3,ylim=c(0,0.18),\n     main='Sample means when rolling 1 die',\n     ylab='Probability',xlab='Sample average')\nplot(seq(1,6,0.5),1/6-abs(3.5-seq(1,6,0.5))/18,type='h',\n     lwd=3,ylim=c(0,0.18),\n     main='Sample means when rolling 2 dice',\n     ylab='Probability',xlab='Sample average')\nplot(table(avgs10)/50000,lwd=3,ylim=c(0,0.08),xlim=c(1.6,5.4),\n     main='Sample means when rolling 10 dice',\n     ylab='Probability',xlab='Sample average')\nplot(table(avgs50)/100000,lwd=2,ylim=c(0,0.04),xlim=c(2.6,4.4),\n     main='Sample means when rolling 50 dice',\n     ylab='Probability',xlab='Sample average')\n\n\n\n\n\n\n\n\nFigure 9.1: Sample means approaching normality as sample size increases\nThe figure above illustrates this principle by considering the sample mean of one or more fair, six-sided dice.\nYou might also notice that the distribution of the sample means has tightened considerably. When rolling just one or two dice, a sample mean of 2 or 5 was fairly common. But when rolling 10 or 50 dice, the sample average would almost never be so low as 2 or so high as 5. As sample size increases, the variance of the sample mean shrinks.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "clt.html#footnotes",
    "href": "clt.html#footnotes",
    "title": "The Central Limit Theorem",
    "section": "",
    "text": "For example, with 10 dice, the sample average could be 3.6 or 3.7 but never 3.65↩︎\nNote: though I use \\(\\mu\\) and \\(\\sigma^2\\) to denote the mean and variance, \\(X\\) does not have to be Normally distributed↩︎",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "clt.html#the-sample-mean-as-a-random-variable",
    "href": "clt.html#the-sample-mean-as-a-random-variable",
    "title": "The Central Limit Theorem",
    "section": "",
    "text": "If a population is uniformly distributed, the minimum of each new sample is not itself uniformly distributed (since low numbers will be more common than high numbers).\nIf a population is Poisson distributed, the variance of each new sample is not Poisson distributed (since Poisson values are integers, and a sample variance can be fractional)\n\n\n\n\n\n\n\nClearly, when we roll only one die, the “mean” of the sample is just the roll itself, and so it shares the same discrete uniform distribution. All integers 1 through 6 are equally likely.\nWhen we roll two dice, a total of 7 is the most common value, which is the same as a sample mean of 3.5. It would be rare to roll two 1s (sample mean of 1) or two 6s (sample mean of 6). The sample mean forms a discrete triangular distribtion.\nWhen we increase the number of dice to 10 or 50, the distribution of sample means will still be centered at 3.5, but now resembles the bell curve of the Normal distribution. Of course it’s not a Normal curve – since that is a continuous distribution defined at every point, while this is still a discrete distribution.1",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "clt.html#formal-and-informal-definitions",
    "href": "clt.html#formal-and-informal-definitions",
    "title": "The Central Limit Theorem",
    "section": "Formal and informal definitions",
    "text": "Formal and informal definitions\nWe can capture this all this behavior with the following rule:\n\n\n\n\n\n\nNote\n\n\n\nThe Central Limit Theorem (CLT): Let \\(X\\) be a random variable with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\)2, and let \\(\\boldsymbol{x} = x_1,x_2,\\ldots,x_n\\) be a sample of independent observations of \\(X\\). Then,\n\\[\\lim_{n \\rightarrow \\infty} P\\left(\\frac{\\bar{x}-\\mu}{\\sigma⁄\\sqrt{n}} &lt; z\\right)=\\Phi(z)\\]\nWhere \\(\\bar{x}\\) is the sample mean of \\(\\boldsymbol{x}\\) and \\(\\Phi(z)\\) is the cumulative distribution function of the standard Normal distribution.\n\n\nIn my experience, many students “bounce off” of this theoretical definition without really understanding what it means or why it can be so helpful. Let me rephrase the CLT in slightly less formal terms.\nInformal CLT: If your sample is independently drawn from a population with a finite mean and a known variance, then as the sample size increases, your sample mean will be approximately normally distributed around the true population mean. Specifically,\n\\[\\textrm{For large } n, \\textrm{and as an approximation,}\\;\\bar{X} \\sim \\textrm{Normal}\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\\] Where \\(\\bar{X}\\) represents the random variable of the sample mean, and \\(\\mu\\), \\(\\sigma^2\\), and \\(n\\) are all the same as in the formal CLT definition above.\nThe beauty of this result is that it provides an easy way to approximate the probable locations of the true mean of the distribution. When we measure our sample mean, we cannot know how far away the true mean might be. We don’t know whether we have underestimated or overestimated the true mean. But we know that the distance between the true mean and the sample mean is a random variable which is roughly normally distributed, meaning that we can identify reasonable lower and upper bounds on how far the true mean is from our sample means.\nThe difficulty of this result is that we have to know the true population variance, \\(\\sigma^2\\), in order to use this equation. We wil address this difficulty in the next section.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "clt.html#example-approximate-confidence-interval-for-true-mean-heights",
    "href": "clt.html#example-approximate-confidence-interval-for-true-mean-heights",
    "title": "The Central Limit Theorem",
    "section": "Example: approximate confidence interval for true mean heights",
    "text": "Example: approximate confidence interval for true mean heights\nEarlier, when discussing the nature of parametric inference, I posed the following question:\n\nWe observe the heights of ten women, and we assume they represent a sample from a population which is normally distributed. Based on our sample, what are realistic ranges for the unknown mean \\(\\mu\\) and variance \\(\\sigma^2\\)?\n\nLet’s attempt a (partial) answer. We will assume the same ten heights described earlier, measured in centimeters:\n\\[\\boldsymbol{x}=\\{170.1,161.6,175.2,166.3,165.6,165.8,152.0,155.8,168.6,154.3\\}\\] And furthermore, we will assume that a little bird has whispered in our ear that the true variance of the heights of adult women in the United States is 50 cm2.\nThe sample mean of our heights is 163.53 cm. That’s a good point estimate for the true mean. But how large or small could the true mean be? We might start by calculating a 95% confidence interval, bounded by:\n\nA lower bound where the true mean would be so low that data “this high” or higher would only occur in 2.5% of random samples\nAn upper bound where the true mean would be so high that data “this low” or lower would only occur in 2.5% of random samples\n\nThe CLT greatly simplifies the process of finding these two bounds. We know that the true mean is likely not far from our sample mean. We also know that the distance between the true mean and sample mean is approximately normal, with mean 0 and standard deviation of \\(\\sigma/\\sqrt{n}\\). Finally, we know that the upper 2.5% of the normal distribution falls roughly 1.96 standard deviations from the mean (i.e. \\(\\Phi^{-1}(0.975) \\approx 1.96\\)). Therefore,\n\\[\\textrm{CI}_{μ,0.95} = \\bar{x} \\pm z_{0.975}^* \\frac{\\sigma}{\\sqrt{n}} \\approx 163.53 \\pm 1.96 \\cdot \\sqrt{50⁄10} \\approx [159.15,167.91]\\] From our sample of ten women’s heights, we believe that the true mean height of adult women in the United States is between 159.15 and 167.91 cm. Compare this to a 2018 Centers for Disease Control publication that reported a true mean height of 161.7 cm: our sample mean was a little high, but the confidence interval did contain the true parameter!",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "clt.html#visualizer",
    "href": "clt.html#visualizer",
    "title": "The Central Limit Theorem",
    "section": "Visualizer",
    "text": "Visualizer\nChoose a distribution and a sample size, and notice whether the approximation of the CLT (in blue) seems accurate or not to the actual distribution of means (in black):\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 960\n\nlibrary(shiny)\nlibrary(bslib)\n\nmyunif &lt;- function(u) qunif(u,min=0,max=6) \nmypois &lt;- function(u) qpois(u,lambda=3) \nmyexp &lt;- function(u) qexp(u,rate=1/sqrt(3))\nmygeom &lt;- function(u) qgeom(u,prob=(sqrt(13)-1)/6)\n\nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Sample mean distribtions approximated by the CLT\",\n  fluidRow(column(width=6,selectInput(\"dist\", \"Distribution\", \n    list(`Uniform(0,6)` = 'myunif', `Poisson(3)` = 'mypois', \n         `Exponential(0.577)` = 'myexp', `Geometric(0.434)` = 'mygeom'))),\n           column(width=6,sliderInput(\"nsamp\", \"N (sample size)\", min=1, max=50, value=5))),\n  fluidRow(column(width=12,plotOutput(\"distPlot1\"))))\n\nserver &lt;- function(input, output) {\n  samp &lt;- reactive(eval(call(input$dist,u=runif(2e4*input$nsamp))))\n  sampmeans &lt;- reactive(rowMeans(matrix(samp(),ncol=input$nsamp)))\n  output$distPlot1 &lt;- renderPlot({\n      (h &lt;- hist(sampmeans(),main='Histogram of 20,000 sample means',\n                 xlab='Sample mean',ylab='Frequency'))\n      points(h$mids,2e4*(pnorm(h$breaks[-1],mean(sampmeans()),sqrt(3)/sqrt(input$nsamp)) - \n               pnorm(h$breaks[-length(h$breaks)],mean(sampmeans()),sqrt(3)/sqrt(input$nsamp))), \n             col='#0000ff')\n      legend(x='topright',legend='CLT Approx',pch=1,col='#0000ff',bty='n')})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "ttests.html",
    "href": "ttests.html",
    "title": "The Student’s t distribution",
    "section": "",
    "text": "Motivation\nThe Central Limit Theorem provides us with an incredible tool hobbled by one glaring drawback: we are required to know the true variance \\(\\sigma^2\\) in order to estimate the range of likely values for the true mean \\(\\mu\\). But in real-world applications, if we don’t know the true mean, then it is very unlikely that we would already know the true variance.\nAt first, this difficulty might seem trivial. We know that the sample-adjusted variance is an unbiased estimator of the true variance. We can estimate \\(\\sigma^2\\) from the data as \\(\\hat{\\sigma}^2 = \\sum_i(x_i-\\bar{x})^2⁄(n-1)\\), so why can’t we use \\(\\hat{\\sigma}^2\\) in place of \\(\\sigma^2\\)?\nThe answer is, we can, but it will create new difficulties. In the original CLT, \\(\\sigma^2\\) was a fixed constant, a known value. When we estimate \\(\\sigma^2\\) from the sample as \\(\\hat{\\sigma}^2\\), we are turning that fixed constant into another random variable. We could be too high, we could be too low; our distribution of sample means might be narrower than we think, or it might be wider than we think.\nAs a result, we are no longer drawing sample means from one distribution, but instead we must actually consider a “smear” of normal distributions each with slightly different variances. Those with smaller variances will create a narrow central peak. Those with larger variances will create thick “tails” at either end. The blend of all these possibilities will distort our distribution away from the true bell curve.\nThe difference is usually very small for larger sample sizes — we can estimate \\(\\sigma^2\\) so precisely that the normal curve is barely “smeared” at all. But for small sample sizes, the difference is more pronounced. The new distribution shape is called the Student’s t-distribution, and it is controlled by a single parameter called “degrees of freedom” (we shall see why later). For univariate tests, the degrees of freedom is usually equal to your sample size minus one:\nCode\n#student's t-distribution\nplot(x=seq(-3,3,0.05),y=dnorm(seq(-3,3,0.05)),lwd=2,type='l',col='#000000',\n     ylab='Density',xlab='x',main='Standard normal and several t-distributions')\nlines(x=seq(-3,3,0.05),y=dt(seq(-3,3,0.05),df=2),col='#0000ff',lwd=2,lty=1)\nlines(x=seq(-3,3,0.05),y=dt(seq(-3,3,0.05),df=5),col='#0000ff',lwd=2,lty=\"54\")\nlines(x=seq(-3,3,0.05),y=dt(seq(-3,3,0.05),df=10),col='#0000ff',lwd=2,lty=\"33\")\nlines(x=seq(-3,3,0.05),y=dt(seq(-3,3,0.05),df=100),col='#0000ff',lwd=2,lty=\"12\")\nlegend(x='topright', lwd=2,lty=c(\"F1\",\"54\",\"33\",\"12\",\"F1\"),\n       legend=c('t(df=2)','t(df=5)','t(df=10)','t(df=100)','Normal(0,1)'),\n       col=c('#0000ff','#0000ff','#0000ff','#0000ff','#000000'))\n\n\n\n\n\n\n\n\nFigure 10.1: Several t-distributions compared to the standard normal distribution\nThe exact derivation of this distribution requires some knowledge beyond the scope of this text, but I have placed a summary of the distribution and its properties in the appendices. For now, I ask that you blithely accept that it does in fact accurately solve our problem of having to estimate the true variance with the sample variance.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Student's *t* distribution</span>"
    ]
  },
  {
    "objectID": "ttests.html#modifying-the-central-limit-theorem",
    "href": "ttests.html#modifying-the-central-limit-theorem",
    "title": "The Student’s t distribution",
    "section": "Modifying the Central Limit Theorem",
    "text": "Modifying the Central Limit Theorem\nLet’s review what the t distribution helps us to describe:\n\nData which we assume are drawn from a normal distribution\nAn unknown mean \\(\\mu\\), which is the subject of a test or confidence interval\nAn unknown variance \\(\\sigma^2\\), which we estimate from the data\n\nThe t-distribution will allow us to judge claims about the mean of the distribution while accurately adjusting for the fact that the variance is unknown.\nAnd while the t distribution may seem limited in application, since it applies only to Normal distributions with unknown variance, we already learned in The Central Limit Theorem that almost any sample mean is approximately Normally distributed around the location of its true population mean.\nSo in fact, we can use the Central Limit Theorem in situations where the variance is not known, so long as we test it against the t distribution instead of the standard Normal distribution:\n\n\n\n\n\n\nNote\n\n\n\nThe t-adjustment to the CLT: Let \\(X\\) be a random variable with mean \\(\\mu\\) and finite but unknown variance, and let \\(\\boldsymbol{x} = x_1,x_2,\\ldots,x_n\\) be a sample of independent observations of \\(X\\). Then,\n\\[\\lim_{n \\rightarrow \\infty} P\\left(\\frac{\\bar{x}-\\mu}{s_x⁄\\sqrt{n}} &lt; t\\right)=P(T_{n-1}&lt;t)\\]\nWhere \\(\\bar{x}\\) is the sample mean of \\(\\boldsymbol{x}\\), \\(s_x\\) is the sample standard deviation of \\(\\boldsymbol{x}\\) and \\(T_{n-1}\\) is the cumulative distribution function of Student’s t distribution with \\(n-1\\) degrees of freedom.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Student's *t* distribution</span>"
    ]
  },
  {
    "objectID": "ttests.html#hypothesis-tests-and-confidence-intervals-concerning-the-true-mean",
    "href": "ttests.html#hypothesis-tests-and-confidence-intervals-concerning-the-true-mean",
    "title": "The Student’s t distribution",
    "section": "Hypothesis tests and confidence intervals concerning the true mean",
    "text": "Hypothesis tests and confidence intervals concerning the true mean\nThe use cases below use either the Normal distribution or the t distribution to make claims about the true mean of a population. Many different professional situations will require you to compute these tests and intervals: they are among the most common statistical procedures used by working professionals.\n\nNotation\nIn all of the tests and intervals which follow, I will continue to use the symbols \\(\\mu\\) and \\(\\sigma^2\\) to represent the true mean and variance of a distribution (any distribution, not just a Normal distribution), and \\(\\boldsymbol{x}\\) to represent a sample. In cases where we must compare two different samples, I will use the subscripts \\(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2,\\boldsymbol{x}_1,\\) and \\(\\boldsymbol{x}_2\\). I will also use the common abbreviation for sample variance, \\(s^2=\\sum_i(x_i - \\bar{x})^2/(n-1)\\) . I will use \\(z_q^*\\) to denote the qth quantile of the standard normal distribution, that is, if \\(Z\\) is a random variable distributed Normally with mean 0 and variance 1, then \\(\\Phi(z) = P(Z \\le z) = q\\), and similarly I will use \\(t_{q,df}^*\\) to denote the qth quantile of the t-distribution with \\(df\\) degrees of freedom.\n\n\nOne-sample z-test and interval\nThis use case was previously described in The Central Limit Theorem, and is only reprinted here as a baseline for comparison.\nVery rarely, we already know the true population variance \\(\\sigma^2\\), and all we need estimate is the true mean \\(\\mu\\) from a sample. If we have a null hypothesis for the mean \\(\\mu_0\\), we can conduct a one-sample z-test for the mean using the following test statistic:\n\\[z = \\frac{\\bar{x} - \\mu_0}{\\sigma⁄\\sqrt{n}}, \\quad z \\sim \\textrm{Normal}(0,1)\\]\nWhich suggests the following \\(1 - \\alpha\\) two-sided confidence interval for the true mean:\n\\[ \\textrm{CI}_{\\mu,1-\\alpha} = \\bar{x} \\pm z^*_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\] And the following \\((1-\\alpha)\\) one-sided confidence intervals for the true mean:\n\\[\\textrm{CI}^{\\textrm{upper}}_{\\mu,1-\\alpha} = \\left[\\bar{x} - z^*_{1-\\alpha}\\frac{\\sigma}{\\sqrt{n}}, \\infty \\right] \\] \\[\\textrm{CI}^{\\textrm{lower}}_{\\mu,1-\\alpha} = \\left[-\\infty, \\bar{x} + z^*_{1-\\alpha}\\frac{\\sigma}{\\sqrt{n}} \\right]\\]\n\n\nOne-sample t-test and interval\nMuch more commonly, we must estimate \\(\\sigma^2\\) from the data. If we have a null hypothesis for the mean \\(\\mu_0\\), we can conduct a one-sample t-test for the mean using the following test statistic:\n\\[t = \\frac{\\bar{x} - \\mu_0}{s⁄\\sqrt{n}}, \\quad t \\sim T_{n-1}\\] Which is the same as the z-test above, only replacing the known value of \\(\\sigma\\) with \\(s\\) which we estimate from the data. Likewise, the two-sided confidence intervals are very similar:\n\\[ \\textrm{CI}_{\\mu,1-\\alpha} = \\bar{x} \\pm t^*_{\\alpha/2,n-1}\\frac{s}{\\sqrt{n}}\\] As are the one-sided confidence intervals:\n\\[\\textrm{CI}^{\\textrm{upper}}_{\\mu,1-\\alpha} = \\left[\\bar{x} - t^*_{1-\\alpha,n-1}\\frac{s}{\\sqrt{n}}, \\infty \\right]\\] \\[\\textrm{CI}^{\\textrm{lower}}_{\\mu,1-\\alpha} = \\left[-\\infty, \\bar{x} + t^*_{1-\\alpha,n-1}\\frac{s}{\\sqrt{n}} \\right]\\]\n\n\nOne-sample proportion test\nWe also may be asked to estimate the true proportion at which something occurs from a sample of data. Imagine that instead of a continuous or discrete variable, you are observing a sample of a 1/0 indicator variable, that is, a Bernoulli variable which takes on a value of 1 for each success and 0 for each failure. We wish to know \\(p\\), the true proportion of successes, for which the natural estimator is the sample proportion: \\(\\hat{p} = (\\sum_i x_i)/n\\).\nNote that the variance of the Bernoulli distribution is \\(p(1-p)\\) and from the basic properties of random variables we know that the variance of a sample mean of Bernoulli observations would be \\(p(1-p)/n\\). So here we have a case where a single parameter sets both the mean and the variance together. In such situations we do not need the t-distribution, which was built to address cases where estimating one parameter requires knowing the value of the other parameter. In other words, the original CLT using the Normal distribution is the best fit.\nIf we have a null hypothesis for the proportion \\(p_0\\), we may conduct a one-sample proportion test for the mean using the following test statistic:1\n\\[z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}, \\quad z \\sim \\textrm{Normal}(0,1)\\]\nThe same concepts lend themselves to two-sided confidence intervals as well. Compare this to the “exact” binomial confidence interval in Exact confidence intervals — while that Clopper-Pearson interval is usually regarded as conservative (containing the true mean more often than its nominal confidence level), the following approximation is regarded as liberal (containing the true mean less often than its nominal confidence level):\n\\[ \\textrm{CI}_{p,1-\\alpha} = \\hat{p} \\pm z^*_{\\alpha/2}\\sqrt\\frac{\\hat{p}(1-\\hat{p})}{\\sqrt{n}}\\]\nWhich in turn suggests the following one-sided confidence intervals for the true proportion:2\n\\[\\textrm{CI}^{\\textrm{upper}}_{p,1-\\alpha} = \\left[\\hat{p} - z^*_{1-\\alpha}\\sqrt\\frac{\\hat{p}(1-\\hat{p})}{\\sqrt{n}}, 1\\right] \\] \\[\\textrm{CI}^{\\textrm{lower}}_{p,1-\\alpha} = \\left[0, \\hat{p} + z^*_{1-\\alpha}\\sqrt\\frac{\\hat{p}(1-\\hat{p})}{\\sqrt{n}} \\right]\\]\nThese test statistics and confidence intervals are still approximations based on the Normal distribution, and become increasingly inaccurate for proportions very close to 0 or 1 and for low sample sizes. In some cases they may even produce impossible confidence intervals suggesting values for \\(p\\) less than 0 or greater than 1!\n\n\nTwo sample t-test, variances assumed to be equal\nWe also may be asked to estimate the true difference between two different population means, or to find out whether we have any evidence that the two populations are really different at all. While you could use a Kolmogorov-Smirnov test to compare their entire distributions (as in Identifying distributions), sometimes we are only interested in whether their means are different.\nWe must choose whether or not to assume that the variances of the two groups are equal. If we assume equal variances, we may conduct a two-sample t-test for a difference in means with “pooled variance”. Our first step is to compute a pooled sample variance, i.e. to estimate a single variance from both groups simultaneously. The formula below essentially weights the two samples’ variances by their sample sizes to create a blended (pooled) estimate:\n\\[s_p^2=\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}\\]\nIf we assume that the observations from both groups all shared this same pooled variance, then we may compute the test statistic for the difference in means as:\n\\[t = \\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{s_p^2/n_1 +s_p^2/n_2}}, \\quad t \\sim T_{n_1+n_2-2}\\] Which suggests the following two-sided confidence interval for the true difference in means:\n\\[ \\textrm{CI}_{\\mu_1-\\mu_2,1-\\alpha} = (\\bar{x}_1 - \\bar{x}_2) \\pm t^*_{\\alpha/2,n_1+n_2-2}\\sqrt{s_p^2/n_1 +s_p^2/n_2}\\]\n(Formulas for one-sided confidence intervals, and z tests for when the pooled variance is known ahead of time, can both be constructed by analogy to the prior subsections above.)\n\n\nTwo sample t-test, variances assumed to be unequal\nThe last remaining case we will treat here is when we are comparing two means but do not believe we can assume the variances of the two groups are the same.\nAs with all of the formuale in this section, the following test statistic is an approximation — here, of a very tricky distributional question called the Behrens-Fisher problem, which was only recently solved after 90 years of study. The approximation I will show is called the Welch test, and its first step involves computing a fractional “degree of freedom” parameter \\(\\nu\\) (nu), which we estimate as follows:\n\\[\\nu = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2} {\\frac{s_1^4}{n_1^2(n_1 - 1)} + \\frac{s_2^4}{n_2^2(n_2 - 1)}}\\]\nWith the degrees of freedom specified, we can now define the test statistic:\n\\[t = \\frac{\\bar{x}_1-\\bar{x}_2}{\\sqrt{s_1^2/n_1 +s_2^2/n_2}}, \\quad t \\sim T_{\\nu}\\]\nWhich suggests the following two-sided confidence interval for the true mean:\n\\[\\textrm{CI}_{\\mu_1-\\mu_2,1-\\alpha} = (\\bar{x}_1 - \\bar{x}_2) \\pm t^*_{\\alpha/2,\\nu}\\sqrt{s_1^2/n_1 +s_p^2/n_2}\\]\nThere are still other situations you may encounter, such estimating the difference between two proportions, or a paired t-test in which each observation from one group has a natural analogous counterpart in the second group. These formulae can easily be found elsewhere, and if you understand how to apply the testing procedures above, you will readily comprehend the remaining cases as well.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Student's *t* distribution</span>"
    ]
  },
  {
    "objectID": "ttests.html#footnotes",
    "href": "ttests.html#footnotes",
    "title": "The Student’s t distribution",
    "section": "",
    "text": "Note: some software packages will automatically apply small-sample corrections which slightly improve upon the CLT’s accuracy, so their results may differ from yours with this formula.↩︎\nNotice how the one-sided intervals for a mean are unbounded on one end, while for a proportion even the “one-sided” intervals are naturally bounded by 0 and 1.↩︎",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Student's *t* distribution</span>"
    ]
  },
  {
    "objectID": "rulesofthumb.html",
    "href": "rulesofthumb.html",
    "title": "Handy rules of thumb",
    "section": "",
    "text": "What’s this all about?\nIn your professional career, you will sometimes be asked for an opinion “on the spot”, with little to no time for preparation or programming. At other times, you will need to identify mistakes in calculations that seemed correct, and produced no programming errors, but which a trained eye would spot as clearly wrong. For these occasions, it helps to have a few “party tricks”, rules of thumb which put you in the ballpark of the correct answer.\nThese four rules are not essential to your understanding of parametric inference. But they might be helpful to you on your personal and professional data science journey.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Handy rules of thumb</span>"
    ]
  },
  {
    "objectID": "rulesofthumb.html#chebychevs-inequality",
    "href": "rulesofthumb.html#chebychevs-inequality",
    "title": "Handy rules of thumb",
    "section": "Chebychev’s Inequality",
    "text": "Chebychev’s Inequality\nThe first such rule is called Chebychev’s Inequality. It helps us to identify how much of a given distribution can be found within a certain distance of its mean:\n\n\n\n\n\n\nNote\n\n\n\nChebychev’s Inequality: Let \\(X\\) be a random variable with mean \\(\\mu\\) and finite, nonzero variance \\(\\sigma^2\\). Then,\n\\[P(|X-\\mu| &gt; k\\sigma) \\le \\frac{1}{k^2}\\]\n\n\nPut in plainer (though less precise) terms, no more than \\(1⁄k^2\\) of a distribution’s mass or density can be more than \\(k\\) standard deviations from its mean. For example,\n\nAt least 75% of a random variable’s values will be observed within 2 standard deviations of the mean (at most 1⁄4=25% of observations will be farther away).\nAt least 96% of a random variable’s values will be observed within 5 standard deviations of the mean (at most 1⁄25=4% of observations will be farther away).\n\nThis inequality can be very conservative. For example, you have probably heard that in the Normal distribution, more than 95% of values are within 2 standard deviations of the mean, much more than the 75% guaranteed above. But Chebychev’s Inequality will always be true, for any random variable with finite variance, and it can be useful to have an easily-remembered bound which works in every situation.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Handy rules of thumb</span>"
    ]
  },
  {
    "objectID": "rulesofthumb.html#common-quantiles-of-the-normal-distribution",
    "href": "rulesofthumb.html#common-quantiles-of-the-normal-distribution",
    "title": "Handy rules of thumb",
    "section": "Common quantiles of the Normal distribution",
    "text": "Common quantiles of the Normal distribution\nYou probably already know the second rule I will share: the relative spread of the Normal distribution:\n\nApproximately 68% of the data falls within 1 standard deviation of the mean\nApproximately 95% of the data falls within 2 standard deviations of the mean\nApproximately 99.7% of the data falls within 3 standard deviations of the mean\n\nThese numbers are only meant for Normally distributed populations. However, they are useful approximations in many other contexts, including:\n\nThe chi-squared distribution with large degrees of freedom1\nThe t distribution with moderate or large degrees of freedom2\nThe binomial distribution, with large sample sizes and probabilities of success that are neither very large nor very small3\nThe Poisson distribution, with large intensities4\n\nIn time, you will build a comfort for when this approximation is useful and when it could mislead you or your colleagues/stakeholders/audience. For now, I will illustrate its relative degree of accuracy by comparing several distributional choices.\n\n\n\nTable 11.1: Normal rule-of-thumb applied to other distributions\n\n\n\n\n\nDistribution name\nParameter choice\nProportion w/in 2 sd of mean\n\n\n\n\nNormal\n(any)\n95.4%\n\n\nStudent’s t\ndf=10\n92.7%\n\n\n\ndf=100\n95.2%\n\n\nBinomial\nN=10, p=0.1\n93.0%\n\n\n\nN=100, p=0.01\n92.1%\n\n\n\nN=100, p=0.1\n95.9%\n\n\nChi-square\ndf=10\n95.9%\n\n\n\ndf=100\n95.6%\n\n\nPoisson\n\\(\\lambda\\)=10\n96.3%\n\n\n\n\\(\\lambda\\)=100\n95.5%\n\n\nExponential\n(any)\n95.0%\n\n\nUniform\n(any)\n100.0%\n\n\n\n\n\n\nMost statistics textbooks will warn you that the statement “95% of values fall within 2 standard deviations of the mean” is only true for the Normal distribution. Those books fail to mention that the rule is actually more true for some other distributions than it is for the Normal distribution itself!",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Handy rules of thumb</span>"
    ]
  },
  {
    "objectID": "rulesofthumb.html#sample-sizes-for-accurate-estimates-of-proportions",
    "href": "rulesofthumb.html#sample-sizes-for-accurate-estimates-of-proportions",
    "title": "Handy rules of thumb",
    "section": "Sample sizes for accurate estimates of proportions",
    "text": "Sample sizes for accurate estimates of proportions\nThe third rule of thumb I have for you concerns a confidence interval for proportions. Sometimes you will be studying or collecting a relatively small sample and need to quickly understand how well the sample proportion estimates the true population proportion. Using the approximate confidence interval for a proportion found in The Student’s t distribution, you may notice that the confidence interval is widest when \\(\\hat{p}=0.5\\), and that if the sample size were 100 then we would have:\n\\[ \\textrm{CI}_{p,0.95} \\approx 0.5 \\pm 1.96\\sqrt\\frac{0.25}{100} \\approx 0.5 \\pm 0.1\\]\nWhile for a sample of 400 we would instead calculate:\n\\[ \\textrm{CI}_{p,0.95} \\approx 0.5 \\pm 1.96\\sqrt\\frac{0.25}{400} \\approx 0.5 \\pm 0.05\\]\nTogether, these two approximations form a helpful guideline when sampling from a population. If the sample truly is representative, we can be fairly certain that the sample proportion from a sample of 100 will be within 10 percentage points of the true population proportion, and that the sample proportion from a sample of 400 will be within 5 percentage points of the true population proportion.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Handy rules of thumb</span>"
    ]
  },
  {
    "objectID": "rulesofthumb.html#the-rule-of-three",
    "href": "rulesofthumb.html#the-rule-of-three",
    "title": "Handy rules of thumb",
    "section": "The Rule of Three",
    "text": "The Rule of Three\nThe last rule of thumb I wish to present is sometimes called The Rule of Three. If you observe a sample, looking for some event to occur (such as inspecting parts from an assembly line for possible defects), and the event has never occurred in your sample, then a very accurate 95% confidence interval for the event’s true proportion is \\([0,3/n]\\) where \\(n\\) is the sample size.\nSo, for example, if you are auditing voting ballots, looking for instances of ballot fraud, and you audit 300 ballots without finding a single case of fraud, then you can say that while we do not know the true incidence rate of fraud, we may be 95% confident that the rate is less than 3⁄300=1%.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Handy rules of thumb</span>"
    ]
  },
  {
    "objectID": "rulesofthumb.html#footnotes",
    "href": "rulesofthumb.html#footnotes",
    "title": "Handy rules of thumb",
    "section": "",
    "text": "Where \\(\\sigma = \\sqrt{2df}\\).↩︎\nWhere \\(\\sigma \\approx 1\\).↩︎\nWhere \\(\\sigma = \\sqrt{Np(1-p)}\\).↩︎\nWhere \\(\\sigma = \\sqrt{\\lambda}\\).↩︎",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Handy rules of thumb</span>"
    ]
  },
  {
    "objectID": "simpleregression.html",
    "href": "simpleregression.html",
    "title": "Simple regression",
    "section": "",
    "text": "Simple regression, unsolved\nThe previous sections have focused on univariate inference, i.e. on determining the distribution of a single column of numbers. Let’s advance to a more complicated topic: modeling one column of numbers (the response variable, sometimes called \\(Y\\)) as a function of one or more other columns (the predictors, sometimes called the \\(X\\)s). Many different models share this generic description, but ordinary least square (OLS) regression remains the most popular method used by novice and expert practitioners alike. OLS regression brings advantages and disadvantages, and many extensions which broaden its applicability, but we will learn those in due time. First, we must start small.\nLet’s start with the case of one response variable (\\(Y\\)) and only a single predictor (\\(X\\)). This framework is sometimes called “simple” regression as opposed to “multiple” regression which uses two or more predictors. In most cases, we set our model up as follows:\nFor now, I am not going to tell you how to find the best values for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). We will discuss solution methods later, but first I want to cement their concepts.\nCode\nX &lt;- c(1,2,3,4,5)\nY &lt;- c(6,8,6,2,3)\n\nplot(X,Y,pch=19,main='Simple regression through five points')\nabline(reg=lm(Y~X),lty=2,lwd=2,col='grey50')\npoints(1:5,lm(Y~X)$fitted.values,col='grey50')\nsegments(x0=X,y0=Y,x1=X,y1=lm(Y~X)$fitted.values,col='#0000ff',lwd=2,lty=3)\nlegend(x='topright',lty=c(NA,NA,2,3),pch=c(19,1,NA,NA),lwd=c(NA,NA,2,2),\n       col=c('#000000','grey50','grey50','#0000ff'),bty='n',\n       legend=c('Observations','Fitted Values','Line of Best Fit','Residuals'))\ntext(x=1.5,y=3,labels=expression(hat(beta)[0]==8.6))\ntext(x=1.5,y=2.3,labels=expression(hat(beta)[1]==-1.2))\n\n\n\n\n\n\n\n\nFigure 13.1: Illustration of simple regression concepts\nMost readers will have seen something like this before, I know. Still, let’s quickly review:\nThese concepts will be useful to us no matter which estimation method we use to fit the model. And already, we can do a few different things with the model:\nThese are useful beginnings, but we cannot do much more without a formal solution method for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple regression</span>"
    ]
  },
  {
    "objectID": "simpleregression.html#simple-regression-unsolved",
    "href": "simpleregression.html#simple-regression-unsolved",
    "title": "Simple regression",
    "section": "",
    "text": "Note\n\n\n\nSuppose a random variable \\(Y\\) is a function of two other random variables: \\(X\\), which we do observe, and \\(\\varepsilon\\), which we do not observe. If we assume that two constants \\(\\beta_0\\) and \\(\\beta_1\\) exist such that:\n\\[Y = \\beta_0 + \\beta_1 X + \\varepsilon\\]\nThen we may say that \\(X\\) is the predictor, that \\(\\varepsilon\\) is the error term, and that \\(\\beta_0\\) and \\(\\beta_1\\) are the intercept and slope, respectively, of a linear model for \\(Y\\). If samples of \\(X\\) and \\(Y\\) are observed with values \\(\\boldsymbol{x} = x_1,x_2,\\ldots,x_n\\) and \\(\\boldsymbol{y} = y_1,y_2,\\ldots,y_n\\), then we may use these samples to estimate the true betas from the data, creating \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), and leading us to a set of fitted values, \\(\\hat{\\boldsymbol{y}}\\), and residuals, \\(\\boldsymbol{e}\\):\n\\[\\hat{\\boldsymbol{y}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\boldsymbol{x}\\] \\[\\begin{aligned} \\boldsymbol{e} &= \\boldsymbol{y} - \\boldsymbol{\\hat{y}} \\\\ &= \\boldsymbol{y} - \\beta_0 - \\beta_1 \\boldsymbol{x}\\end{aligned}\\] The fitted values are estimators of \\(\\boldsymbol{y}\\) and the residuals are estimators of \\(\\varepsilon\\). The usefulness and exact properties of these estimators are all dependent on the method used to determine \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), which together create the “line of best fit” between the samples \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\), or between the variables \\(X\\) and \\(Y\\).\n\n\n\n\n\n\nThe observations, or the data, or \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{x}\\), are plotted as black dots. These are the only “real” elements of the graph above. They may be unrepresentative, they may be misunderstood, our model for them might be poor, but they were observed.\nThe linear model, or the line of best fit, is plotted as a grey dashed line. This line offers exactly the same information as \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) — the intercept and slope of the line — but it also provides a way of identifying visually what value of \\(Y\\) we would predict for a given value of \\(X\\).\nThe fitted values, or the predictions, or \\(\\hat{\\boldsymbol{y}}\\), are plotted as grey circles along the line of best fit. Even though we observed these points elsewhere, our model would suggest that they “should” be found on the line of best fit, and that new observations with the same \\(x\\)-values would tend to cluster there.\nThe residuals, or \\(\\boldsymbol{e}\\), are plotted as dotted blue lines. These are the vertical distances between our observations \\(\\boldsymbol{y}\\) and our predictions \\(\\hat{\\boldsymbol{y}}\\). They are sometimes called the errors, but we must be careful with that term, because it is also used for other, similar concepts.\nThe estimated intercept, or \\(\\hat{\\beta_0}\\), is displayed in the bottom left. When \\(X=0\\), this is the value predicted for \\(Y\\).\nThe estimated slope, or \\(\\hat{\\beta_1}\\), is also displayed at bottom left. A one-unit increase in \\(X\\) will, on average, produce a \\(\\hat{\\beta_1}\\)-unit change in Y. Positive values of \\(\\hat{\\beta_1}\\) indicate a direct relationship (\\(X\\) and \\(Y\\) go up and down together), while negative values indicate an inverse relationship (as \\(X\\) goes up, \\(Y\\) goes down, and vice versa).\n\n\n\nWe can predict what \\(Y\\) will be, even for values of \\(X\\) we have not seen. For any given \\(X\\) value \\(x_0\\), we would say \\(\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0\\).\nWe can identify apparent outliers, by examining those observations which are farthest from their predictions.\nWe can measure the performance of our model in several ways. For example, we can compute the root mean square error (RMSE), which is similar to the standard deviation of our residuals:\n\\[\\textrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_i(y_i - \\hat{y}_i)^2} \\] Alternatively, we could compute the mean absolute error (MAE), which measures how large our average error is:\n\\[\\textrm{MAE}=\\frac{1}{n}\\sum_i|y_i - \\hat{y}_i|\\]",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple regression</span>"
    ]
  },
  {
    "objectID": "simpleregression.html#simple-regression-solved-by-least-squares",
    "href": "simpleregression.html#simple-regression-solved-by-least-squares",
    "title": "Simple regression",
    "section": "Simple regression, solved by least squares",
    "text": "Simple regression, solved by least squares\nHow do we know that a line fit to the data is the line of best fit? Statisticians don’t really know the answer to this question. What seems best to some people might not seem best to others. For example, some people might like “the line which creates the smallest mean absolute error”, but other people might prefer “the line which creates the smallest maximum percentage error”. And yet, in every textbook, you will find a different method, called “The Method of Least Squares”.1 Statisticians use this method for two reasons:\n\nIt has “nice mathematical properties”, which is how academics say that it’s easy to calculate and connects smoothly to work which came before2\nIt matches the parameter estimates of another common method, maximum likelihood, without requiring the same assumptions about the data\n\nI’ll proceed straightaway to the solution:3\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{x}\\) be paired samples of two random variables and define \\(\\hat{\\boldsymbol{y}}\\) to be a set of predictions for \\(\\boldsymbol{y}\\) according to the linear model:\n\\[\\hat{\\boldsymbol{y}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\boldsymbol{x}\\]\nDefine the residual sum of squares (RSS), also known as the sum of squared errors (SSE), as follows:\n\\[\\textrm{RSS} = \\textrm{SSE} = \\sum_i (y_i - \\hat{y}_i)^2 =  \\sum_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\\] The Method of Least Squares chooses the estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimize RSS:\n\\[\\hat{\\boldsymbol{\\beta}}_\\textrm{LS} = \\mathop{\\mathrm{argmin}}_{\\hat{\\beta_0},\\,\\hat{\\beta}_1} \\,\\textrm{RSS}\\]\nFor this specific model (simple regression), it can be proven that,\n\\[\\hat{\\beta}_{0,LS} = \\bar{y} - \\hat{\\beta}_{1,LS} \\bar{x}\\]\n\\[\\hat{\\beta}_{1,LS} = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\textrm{Cov}(\\boldsymbol{y},\\boldsymbol{x})}{\\mathbb{V}(\\boldsymbol{x})}\\]\n\n\nThis Least Squares method provides a convenient way to quickly determine the line of best fit through the data, because the parameters \\(\\hat{\\boldsymbol{\\beta}}_\\textrm{LS}\\) can be computed directly from descriptive statistics of the data: \\(\\bar{x},\\bar{y},\\mathbb{V}(\\boldsymbol{x}),\\) and \\(\\textrm{Cov}(\\boldsymbol{y},\\boldsymbol{x})\\). No derivatives, no iterative algorithms, no distributional assumptions.\nFor the case of simple regression, we can easily visualize the residual sum of squares by drawing literal squares around each residual. Since the residuals form one side of each square, the area of each square will be square of the residual, and the total sum of squares will be the combined area of all the squares. The figure below illustrates two different models for the same data found in the previous figure. The data are identical on both sides. On the left side we see a Least Squares solution and the associated residual squares. On the right side we see a second model which may be an acceptable fit, but which creates a larger residual sum of squares.\n\n\nCode\npar(mfrow=c(1,2))\nplot(X,Y,pch=19,main=\"Least Squares 'best' model\",\n     asp=1,xlim=c(-1.5,5.5))\nabline(reg=lm(Y~X),lty=2,lwd=2,col='grey50')\nsegments(x0=X,y0=Y,x1=X,y1=lm(Y~X)$fitted.values,col='#0000ff',lwd=2,lty=3)\nrect(xleft=c(-0.4,2,3,2.2,5),ybottom=c(6,6.2,5,2,2.6),\n     xright=c(1,3.8,4,4,5.4),ytop=c(7.4,8,6,3.8,3),\n     col='#0000ff79',border=NA)\ntext(x=0.1,y=4,labels=expression(hat(Y) == 8.6-1.2*X))\ntext(x=-0.3,y=3.2,labels=expression(Area == 9.6),col='#0000ff')\n\nplot(X,Y,pch=19,main=\"Alternative model\",\n     asp=1,xlim=c(-1.5,5.5))\nabline(a=7,b=-1,lty=2,lwd=2,col='grey50')\nsegments(x0=X,y0=Y,x1=X,y1=7-X,col='#0000ff',lwd=2,lty=3)\nrect(xleft=c(-1,3,3,5),ybottom=c(5,4,2,2),\n     xright=c(2,5,4,6),ytop=c(8,6,3,3),\n     col='#0000ff79',border=NA)\ntext(x=1,y=4,labels=expression(hat(Y) == 7-1*X))\ntext(x=1,y=3.2,labels=expression(Area == 15),col='#0000ff')\n\n\n\n\n\n\n\n\nFigure 13.2: Least Squares comparison of two models\n\n\n\n\n\nWe now have a modeling process: given a response vector \\(\\boldsymbol{y}\\) and a predictor vector \\(\\boldsymbol{x}\\), we can use the Least Squares method to find the intercept and slope of the “best” line through the data. We can create fitted values \\(\\hat{y}_i\\) for each observation \\(i\\). We can measure the discrepancy between these fitted values and the actual data, creating the residuals. We can even use the model to predict values for \\(Y\\) at locations of \\(X\\) not observed in the data.\nBut we could do much more with this model. Consider that so far, I haven’t actually used any distributional assumptions. There have been no probability calculations, no random variables, just two columns of numbers and a formula for the line between them.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple regression</span>"
    ]
  },
  {
    "objectID": "simpleregression.html#footnotes",
    "href": "simpleregression.html#footnotes",
    "title": "Simple regression",
    "section": "",
    "text": "Sometimes abbreviated LS.↩︎\nYou see, we are a pretty lazy bunch, just like professionals in other fields.↩︎\nA proof can be found in the appendices.↩︎\nTextbooks often present this differently, writing \\(Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\\), with \\(varepsilon \\sim \\mathrm{Normal}(0,\\sigma^2)\\). This notation is mathematically equivalent, but I want to emphasize that \\(Y\\) is conditionally Normal for a fixed choice of \\(X\\).↩︎\nThe Gauss-Markov Theorem, for those seeking more advanced treatments of this topic.↩︎",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple regression</span>"
    ]
  },
  {
    "objectID": "simpleregression.html#simple-regression-solved-by-maximum-likelihood",
    "href": "simpleregression.html#simple-regression-solved-by-maximum-likelihood",
    "title": "Simple regression",
    "section": "Simple regression, solved by maximum likelihood",
    "text": "Simple regression, solved by maximum likelihood\nLet’s now move away from Least Squares and try a different approach, where we assume that the samples of data in front of us came from a known probability distribution. First, let us suppose we have a single normally distributed variable, \\(Y\\):\n\\[Y \\sim \\mathrm{Normal}(\\mu,\\sigma^2)\\] Easy enough: we have seen this before. Now let’s add a wrinkle: each observation of \\(Y\\) will have a different theoretical mean. Same variance, but a different mean:\n\\[Y_i \\sim \\mathrm{Normal}(\\mu_i,\\sigma^2)\\]\nFinally, let’s specify where that mean will be. Suppose another random variable \\(X\\), with any distribution you like (or no particular distribution at all). We will use the observations of \\(X\\) to determine the mean for each observation of \\(Y\\):\n\\[\\mu_i = \\beta_0 + \\beta_1 X_i\\]\nWhere \\(\\beta_0\\) and \\(\\beta_1\\) are unknown constants. So if you like, we could re-write \\(Y\\) as follows:4\n\\[Y_i \\sim \\mathrm{Normal}(\\beta_0 + \\beta_1 X_i,\\sigma^2)\\]\nNow imagine that we observe samples of both \\(X\\) and \\(Y\\), which we write as \\(\\boldsymbol{x} = x_1,x_2,\\ldots,x_n\\) and \\(\\boldsymbol{y} = y_1,y_2,\\ldots,y_n\\). How can we estimate the three unknown parameters \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\)? Well, I didn’t teach you maximum likelihood estimation for nothing… if you’d rather not work through the math, you can skip to the conclusions at the end, but for those who enjoy the journey, first we will write out the likelihood function:\n\\[\\begin{aligned} \\mathcal{L}(\\beta_0,\\beta_1,\\sigma^2│\\boldsymbol{x},\\boldsymbol{y}) &= \\prod_{i=1}^n f(y_i│x_i,\\beta_0,\\beta_1,\\sigma^2) \\\\ &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(y_i-\\mu_i )^2} \\\\ &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2}\\end{aligned}\\]\nThen we will take the log-likelihood, which should simplify the steps that follow:\n\\[\\begin{aligned} \\mathcal{l}(\\beta_0,\\beta_1,\\sigma^2 | \\boldsymbol{x},\\boldsymbol{y}) &= \\log \\mathcal{L}(\\beta_0,\\beta_1,\\sigma^2 | \\boldsymbol{x},\\boldsymbol{y}) \\\\ &= \\log \\prod_i \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2} \\\\ &= \\sum_i \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2} \\\\ &= \\sum_i \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\sum_i \\frac{1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\ &= -\\frac{n}{2} \\log 2\\pi -\\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 \\end{aligned}\\]\nSolution Method 1: Looking at the last line above, notice that the values of \\(\\beta_0\\) and \\(\\beta_1\\) which maximize the log-likelihood are the same as the values which would minimize \\(\\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2\\) — the residual sum of squares! This is an extremely fortunate and important result: the Least Squares solution for the betas and the Maximum Likelihood solution for the betas are mathematically equal.\nSolution Method 2: If you want the full solution, then we must take partial derivatives of the log-likelihood with respect to each parameter, set them equal to 0, and solve to find the maximum likelihood estimators. First, we start with \\(\\beta_0\\):\n\\[\\begin{aligned} \\frac{\\partial\\mathcal{l}}{\\partial\\beta_0} &= \\frac{\\partial}{\\partial\\beta_0}\\left[-\\frac{1}{2\\sigma^2} \\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 \\right] \\\\ &= \\frac{1}{\\sigma^2} \\sum_i (y_i - \\beta_0 - \\beta_1 x_i) \\end{aligned}\\]\n\\[\\begin{aligned} \\frac{\\partial\\mathcal{l}}{\\partial\\beta_0} = 0 &\\Longrightarrow \\sum_i (y_i - \\beta_0 - \\beta_1 x_i) = 0 \\\\ &\\Longrightarrow n\\bar{y} - n\\beta_0 - n\\beta_1\\bar{x} = 0 \\\\ &\\Longrightarrow \\hat{\\beta}_0 = \\bar{y} - \\beta_1\\bar{x}\\end{aligned}\\]\nNotice that this “solution” for \\(\\hat{\\beta}_0\\) seems incomplete. It requires us to know \\(\\beta_1\\), which hasn’t been determined yet. However, we will be able to use this information to help solve for \\(\\hat{\\beta}_1\\):\n\\[\\begin{aligned} \\frac{\\partial\\mathcal{l}}{\\partial\\beta_1} &= \\frac{\\partial}{\\partial\\beta_1}\\left[-\\frac{1}{2\\sigma^2} \\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 \\right] \\\\ &= \\frac{1}{\\sigma^2} \\sum_i (y_i - \\beta_0 - \\beta_1 x_i) x_i \\\\ &= \\frac{1}{\\sigma^2} \\sum_i (y_i - (\\bar{y} - \\beta_1\\bar{x}) - \\beta_1 x_i) x_i \\\\ &= \\frac{1}{\\sigma^2} \\sum_i \\left((y_i - \\bar{y}) - \\beta_1(x_i - \\bar{x})\\right) x_i \\\\ &= \\frac{1}{\\sigma^2} \\sum_i (y_i - \\bar{y}) x_i - \\beta_1 \\frac{1}{\\sigma^2} \\sum_i (x_i - \\bar{x}) x_i \\end{aligned}\\]\nThrough a little algebraic flim-flam, covered in the appendices, we can re-write this as,\n\\[= \\frac{1}{\\sigma^2} \\sum_i (y_i - \\bar{y})(x_i - \\bar{x}) - \\beta_1 \\frac{1}{\\sigma^2} \\sum_i (x_i - \\bar{x})(x_i - \\bar{x})\\] And complete the solution:\n\\[\\begin{aligned} \\frac{\\partial\\mathcal{l}}{\\partial\\beta_1} = 0 &\\Longrightarrow \\beta_1 \\frac{1}{\\sigma^2} \\sum_i (x_i - \\bar{x})^2 = \\frac{1}{\\sigma^2} \\sum_i (y_i - \\bar{y})(x_i - \\bar{x}) \\\\ &\\Longrightarrow \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\end{aligned}\\]\nFinally, we solve for the error variance, \\(\\hat{\\sigma}^2\\):\n\\[\\begin{aligned} \\frac{\\partial\\mathcal{l}}{\\partial\\sigma^2} &= \\frac{\\partial}{\\partial\\sigma^2}\\left[-\\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 \\right] \\\\ &= -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 \\end{aligned}\\]\n\\[\\begin{aligned} \\frac{\\partial\\mathcal{l}}{\\partial\\sigma^2} = 0 &\\Longrightarrow \\frac{n}{2\\sigma^2} = \\frac{1}{2\\sigma^4}\\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\ &\\Longrightarrow \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2\\end{aligned}\\]\nAll of this yields the following results:\n\n\n\n\n\n\nNote\n\n\n\nSuppose a random variable \\(X\\) and a Normally distributed variable \\(Y\\) with mean \\(\\mu = \\beta_0 + \\beta_1 X\\) and variance \\(\\sigma^2\\). If samples of \\(X\\) and \\(Y\\) are observed with values \\(\\boldsymbol{x} = x_1,x_2,\\ldots,x_n\\) and \\(\\boldsymbol{y} = y_1,y_2,\\ldots,y_n\\), then the maximum likelihood estimators for the unknown parameters are:\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\n\\[\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\textrm{Cov}(\\boldsymbol{y},\\boldsymbol{x})}{\\mathbb{V}(\\boldsymbol{x})}\\]\n\\[\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 = \\frac{\\mathrm{RSS}}{n}\\]\n\n\nWe have used maximum likelihood estimation to produce perfectly fine guesses for our betas (slopes and intercept). In fact, with a little further statistical training5, you can prove that these are the best linear unbiased estimators (BLUE):\n\nBest meaning that the variance of this estimator is provably as low as possible; no other unbiased estimator can have a lower variance.\nLinear meaning that the estimator is formed from a linear combination of the data in the sample, essentially \\(\\hat{\\beta}_j = \\sum_i c_i y_i\\) for all \\(j\\), with the weights \\(c_i\\) being a function of the predictor variables.\nUnbiased meaning, as stated previously, that the estimates produced by these methods are, on average, equal to the true parameters, and neither too high nor too low.\nEstimators, which I hope needs no further explanation at this point.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple regression</span>"
    ]
  },
  {
    "objectID": "simpleregression.html#visualizer",
    "href": "simpleregression.html#visualizer",
    "title": "Simple regression",
    "section": "Visualizer",
    "text": "Visualizer\nUsing the same simple dataset above, try choosing different values for the intercept and slope, and see how the MLE choices of \\(\\hat{\\beta}_0 = 8.6\\) and \\(\\hat{\\beta}_1 = -1.2\\) both maximize the likelihood function and minimize the residual sum of squares (RSS):\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 960\n\nlibrary(shiny)\nlibrary(bslib)\n\nX &lt;- c(1,2,3,4,5)\nY &lt;- c(6,8,6,2,3)\nYhat &lt;- function(betas) betas[1] + betas[2]*X\nRSS &lt;- function(betas) sum((Y-Yhat(betas))^2)\nLL &lt;- function(betas){\n  sigma2 &lt;- RSS(betas)/5\n  return(-5*log(2*pi)/2 - 5*log(sigma2)/2 - RSS(betas)/(2*sigma2))}\nxgrid &lt;- seq(5,10,0.05)\nygrid &lt;- seq(-3,0,0.05)\nRSSgrid &lt;- matrix(apply(cbind(rep(xgrid,times=length(ygrid)),rep(ygrid,each=length(xgrid))),1,RSS),ncol=length(ygrid))\nLLgrid &lt;- matrix(apply(cbind(rep(xgrid,times=length(ygrid)),rep(ygrid,each=length(xgrid))),1,LL),ncol=length(ygrid))\n\nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Optimizing Likelihood and RSS\",\n  fluidRow(column(width=6,sliderInput(\"beta0\", \"Beta0 (Intercept)\", min=5, max=10, value=8.6,step=0.1)),\n           column(width=6,sliderInput(\"beta1\", \"Beta1 (Slope)\", min=-3, max=0, value=-1.2,step=0.1))),\n  fluidRow(column(width=12,plotOutput(\"distPlot1\"))),\n  fluidRow(column(width=6,plotOutput(\"distPlot2\")),\n           column(width=6,plotOutput(\"distPlot3\"))))\n\nserver &lt;- function(input, output) {\n  output$distPlot1 &lt;- renderPlot({\n    plot(X,Y,pch=19)\n    abline(input$beta0,input$beta1,lty=2,lwd=2,col='grey50')\n    points(1:5,Yhat(c(input$beta0,input$beta1)),col='grey50')\n    segments(x0=X,y0=Y,x1=X,y1=Yhat(c(input$beta0,input$beta1)),col='#0000ff',lwd=2,lty=3)})\n  output$distPlot2 &lt;- renderPlot({\n    contour(xgrid,ygrid,RSSgrid, main='RSS for Chosen Betas',\n            xlab='Beta0 (Intercept)',ylab='Beta1 (Slope)')\n    points(x=input$beta0,y=input$beta1,col='#0000ff')})\n  output$distPlot3 &lt;- renderPlot({\n    contour(xgrid,ygrid,LLgrid, main='Log-Lik for Chosen Betas',\n            xlab='Beta0 (Intercept)',ylab='Beta1 (Slope)')\n    points(x=input$beta0,y=input$beta1,col='#0000ff')})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simple regression</span>"
    ]
  },
  {
    "objectID": "multiplemle.html",
    "href": "multiplemle.html",
    "title": "Multiple regression",
    "section": "",
    "text": "Terminology and solution\nSo far, we have dealt with simple regression, which uses only a single predictor variable \\(X\\). The math of this model is easier to present and also easier to illustrate, but this perspective limits the theory we can present and the types of problems we can solve. Now we will consider the more general form of multiple regression.\nConsider, as before, a random variable \\(Y\\) distributed according to a Normal distribution with a fixed variance \\(\\sigma^2\\) and a mean dependent on the values of one or more random variables \\(X_1,\\ldots,X_k\\). In this notation, \\(k\\) represents the number of predictors, and for each \\(j,\\; 1 \\le j \\le k,\\; X_j\\) is a different random variable.\nNow suppose that we have samples from these random variables: some vector of our response variable \\(\\boldsymbol{y}\\) along with predictor vectors \\(\\boldsymbol{x}_1,\\ldots,\\boldsymbol{x}_k\\). Each vector contains \\(n\\) observations. We could write out a relationship arithmetically, observation by observation:\n\\[y_i = \\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}+\\varepsilon_i,\\quad \\varepsilon_i \\sim \\textrm{Normal} (0,\\sigma^2)\\]\nHowever, we can save space and afford ourselves new analytical tools by using a linear algebraic notation. First, note that \\(\\beta_0 = \\beta_0 \\cdot 1\\), so that we may write\n\\[\\begin{aligned} y_1 &= \\beta_0 1 + \\beta_1 x_{1,1} + \\cdots + \\beta_k x_{k,1}+ \\varepsilon_1 \\\\ y_2 &= \\beta_0 1 + \\beta_1 x_{1,2} + \\cdots + \\beta_k x_{k,2}+ \\varepsilon_2 \\\\ \\vdots \\\\ y_n &= \\beta_0 1 + \\beta_1 x_{1,n} + \\cdots + \\beta_k x_{k,n}+ \\varepsilon_n \\end{aligned}\\] Next, we collect these equations together by collapsing them vertically. Define the vector of ones \\(1\\!\\!1 = (1,1,\\ldots,1)\\) with total length \\(n\\). Then we can consider this vector of ones as a predictor in the model, multiplied by a beta just like all the other predictors. This creates a matrix of predictors formed by combining each predictor vector, including \\(1\\!\\!1\\):\n\\[\\mathbf{X} = [1\\!\\!1,\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\ldots,\\boldsymbol{x}_k]= \\left[ \\begin{array}{cccc} 1 & x_{1,1} & \\cdots & x_{k,1} \\\\ 1 & x_{1,2} & \\cdots & x_{k,2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{1,n} & \\cdots & x_{k,n} \\end{array}\\right]\\]\nNow we can collapse the entirety of the model into a few symbols. The response vector will be the matrix-product of the predictor matrix and a vector of unknown betas, plus a set of Normally distributed errors:\n\\[\\boldsymbol{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\quad \\varepsilon_i \\sim \\textrm{Normal}(0,\\sigma^2)\\]\nWe could also write this as:1\n\\[\\boldsymbol{y} \\sim \\textrm{MVNormal}(\\mathbf{X}\\boldsymbol{\\beta},σ^21\\!\\!1 )\\] Notice in either case that \\(1\\!\\!1\\), the column of ones, is being treated just like any other predictor, and that \\(\\beta_0\\), the intercept, is being treated just like any other beta/slope.\nNow we wish to find the data-based estimators for \\(\\boldsymbol{\\beta}\\), that is, the intercept and slope(s) of the line-of-best-fit for our data.2 As before, it can be shown that the least squares and maximum likelihood methods both find exactly the same solutions, since they end up minimizing the same sum. A sketch of the proof can be found in the appendix, but the basic result is found below:\n\\[\\hat{\\boldsymbol{\\beta}}_{ML} = \\hat{\\boldsymbol{\\beta}}_{LS} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{y}\\]\nIn plainer terms, maximum likelihood and least squares agree on a solution for the betas which can be computed directly (and in one pass) from your data. Compared to machine learning models which often require a huge amount of storage space and processing power, this is an extremely lightweight method which produces exact, replicable solutions from almost any environment that can read the data.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiplemle.html#bias-in-the-estimator-for-error-variance",
    "href": "multiplemle.html#bias-in-the-estimator-for-error-variance",
    "title": "Multiple regression",
    "section": "Bias in the estimator for error variance",
    "text": "Bias in the estimator for error variance\nWe now have a solid way of estimating the intercept and slopes of a multiple regression line, i.e. the betas. Both maximum likelihood and least squares techniques arrive at the same set of betas. However, least squares techniques offer no way for us to estimate the error variance \\(\\sigma^2\\), because the least squares solution does not place any distributional assumption on these errors. We can estimate \\(\\sigma^2\\) using maximum likelihood, but its estimator for the error variance is provably biased; specifically, it underestimates the true variance. The proof of this must be left for an appendix, as the general case involves a good deal of vector calculus and linear algebra, but I can supply the intuition:\n\nFirst, recall that our uncertainty in an estimate shrinks as we gain more data. That’s why calculations for variance typically divide by \\(n\\) (the sample size). As \\(n\\) increases, our uncertainty decreases.\nHowever, our estimator for the \\(\\sigma^2\\) is necessarily a function of the estimated betas, \\(\\hat{\\boldsymbol{\\beta}}\\) and not the true betas, \\(\\boldsymbol{\\beta}\\). These estimated betas are also a function of the data (\\(\\boldsymbol{y}\\) and \\(\\mathbf{X}\\)).\nTherefore, we don’t really have \\(n\\) fully independent sets of observations from which to estimate \\(\\sigma^2\\). The values of some of those observations are constrained by our choices for \\(\\hat{\\boldsymbol{\\beta}}\\), which is an input for our estimator \\(\\hat{\\sigma}^2\\).\nThis interdependence between estimators means that a variance estimated using \\(\\hat{\\boldsymbol{\\beta}}\\) provides us with less certainty than we would expect from \\(n\\) observations; effectively, only the same certainty as \\(n-k-1\\) fully independent observations would provide us.3\n\n\n\n\n\n\n\nNote\n\n\n\nAssume a Normally-distributed variable \\(Y\\) whose mean is determined by a vector of ones \\(1\\!\\!1\\) and one or more other random variables \\(\\mathbf{X} = X_1,\\ldots,X_k\\) such that \\(\\mathbf{X} = [1\\!\\!1,\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\ldots,\\boldsymbol{x}_k]\\) and:\n\\[\\boldsymbol{y} \\sim \\mathrm{MVNormal}(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^21\\!\\!1)\\]\nFurthermore, assume that the parameters \\(\\boldsymbol{\\beta}\\) are estimated as \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{y}\\), that \\(\\boldsymbol{y}\\) is estimated as \\(\\hat{\\boldsymbol{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) and that RSS takes its usual meaning, i.e. \\(\\sum_i (y_i - \\hat{y}_i)^2\\). Then we may say that the following estimator for \\(\\sigma^2\\) is unbiased:\n\\[\\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{n-k-1}\\]\n\n\nFrom here on, we will abandon the biased maximum likelihood estimator \\(\\hat{\\sigma}^2_{\\mathrm{ML}} = \\mathrm{RSS}/n\\) and instead use the unbiased estimator \\(\\hat{\\sigma}^2 = \\mathrm{RSS}/(n - k - 1)\\).",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiplemle.html#new-capabilities-made-possible-by-distributional-assumptions",
    "href": "multiplemle.html#new-capabilities-made-possible-by-distributional-assumptions",
    "title": "Multiple regression",
    "section": "New capabilities made possible by distributional assumptions",
    "text": "New capabilities made possible by distributional assumptions\nMoving from a least squares approach to a maximum likelihood approach requires us to introduce strong assumptions about the data, including that \\(Y\\) is conditionally Normal and that its mean varies linearly with the values of one or more predictors \\(X\\). Assumptions such as these are limiting, because the model may no longer fit certain use cases that could still be approached through a least-squares perspective. However, the tradeoff for these limitations is that we can assess and interpret our models in new ways, and solve new types of problems that we couldn’t before. This section focuses on how we use the distributional assumptions to gain new capabilities.\nWe will cover the distributional assumptions of linear regression in more detail in the next chapter, but for now let me summarize some of the most important assumptions:\n\n\n\n\n\n\nNote\n\n\n\nAssume a relationship between a response variable \\(Y\\) and one or more predictor variables \\(\\boldsymbol{X}\\) (typically but not necessarily including the vector of ones, \\(X_0=1\\!\\!1\\)). We say that the relationship between the response and the predictors satisfies the assumptions of ordinary least squares (OLS) regression if:\n\n\\(Y\\) varies linearly with \\(\\boldsymbol{X}\\). Specifically, we assume that the mean of \\(Y\\) can be expressed as a linear combination of the predictors:\n\\[\\mathbb{E}[Y|\\boldsymbol{X}] = \\hat{Y} = \\hat{\\mu}_{Y|\\boldsymbol{X}} = \\sum_{j=0}^k \\beta_j X_j\\]\nConditioned on \\(\\boldsymbol{X}\\), \\(Y\\) is distributed Normally. We can write this in several ways:\n\\[P(Y \\le y|\\boldsymbol{X} = \\boldsymbol{x}) = \\Phi\\left(\\frac{y - \\hat{\\mu}_{Y|\\boldsymbol{x}}}{\\sigma}\\right)\\]\n\\[Y \\sim \\mathrm{Normal}(\\boldsymbol{X} \\circ \\boldsymbol{\\beta},\\sigma^2)\\]\n\\[y_i = \\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}  + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathrm{Normal}(0,\\sigma^2 )\\]\nEach of these equations are consistent with each other, and reinforce the abstraction which lurks behind our data. The sample \\(\\boldsymbol{y}\\) which we are studying was generated by a random variable \\(Y\\) according to an unknown (but estimable) probability distribution.\nThe error variance displays homoskedasticity. The variance of \\(Y\\) around its mean is exactly the same size for all predictor choices. This does not mean that the errors are all the same size, simply that they are all drawn from the same distribution. Rather than saying that each observation has a unique error variance, \\(\\sigma_i^2\\), we say they all share the same error variance: \\(\\sigma^2\\).\nThe errors are independent of each other and of the predictors. Mathematically we could write that,\n\\[P(\\varepsilon_a \\le e_1,\\varepsilon_b \\le e_b) = P(\\varepsilon_a\\le e_1) P(\\varepsilon_b \\le e_b) \\;\\forall a \\ne b\\]\n\\[\\boldsymbol{\\varepsilon} \\bot X_j\\; \\forall k, 1 \\le j \\le k\\]\nBut here is a case where I think plain language (bolded, above) is the most straightforward. As an aside, many of the results which follow will still be true under the slightly weaker condition that the errors are uncorrelated with each other rather than truly independent.\n\n\n\nAssumptions 2–4 above are sometimes known as the IID assumptions for linear regression because the errors are required to be (i)dentically and (i)ndependently (d)istributed normally. With these assumptions, the linear regression model gains some new analytical features.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiplemle.html#confidence-intervals-and-tests-for-the-locations-of-the-betas",
    "href": "multiplemle.html#confidence-intervals-and-tests-for-the-locations-of-the-betas",
    "title": "Multiple regression",
    "section": "Confidence intervals and tests for the locations of the betas",
    "text": "Confidence intervals and tests for the locations of the betas\nPreviously, we used Least Squares to find point estimates for the beta parameters (the intercept and slopes). We identified the line of best fit, but could not measure which lines were also reasonably well-fit to the data, or which lines could plausibly be the real relationship between the response and the predictors. Now, with a distributional assumption for \\(Y\\), we can.\n\n\n\n\n\n\nNote\n\n\n\nAssume a relationship between a response variable \\(Y\\) and \\(k\\) predictor variables \\(X_1,\\ldots,X_k\\) which meet the assumptions of ordinary least squares (OLS) regression, and use all the usual notation so far for its parameters, distributional form, and samples\n\nEach estimated intercept or slope _j is Normally distributed around its true value \\(\\beta_j\\), with a standard error of \\(\\mathrm{s.\\!e.}(\\hat{\\beta}_j)= \\hat{\\sigma}\\sqrt{(\\mathbf{X}^T \\mathbf{X})^{-1}_{j,j}}\\)\nA hypothesis test for whether the true parameter \\(\\beta_j\\) might equal some candidate value \\(b_j\\) can be performed as follows:\n\\[t_j = \\frac{\\hat{\\beta}_j-b_j}{\\mathrm{s.\\!e.}(\\hat{\\beta}_j)},\\qquad t_j \\sim T_{n-k-1}\\]\nThe bounds for a symmetric, two-sided \\(1-\\alpha\\) confidence interval for the true location of the parameter \\(\\beta_j\\) can be approximated as follows:\n\\[\\mathrm{CI}_{\\beta_j,1-\\alpha} \\approx \\hat{\\beta}_j \\pm t_{n-k-1,\\alpha⁄2}^*\\cdot \\mathrm{s.\\!e.}(\\hat{\\beta}_j)\\]\n\n\n\nThese last two results allow us to test whether one of the true slopes \\(\\beta_j\\) might be different than the best guess afforded by our data, \\(\\hat{\\beta}_j\\), and indeed allows us to find the whole range of likely values for \\(\\beta_j\\). In practice, these calculations are rarely done by hand, but easily computed from any statistical software output.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiplemle.html#distributions-for-a-predicted-value-and-for-a-predicted-mean-response",
    "href": "multiplemle.html#distributions-for-a-predicted-value-and-for-a-predicted-mean-response",
    "title": "Multiple regression",
    "section": "Distributions for a predicted value and for a predicted mean response",
    "text": "Distributions for a predicted value and for a predicted mean response\nBefore we added distributional assumptions, we could still make a point estimate for the value of \\(Y\\) at a specific combination of predictors, using the estimated relationship \\(\\hat{Y} = \\boldsymbol{X\\hat{\\beta}}\\). Now we can add a distribution around this prediction.\nThe distribution is helpful because new values of \\(Y\\) won’t be exactly where we predict it, but more likely somewhere nearby. The distribution allows us to place reasonable bounds on how far off we might be.\nWhen we make predictions for \\(Y\\) we need to distinguish between two types of prediction, which share the same point estimate yet have different variances:\n\nWe can make a prediction for the true mean response \\(\\mu_0 = \\mathbb{E}[Y|\\boldsymbol{X}=\\boldsymbol{x}_0]\\). In plain language, we want to know, “how far away would many observations at this location cluster?” Our main source of error is from the estimated betas, and when our sample size is high our predictions will have a low standard error.\nAlternatively, we can make a prediction for a new value of \\(Y\\) when \\(\\boldsymbol{X}=\\boldsymbol{x}_0\\). We expect that these values fall far from our prediction, because they are influenced by two different errors. First, as above, our uncertainty about the true betas will mean that the new values cluster somewhere different than where we predict. And second, individual values can be found near or far from the center of the cluster, depending on the variance \\(\\sigma^2\\). New values can easily fall \\(2\\sigma\\) or farther away from the mean \\((\\mu_0)\\). Because of this second error, our predictions for a single observation will generally have a high standard error.\n\n\n\n\n\n\n\nNote\n\n\n\nAssume a relationship between a response variable \\(Y\\) and \\(k\\) predictor variables \\(X_1,\\ldots,X_k\\) which meet the assumptions of ordinary least squares (OLS) regression, and use all the usual notation so far for its parameters, distributional form, and samples\n\nThe point estimate for \\(\\mu_0\\), the mean response of \\(Y\\) at a new predictor value \\(\\boldsymbol{x}_0\\), is:\n\\[\\hat{\\mu}_0 = \\boldsymbol{x}_0\\circ \\boldsymbol{\\hat{\\beta}} = \\sum_{j=0}^k \\beta_jx_{j,0}\\]\nThe standard error for \\(\\hat{\\mu}_0\\) is:\n\\[\\mathrm{s.\\!e.}(\\hat{\\mu}_0) = \\hat{\\sigma}\\sqrt{\\boldsymbol{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\boldsymbol{x}_0}\\]\nA symmetric, two-sided \\(1-\\alpha\\) confidence interval for the true location of the mean response \\(\\mu_0\\) when \\(\\boldsymbol{X} = \\boldsymbol{x}_0\\) is:\n\\[\\mathrm{CI}_{\\mu_0,1-\\alpha} = \\hat{\\mu}_0 \\pm t_{n-k-1,\\alpha⁄2}^* \\cdot \\mathrm{s.\\!e.}(\\hat{\\mu}_0)\\]\nThe point estimate for \\(y_0\\), the value of \\(Y\\) at a new predictor value \\(\\boldsymbol{x}_0\\), is also:\n\\[\\hat{y}_0 = \\boldsymbol{x}_0\\circ \\boldsymbol{\\hat{\\beta}} = \\sum_{j=0}^k \\beta_jx_{j,0}\\]\nThe standard error for \\(\\hat{y}_0\\) is:\n\\[\\mathrm{s.\\!e.}(\\hat{y}_0) = \\hat{\\sigma}\\sqrt{1+\\boldsymbol{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1}\\boldsymbol{x}_0}\\]\nA symmetric, two-sided \\(1-\\alpha\\) confidence interval for the true location of an unobserved value \\(y_0\\) when \\(\\boldsymbol{X} = \\boldsymbol{x}_0\\) is:\n\\[\\mathrm{CI}_{y_0,1-\\alpha} = \\hat{y}_0 \\pm t_{n-k-1,\\alpha⁄2}^* \\cdot \\mathrm{s.\\!e.}(\\hat{y}_0)\\]\n\n\n\nThese new prediction tools allow us to give best- and worst-case scenarios for new predictions. As you can see in the figure below, the two prediction intervals are each centered around the line of best fit, but the estimated individual values \\(\\hat{y}_0\\) are found in a wider interval than their means \\(\\hat{\\mu}_0\\). You’ll also notice that both intervals “flare” a little toward the far left and right sides of the graph. This is because we do not know the true linear relationship, but we do assume that it must pass through the middle of the data, i.e. the line-of-best-fit must pass through the point \\((\\bar{x},\\bar{y})\\). Therefore, near the middle of the data, our error from estimating the parameters \\(\\boldsymbol{\\beta}\\) will be relatively small. But if we have the slopes wrong, then the small degree of “tilt” away from the true values will create quite large errors in our predictions at locations far from the center of the data.\n\n\nCode\nX &lt;- c(1,2,3,4,5)\nY &lt;- c(6,8,6,2,3)\ny0 &lt;- predict(lm(Y~X),newdata=data.frame(X=seq(1,5,0.1)),\n               level=0.9,interval='prediction')\nmu0 &lt;- predict(lm(Y~X),newdata=data.frame(X=seq(1,5,0.1)),\n               level=0.9,interval='confidence')\n\npar(mfrow=c(2,1))\nplot(X,Y,ylim=c(-3,13),main='Predictions for new values of Y')\nabline(reg=lm(Y~X))\nlines(seq(1,5,0.1),y0[,2],lty=3,col='#0000ff')\nlines(seq(1,5,0.1),y0[,3],lty=3,col='#0000ff')\nlegend(x='topright',bty='n',lty=c(1,3),col=c('#000000','#0000ff'),\n       legend=c('Point estimate','90% Confidence Interval'))\n\nplot(X,Y,ylim=c(-3,13),main='Predictions for mean response of Y')\nabline(reg=lm(Y~X))\nlines(seq(1,5,0.1),mu0[,2],lty=2,col='#0000ff')\nlines(seq(1,5,0.1),mu0[,3],lty=2,col='#0000ff')\nlegend(x='topright',bty='n',lty=c(1,2),col=c('#000000','#0000ff'),\n       legend=c('Point estimate','90% Confidence Interval'))\n\n\n\n\n\n\n\n\nFigure 14.1: Prediction intervals for OLS regression",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiplemle.html#footnotes",
    "href": "multiplemle.html#footnotes",
    "title": "Multiple regression",
    "section": "",
    "text": "“MVNormal” here indicates a multivariate Normal distribution defined by a vector of means and a corresponding vector of variances, a straightforward extension of the Normal distribution we already know.↩︎\n“Line-of-best-fit” is now just an analogy, really what we are doing is projecting a high-dimensional vector \\(\\boldsymbol{y}\\) (which exists in an \\(n\\)-dimensional vector space) into a lower-dimensional space spanned by the predictors (at most \\((k+1)\\)-dimensional, where \\(k\\) is the number of predictors other than the vector of ones). It would be more accurate to say we are finding the “hyperplane-of-best-fit”, but this might cause some consternation…↩︎\nIn the simplest case, for a single observation \\(y_1=10\\), we could model the mean as \\(\\hat{\\mu}=\\hat{y}=10\\). But if we then calculate the variance as \\(\\hat{\\sigma}^2 = \\sum_i(y_i-\\hat{\\mu})^2/n = (10-10)^2/1 = 0\\), clearly our estimate would be biased.↩︎",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "variancedecomposition.html",
    "href": "variancedecomposition.html",
    "title": "Variance decomposition",
    "section": "",
    "text": "Motivating the problem\nMost of these notes concern Maximum Likelihood theory, but I’d like to briefly return to the Least Squares interpretation. We saw previously that the Least Squares method identifies the line-of-best-fit by minimizing the residual sum of squares (RSS), which is also sometimes called the sum of squared errors (SSE).1\nJust as the Maximum Likelihood approach allowed us to construct \\(t\\)-tests and confidence intervals, the Least Squares interpretation suggests a powerful tool of its own, which I will call variance decomposition. Before I explain this tool, let me explain the problem that it will solve.\nLikelihood theory allows us to say whether one set of estimates for \\(\\boldsymbol{\\beta}\\) is better or worse than another. It even allows us to find the best estimates for \\(\\boldsymbol{\\beta}\\). But these are all relative measures: likelihood theory cannot tell us if the best fitting line is actually any good, in an absolute sense. What use is the best model if it’s still a bad model? How can we measure model performance on an absolute scale?\nIn the sections above, I’ve already shown metrics of predictive accuracy such as mean absolute error (MAE) and root mean squared error (RMSE):\n\\[\\mathrm{MAE} = \\frac{1}{n} \\sum_i |y_i - \\hat{y}_i|\\]\n\\[\\mathrm{RMSE} = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2\\]\nThese metrics are very dependent on the scale of the data and the natural variance of the response variable \\(Y\\). If I compute a model RMSE of 5, or a model MAE of 6000, I still don’t know whether these numbers are impressive or useful until I better understand the data and the modeling context. It would be helpful to construct a regression model metric with the following properties:",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variance decomposition</span>"
    ]
  },
  {
    "objectID": "variancedecomposition.html#the-hook-for-our-solution-the-variance-of-y",
    "href": "variancedecomposition.html#the-hook-for-our-solution-the-variance-of-y",
    "title": "Variance decomposition and R2",
    "section": "The hook for our solution: the variance of Y",
    "text": "The hook for our solution: the variance of Y\nWe will bring the math in soon, but the idea we will follow is to use the variance of our response vector \\(\\boldsymbol{y}\\) as a benchmark. If our model can “explain” 100% of the variance of \\(\\boldsymbol{y}\\), that is the best any model can do.2 If the model can “explain” 0% of the variance of \\(\\boldsymbol{y}\\), then it is the worst (or at least simplest) model we can build, and we should continue to look for better models.\nWhat does it mean for a model to “explain” the variance of \\(\\boldsymbol{y}\\)? The math below shows that we can always take the variance of \\(\\boldsymbol{y}\\) and split it into two parts: one part of the variance which is matched by the model predictions \\(\\hat{\\boldsymbol{y}}\\), and another part which is matched by the residuals \\(\\boldsymbol{e}\\).\nThis method is sometimes referred to as a decomposition of variance, or simply an analysis of variance, which in turn is shortened to ANOVA.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Variance decomposition and R^2^</span>"
    ]
  },
  {
    "objectID": "variancedecomposition.html#mathematical-foundation",
    "href": "variancedecomposition.html#mathematical-foundation",
    "title": "Variance decomposition and R2",
    "section": "Mathematical foundation",
    "text": "Mathematical foundation\nRecall the basic framework of the linear model. We believe that our response vector is the sum of (i) an unknown linear combination of the predictors, and (ii) an unknown set of errors:\n\\[\\boldsymbol{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\]\nWe observe \\(\\boldsymbol{y}\\) and \\(\\mathbf{X}\\), but there are an infinite number of interdependent solutions for \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\varepsilon}\\). Using some method (least squares, maximum likelihood, blind guesses, etc.) we estimate \\(\\boldsymbol{\\beta}\\) which also creates an estimate for the errors \\(\\boldsymbol{\\varepsilon}\\). We call the estimated errors the “residuals”, and sometimes write them as \\(\\boldsymbol{e}\\):\n\\[\\begin{aligned} \\hat{\\boldsymbol{y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\ \\boldsymbol{y} - \\hat{\\boldsymbol{y}} &= \\boldsymbol{e} \\end{aligned}\\]\nLet me start by asserting two preliminary results. (The proofs for these statements can be found in the appendices.)\n\nThe average of the residuals of OLS regression is always zero:\n\\[\\bar{\\boldsymbol{e}} = \\frac{1}{n}\\sum_i e_i = 0\\]\nThe average of the fitted values \\(\\hat{\\boldsymbol{y}}\\) is the same as the average of the sample \\(\\boldsymbol{y}\\):\n\\[\\bar{\\hat{\\boldsymbol{y}}} = \\frac{1}{n}\\sum_i \\hat{y}_i = \\frac{1}{n}\\sum_i y_i = \\bar{\\boldsymbol{y}}\\]\n\nNext, consider the following arithmetic equality:3\n\\[(y_i - \\bar{y}) = (\\hat{y}_i - \\bar{y}) + (y_i - \\hat{y}_i)\\]\nIf we prefer, we can represent this equality geometrically, using vectors. I will use the notation \\(\\tilde{\\boldsymbol{y}} = 1\\!\\!1 \\cdot \\bar{y}\\) to stay as precise as possible as we move from scalars to vectors, so you can think of \\(\\tilde{\\boldsymbol{y}}\\) as “the vectorized mean of y”:\n\\[(\\boldsymbol{y} - \\tilde{\\boldsymbol{y}}) = (\\hat{\\boldsymbol{y}} - \\tilde{\\boldsymbol{y}}) + (\\boldsymbol{y} - \\hat{\\boldsymbol{y}})\\]\nIf we draw these vectors on a coördinate plane, we should recall the assumption that the residuals are independent of the predictors \\(\\mathbf{X}\\), which means they are also independent of any linear combination of those predictors, such as \\(\\hat{\\boldsymbol{y}}\\). This orthogonality creates a right triangle out of the arithmetic equality above:\n\n\nCode\nX &lt;- c(1,2,3,4,5)\nY &lt;- c(6,8,6,2,3)\nYhat &lt;- lm(Y~X)$fitted.values\ne &lt;- lm(Y~X)$residuals\nanova(lm(Y~X))\n\n\nAnalysis of Variance Table\n\nResponse: Y\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nX          1   14.4    14.4     4.5  0.124\nResiduals  3    9.6     3.2               \n\n\nCode\nvec.length &lt;- function(z) sqrt(c(z%*%z))\nplot(x=NA,y=NA,xlim=c(0,5),ylim=c(-0.5,4),xlab='Model space',ylab='Error space',\n     axes=FALSE,main='Arithmetic of ANOVA Triangle')\nlines(x=c(0,vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y)),0),\n      y=c(0,0,vec.length(e),0))\ntext(x=1.75,y=2,labels=expression(Y-tilde(Y)))\ntext(x=2.5,y=-0.25,labels=expression(hat(Y)-tilde(Y)))\ntext(x=4.25,y=1.5,labels=expression(Y-hat(Y)))\n\nplot(x=NA,y=NA,xlim=c(-3,8),ylim=c(-4,7),xlab='Model space',ylab='Error space',\n     axes=FALSE,main='Geometry of ANOVA Sum of Squares',asp=1)\npolygon(x=c(0,vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y))-vec.length(e),-vec.length(e)),\n        y=c(0,vec.length(e),vec.length(e)+vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y))))\npolygon(x=c(0,vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y)),0),\n        y=c(0,0,-vec.length(Yhat-mean(Y)),-vec.length(Yhat-mean(Y))),\n        col='#0000003f')\npolygon(x=c(vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y))+vec.length(e),vec.length(Yhat-mean(Y))+vec.length(e)),\n        y=c(0,vec.length(e),vec.length(e),0),\n        col='#0000ff3f')\ntext(x=1.5,y=2,labels=expression(Y-tilde(Y)),cex=0.7)\ntext(x=2.5,y=-0.5,labels=expression(hat(Y)-tilde(Y)),cex=0.7)\ntext(x=4.5,y=1.5,labels=expression(Y-hat(Y)),cex=0.7)\ntext(x=0.2,y=3.5,labels='SST=SSM+SSE=24')\ntext(x=2,y=-2.5,labels='SSM=14.4',col='#3f3f3f')\ntext(x=5.2,y=2.5,labels='SSE=9.6',col='#0000ff')\n\n\n\n\n\n\n\n\nFigure 16.1: Introduction to the ANOVA ‘Triangle’\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.2: Introduction to the ANOVA ‘Triangle’\n\n\n\n\n\nFrom here we can use the Pythagorean Theorem to create a new equality, linking the squares of each of the sides. Define the areas as follows: The total sum of squares: SST=∑_i▒(y_i-y ̅ )^2 The model sum of squares: SSM=∑_i▒(y ̂_i-y ̅ )^2 The residual sum of squares (or sum of squared errors) : RSS=SSE=∑_i▒(y_i-y ̂_i )^2 Recall the small regression example in previous sections, with X={1,2,3,4,5} and Y={6,8,6,2,3}, and β ̂_0=8.6 and β ̂_1=-1.2. In that case, we could draw the sums of squares as follows:\nFigure 21: Completed ANOVA triangle Ultimately, these geometric representations are most helpful in describing a Least Squares interpretation of the model which we may return to later. And by now, the reader may be puzzled as to where this digression will lead. Consider, then, the equivalence of the following three statements: SST=SSM+SSE ∑_i▒(y_i-y ̅ )^2 =∑_i▒(y ̂_i-y ̅ )^2 +∑_i▒(y_i-y ̂_i )^2 V[y]=V[y ̂ ]+V[e] We are showing that the total variance of Y can be split into two parts: an amount of variance explained by the model (i.e. the variance of the fitted values), and an amount of variance that is unexplained by the model. Left over. Residual variance. We can now construct a metric which measures what proportion of the variance of Y is captured by our model: R^2=SSM/SST=1-SSE/SST This measure R2 is also known as the coefficient of determination for a regression model, though in practice we almost always refer to it simply as “R-squared”. R2 will always be between 0 and 1, and in the case of simple regression, R2 really is the square of r, the correlation between X and Y. While it is true that we usually prefer larger R2 to smaller R2, we must consider a few nuances: An R2 of exactly 1 is not the sign of a good model, it is the sign of an overdetermined model, where there is no uncertainty or randomness in the outcome. Typically this happens when the complete information of Y is found within some of the predictors, such as if I attempted to predict students’ height in inches from their height in centimeters. It can also happen when there are at least as many predictors as observations (that is, when k≥n). As we add more and more predictors to our model, R2 will generally increase (mathematically, it cannot decrease, though if we add a perfectly uncorrelated predictor, it can stay the same). These additional predictors may not capture real connections between the predictors and the response variable, but instead might simply highlight a chance correlation in the random noise of our data. In some domains or fields, higher R2 values are much easier to obtain. In the physical sciences, researchers often suppose hard physical laws connect the predictors to the response variable, and R2 values of 0.99 are very common. In the social sciences, researchers studying human behavior might be proud to publish even with an R2 of only 0.10. Humans are so complex and idiosyncratic that explaining even 10% of their variation along many metrics represents an important understanding.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Variance decomposition and R^2^</span>"
    ]
  },
  {
    "objectID": "variancedecomposition.html#footnotes",
    "href": "variancedecomposition.html#footnotes",
    "title": "Variance decomposition",
    "section": "",
    "text": "Of course, we can find the same line by maximizing the likelihood function.↩︎\nThough it also means that something has probably gone wrong…↩︎\nIf it’s not already obvious, notice that we can cancel out the two \\(\\hat{y}_i\\) terms on the right hand side.↩︎\nAlso called the residual sum of squares, RSS.↩︎",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variance decomposition</span>"
    ]
  },
  {
    "objectID": "variancedecomposition.html#the-anova-triangle",
    "href": "variancedecomposition.html#the-anova-triangle",
    "title": "Variance decomposition",
    "section": "The ANOVA ‘Triangle’",
    "text": "The ANOVA ‘Triangle’\nThe idea we will follow is to use the variance of our response vector \\(\\boldsymbol{y}\\) as a predictive benchmark. If our model can “explain” 100% of the variance of \\(\\boldsymbol{y}\\), that is the best any model can do.2 If the model can “explain” 0% of the variance of \\(\\boldsymbol{y}\\), then it is the worst (or at least simplest) model we can build, and we should continue to look for better models.\nWhat does it mean for a model to “explain” the variance of \\(\\boldsymbol{y}\\)? The math below shows that we can always take the variance of \\(\\boldsymbol{y}\\) and split it into two parts: one part of the variance which is matched by the model predictions \\(\\hat{\\boldsymbol{y}}\\), and another part which is matched by the residuals \\(\\boldsymbol{e}\\).\nThis method is sometimes referred to as a decomposition of variance, or simply an analysis of variance, which in turn is shortened to ANOVA.\nRecall the basic framework of the linear model. We believe that our response vector is the sum of (i) an unknown linear combination of the predictors, and (ii) an unknown set of errors:\n\\[\\boldsymbol{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\]\nWe observe \\(\\boldsymbol{y}\\) and \\(\\mathbf{X}\\), but there are an infinite number of interdependent solutions for \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\varepsilon}\\). Using some method (least squares, maximum likelihood, blind guesses, etc.) we estimate \\(\\boldsymbol{\\beta}\\) which also creates an estimate for the errors \\(\\boldsymbol{\\varepsilon}\\). We call the estimated errors the “residuals”, and sometimes write them as \\(\\boldsymbol{e}\\):\n\\[\\begin{aligned} \\hat{\\boldsymbol{y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\ \\boldsymbol{y} - \\hat{\\boldsymbol{y}} &= \\boldsymbol{e} \\end{aligned}\\]\nLet me start by asserting two preliminary results. (The proofs for these statements can be found in the appendices.)\n\nThe average of the residuals of OLS regression is always zero:\n\\[\\bar{\\boldsymbol{e}} = \\frac{1}{n}\\sum_i e_i = 0\\]\nThe average of the fitted values \\(\\hat{\\boldsymbol{y}}\\) is the same as the average of the sample \\(\\boldsymbol{y}\\):\n\\[\\bar{\\hat{\\boldsymbol{y}}} = \\frac{1}{n}\\sum_i \\hat{y}_i = \\frac{1}{n}\\sum_i y_i = \\bar{\\boldsymbol{y}}\\]\n\nNext, consider the following arithmetic equality:3\n\\[(y_i - \\bar{y}) = (\\hat{y}_i - \\bar{y}) + (y_i - \\hat{y}_i)\\]\nIf we prefer, we can represent this equality geometrically, using vectors. I will use the notation \\(\\tilde{\\boldsymbol{y}} = 1\\!\\!1 \\cdot \\bar{y}\\) to stay as precise as possible as we move from scalars to vectors, so you can think of \\(\\tilde{\\boldsymbol{y}}\\) as “the vectorized mean of y”:\n\\[(\\boldsymbol{y} - \\tilde{\\boldsymbol{y}}) = (\\hat{\\boldsymbol{y}} - \\tilde{\\boldsymbol{y}}) + (\\boldsymbol{y} - \\hat{\\boldsymbol{y}})\\]\nIf we draw these vectors on a coördinate plane, we should recall the assumption that the residuals are independent of the predictors \\(\\mathbf{X}\\), which means they are also independent of any linear combination of those predictors, such as \\(\\hat{\\boldsymbol{y}}\\). This orthogonality creates a right triangle out of the arithmetic equality above:\n\n\nCode\nX &lt;- c(1,2,3,4,5)\nY &lt;- c(6,8,6,2,3)\nYhat &lt;- lm(Y~X)$fitted.values\ne &lt;- lm(Y~X)$residuals\nvec.length &lt;- function(z) sqrt(c(z%*%z))\nplot(x=NA,y=NA,xlim=c(0,5),ylim=c(-0.5,4),xlab='Model space',ylab='Error space',\n     axes=FALSE,main='Arithmetic of ANOVA Triangle')\nlines(x=c(0,vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y)),0),\n      y=c(0,0,vec.length(e),0))\ntext(x=1.75,y=2,labels=expression(Y-tilde(Y)))\ntext(x=2.5,y=-0.25,labels=expression(hat(Y)-tilde(Y)))\ntext(x=4.25,y=1.5,labels=expression(Y-hat(Y)))\n\n\n\n\n\n\n\n\nFigure 15.1: Introduction to the ANOVA ‘Triangle’\n\n\n\n\n\nFrom here we can use the Pythagorean Theorem to create a new equality, linking the squares of each of the sides. Define the areas as follows:\n\nThe total sum of squares: \\(\\mathrm{SST} = \\sum_i (y_i - \\bar{y})^2\\)\nThe model sum of squares: \\(\\mathrm{SSM} = \\sum_i (\\hat{y}_i - \\bar{y})^2\\)\nThe sum of squared errors:4 \\(\\mathrm{SSE} = \\sum_i (y_i - \\hat{y})^2\\)\n\nRecall the small regression example in previous sections, with \\(\\boldsymbol{x}=\\{1,2,3,4,5\\}\\) and \\(\\boldsymbol{y}=\\{6,8,6,2,3\\}\\), and \\(\\hat{\\beta}_0=8.6\\) and \\(\\hat{\\beta}_1=-1.2\\). In that case, we could draw the sums of squares as follows:\n\n\nCode\nplot(x=NA,y=NA,xlim=c(-3,8),ylim=c(-4,7),xlab='Model space',ylab='Error space',\n     axes=FALSE,main='Geometry of ANOVA Sum of Squares',asp=1)\npolygon(x=c(0,vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y))-vec.length(e),-vec.length(e)),\n        y=c(0,vec.length(e),vec.length(e)+vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y))))\npolygon(x=c(0,vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y)),0),\n        y=c(0,0,-vec.length(Yhat-mean(Y)),-vec.length(Yhat-mean(Y))),\n        col='#0000003f')\npolygon(x=c(vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y)),vec.length(Yhat-mean(Y))+vec.length(e),vec.length(Yhat-mean(Y))+vec.length(e)),\n        y=c(0,vec.length(e),vec.length(e),0),\n        col='#0000ff3f')\ntext(x=1.5,y=2,labels=expression(Y-tilde(Y)),cex=0.7)\ntext(x=2.5,y=-0.5,labels=expression(hat(Y)-tilde(Y)),cex=0.7)\ntext(x=4.5,y=1.5,labels=expression(Y-hat(Y)),cex=0.7)\ntext(x=0.2,y=3.5,labels='SST=SSM+SSE=24')\ntext(x=2,y=-2.5,labels='SSM=14.4',col='#3f3f3f')\ntext(x=5.2,y=2.5,labels='SSE=9.6',col='#0000ff')\n\n\n\n\n\n\n\n\nFigure 15.2: Completed ANOVA ‘Triangle’\n\n\n\n\n\nI want you to notice two different equivalencies in the diagram. Consider the sides of the right triangle to be ‘A’ and ‘B’ and the hypotenuse ‘C’.\n\nFirst, notice that the vectors A and B sum to C, using vector addition.\n\\[ \\begin{array}{ccccc} (\\boldsymbol{y} - \\tilde{\\boldsymbol{y}}) & = & (\\hat{\\boldsymbol{y}} - \\tilde{\\boldsymbol{y}}) & + & (\\boldsymbol{y} - \\hat{\\boldsymbol{y}}) \\\\ \\mathcal{C} & = & \\mathcal{A} & + & \\mathcal{B} \\end{array}\\]\nNext, notice that by the Pythagorean Theorem, the areas A2 and B2 sum to C2.\n\\[\\begin{array}{ccccc} \\mathcal{C}^2 & = & \\mathcal{A}^2 & + & \\mathcal{B}^2 \\\\ \\mathrm{SST} & = & \\mathrm{SSM} & + & \\mathrm{SSE} \\\\ \\sum_i(y_i - \\bar{y})^2 & = & \\sum_i(\\hat{y}_i - \\bar{y})^2 & + & \\sum_i(y_i - \\hat{y}_i)^2 \\\\ \\mathbb{V}[\\boldsymbol{y}] & = & \\mathbb{V}[\\hat{\\boldsymbol{y}}] & + &\\mathbb{V}[\\boldsymbol{e}]\\end{array}\\]\n\nWe are showing that the total variance of \\(\\boldsymbol{y}\\) can be split into two parts: an amount of variance explained by the model (i.e. the variance of the fitted values), and an amount of variance that is unexplained by the model. Left over variance. Residual variance — hence the name ‘residual’.\nFrom this result we can now construct a metric which measures what proportion of the variance of \\(\\boldsymbol{y}\\) is captured by our model:\n\\[R^2=\\frac{\\mathrm{SSM}}{\\mathrm{SST}}=1-\\frac{\\mathrm{SSE}}{\\mathrm{SST}}\\]\nThis measure R2 is also known as the coefficient of determination for a regression model, though in practice we almost always refer to it simply as “R-squared”. R2 will always be between 0 and 1, and in the case of simple regression, R2 really is the square of \\(r\\), the correlation between \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\).\nWhile it is true that we usually prefer larger R2 to smaller R2, we must consider a few nuances:\n\nAn R2 of exactly 1 is not the sign of a good model, it is the sign of an overdetermined model, where there is no uncertainty or randomness in the outcome. Typically this happens when the complete information of \\(Y\\) is found within some of the predictors, such as if I attempted to predict students’ height in inches from their height in centimeters. It can also happen when there are at least as many predictors as observations (that is, when \\(k \\ge n\\)).\nAs we add more and more predictors to our model, R2 will generally increase (mathematically, it cannot decrease, though if we add completely uninformative predictors, it can stay the same). These additional predictors may not capture real connections between the predictors and the response variable, but instead might simply highlight a chance correlation in the random noise of our data.\nIn some domains or fields, higher R2 values are much easier to obtain. In the physical sciences, researchers often hypothesize that hard physical laws connect the predictors to the response variable, and R2 values of 0.99 are very common. By contrast, in the social sciences, researchers studying human behavior might be proud to publish even with an R2 of only 0.10. Humans are so complex and idiosyncratic that explaining even 10% of their variation along many metrics represents an important understanding.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variance decomposition</span>"
    ]
  },
  {
    "objectID": "variancedecomposition.html#visualizer",
    "href": "variancedecomposition.html#visualizer",
    "title": "Variance decomposition",
    "section": "Visualizer",
    "text": "Visualizer",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variance decomposition</span>"
    ]
  },
  {
    "objectID": "howtounivariate.html",
    "href": "howtounivariate.html",
    "title": "Coding it up in R",
    "section": "",
    "text": "How to compute and optimize a simple likelihood function",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtounivariate.html#how-to-compute-and-optimize-a-simple-likelihood-function",
    "href": "howtounivariate.html#how-to-compute-and-optimize-a-simple-likelihood-function",
    "title": "Coding it up in R",
    "section": "",
    "text": "Using a completely automated routine\nIn simple cases, where you know the distribution name and only need to estimate the parameters, existing R functions will perform all the math for you. The two most common functions are:\n\nMASS::fitdistr — comes pre-installed (but not loaded) in all base distributions of R, handles many basic cases\nfitdistrplus::fitdist — requires installation of the ‘fitdistr’ package, handles more edge cases and estimation methods\n\nFor simplicity we will use MASS::fitdistr; however, the same outputs can be reached with either function.1\nThe code below generates some exponential data with true parameter \\(\\lambda=5\\), gives you the documentation for the MASS::fitdistr function, and then uses that function to estimate the parameter:\n\nlibrary(MASS)                                           \nset.seed(1138)                                          \nexpsample &lt;- rexp(n=5000,rate=5)                        \nhelp(fitdistr) #for your reference\nauto.sol &lt;- fitdistr(x=expsample,densfun='exponential')           \nman.sol &lt;- 1/mean(expsample)                                     \nc(auto.solution=auto.sol$estimate,\n  manual.solution=man.sol)\n\nauto.solution.rate    manual.solution \n          4.893315           4.893315 \n\n\nThings to notice about this output:\n\nThe MLE guess of \\(\\hat{\\lambda} \\approx 4.893\\) is close to the true value of 5, but not exactly right. In general, we will always be wrong with our estimates, and that’s fine!\nOf course, in the real world, we would not know the true value of 5, and we would only see the guess of 4.893, and have no idea how close or far we are from the truth.\nThe estimate returned by the fitdistr() function is exactly equal to \\(1/\\bar{x}\\).2 You could solve this by hand and arrive at the same conclusion! For more complicated distributions, the function will instead approximate the MLE with iterative algorithms.\n\n\nxticks &lt;- seq(1,9,0.01)\nloglik &lt;- sapply(xticks,function(z) sum(dexp(expsample,z,log=TRUE)))\nplot(xticks,loglik,type='l',xlab='lambda-hat',ylab='log-lik',main=NA)\nabline(v=c(man.sol,5),col=c('#000000','#0000ff'),lty=2,lwd=2)\ntext(x=c(man.sol,5),y=1000,pos=c(2,4),labels=c('MLE','Truth'),\n     col=c('#000000','#0000ff'))\n\n\n\n\n\n\n\n\n\n\nOptimizing custom log-likelihood functions\nSometimes your work will suggest a distribution not handled by basic R functions. You can still use R to help you maximize a custom likelihood function using stats::optim, which comes pre-loaded in all R versions.\nLet’s consider a truncated Poisson distribution of speeding tickets. Perhaps you have records of all the speeding tickets issued to drivers in one locality. Some drivers have multiple speeding tickets. Presumably, some drivers have no speeding tickets, but these drivers don’t leave a record in the ticket database. If all drivers recieve tickets through a Poisson process, and we only observe the positive counts (not the zeroes), can we reconstruct the original rate?\nFirst we will make a set of data for 50,000 drivers using a rate of \\(\\lambda = 0.07\\) tickets per driver, then we’ll censor the zeroes to mimic a real dataset. Next we’ll create a custom likelihood function to represent such data (essentially just re-weighting the original Poisson PMF), and finally we’ll optimize this function using optim():\n\nset.seed(1139)                                          \nhidden.data &lt;- rpois(n=50000,lambda=.07)\ntable(hidden.data)                                     \n\nhidden.data\n    0     1     2     3 \n46664  3203   129     4 \n\nobserved.data &lt;- hidden.data[hidden.data!=0]\ntable(observed.data)                                   \n\nobserved.data\n   1    2    3 \n3203  129    4 \n\nll.trunc.pois &lt;- function(lambda,x){                   \n  sum(x)*log(lambda) - lambda*sum(1/factorial(x)) -    \n    length(x)*log(1-exp(-1*lambda))}                   \n\nhelp(optim) # for your reference\nopt.sol &lt;- optim(par=0.1,fn=ll.trunc.pois,             \n  x=observed.data, control=list(fnscale=-1),           \n  method='L-BFGS-B', lower=0.01,upper=0.5)             \nopt.sol$par\n\n[1] 0.08437934\n\n\nThis call to optim() gives a starting value for lambda, specifies the function being maximized, passes data to the likelihood function, re-scales the algorithm to maximize (rather than minimize), chooses an optimization method, and puts bounds on the values of lambda. The optim() algorithm converges on the MLE of 0.084, which is near the true value of 0.07:\n\nxticks &lt;- seq(0.01,0.15,0.001)\nloglik &lt;- sapply(xticks,ll.trunc.pois,x=observed.data)\nplot(xticks,loglik,type='l',xlab='lambda-hat',ylab='log-lik',main=NA)\nabline(v=c(opt.sol$par,0.07),col=c('#000000','#0000ff'),lty=2,lwd=2)\ntext(x=c(opt.sol$par,0.07),y=-575,pos=c(4,2),labels=c('MLE','Truth'),\n     col=c('#000000','#0000ff'))",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtounivariate.html#footnotes",
    "href": "howtounivariate.html#footnotes",
    "title": "Coding it up in R",
    "section": "",
    "text": "If using fitdistrplus::fitdist you should be sure to specify method='mle' or else you may not match the results of MASS::fitdistr.↩︎\nAny printed difference in the code output is simply a matter of significant digits.↩︎\nSpoiler: none of them offer exact \\(1 - \\alpha\\) coverage of the true parameter in all situations.↩︎\nThis scenario assumes that the sample is representative of the electorate and that the respondents are honest about their preferences.↩︎\nI generated the first sample from a Normal(25.2,0.32) distribution and the second sample from a Normal(25.7,0.52) distribution… you may draw your own conclusions.↩︎",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtounivariate.html#how-to-perform-a-hypothesis-test",
    "href": "howtounivariate.html#how-to-perform-a-hypothesis-test",
    "title": "Coding it up in R",
    "section": "How to perform a hypothesis test",
    "text": "How to perform a hypothesis test\nAlmost all parametric inference includes hypothesis tests of some kind, so this is a broad category. We’ll start from basics and in future chapters we will add on more bells and whistles. I present several scenarios below and you should be able to work your way through many more once you master these building blocks.\n\nScenario 1: Two-sided binomial distribution test\nJohn Edmund Kerrich (1903–1985) was a British statistician who became a prisoner of war during WWII. During his internment he flipped a coin 10,000 times and recorded 5067 heads, 4933 tails. Using a two-sided test at a 10% significance threshold, was his coin fair?\n\nhelp(pbinom) # for your reference\n(p.value &lt;- 2*pbinom(q=4933,size=10000,prob=0.5))\n\n[1] 0.1835155\n\np.value &lt;= 0.1\n\n[1] FALSE\n\n\nWith a p-value of 18% we cannot reject the null hypothesis, and continue to assume that the coin is fair.\nThis simple calculation took advantage of the fact that the binomial distribution is symmetric, so that the left tail region (0–4933 heads) and the right tail region (5067–10000 heads) would have the same probability. We could have done this many ways:\n\n2*pbinom(4933,10000,0.5)\n\n[1] 0.1835155\n\npbinom(4933,10000,0.5) + pbinom(5066,10000,0.5,lower.tail=FALSE)\n\n[1] 0.1835155\n\npbinom(4933,10000,0.5) + (1 - pbinom(5066,10000,0.5))\n\n[1] 0.1835155\n\n2*sum(dbinom(0:4933,10000,0.5))\n\n[1] 0.1835155\n\n1 - sum(dbinom(4934:5066,10000,0.5))\n\n[1] 0.1835155\n\n\n\n\nScenario 2: One-sided exponential distribution test\nThe bus stop sign outside your house claims that buses arrive, on average, every 12 minutes, but the first time you waited, it took 18 minutes, and the next time, 30 minutes. Assume bus arrival times are exponentially distributed — using a one-sided test at a 5% significance threshold, do you still believe the sign?\n\nhelp(pexp) # for your reference\n(p.value &lt;- (1-pexp(18/12))*(1-pexp(30/12)))\n\n[1] 0.01831564\n\np.value &lt;= 0.05\n\n[1] TRUE\n\n\nWith a p-value of 1.8% we reject the null hypothesis, and believe that buses arrive less often than the sign says.\nThis calculation took advantage of the fact that the exponential distribution is memoryless and self-similar (displays fractal properties). We could have also used the Poisson distribution to calculate the probability that no buses would arrive during our waiting periods. The idea of “equally extreme or more extreme” outcomes from our two bus waiting periods can be calculated in a number of ways, all of which are numerically equal:\n\n(1-pexp(q=18/12,rate=1))*(1-pexp(q=30/12,rate=1))\n\n[1] 0.01831564\n\n1-pexp(q=48/12)\n\n[1] 0.01831564\n\n1-pexp(q=48,rate=1/12)\n\n[1] 0.01831564\n\ndpois(0,18/12)*dpois(0,30/12)\n\n[1] 0.01831564\n\ndpois(0,48/12)\n\n[1] 0.01831564\n\n\n\n\nScenario 3: One-sided permutation test\nIn 1919 the phycologist Muriel Bristol worked alongside the statistician Ronald Fisher and one day told him that she preferred her tea poured first and then milk added, rather than the other way around. Fisher, skeptical, resolved to test her. He prepared eight cups of tea: four with the milk added first, four with the milk added second. He told Bristol to guess the four cups with milk added first, and she did, without any errors. At a 5% significance level, can we conclude that Bristol indeed could tell the cups apart?\n\nhelp(choose) # for your reference\n(p.value &lt;- 1/choose(8,4))\n\n[1] 0.01428571\n\np.value &lt;= 0.05\n\n[1] TRUE\n\n\nWith a p-value of 1.4% we reject the null hypothesis, and believe that Muriel Bristol could truly taste a difference between the two methods.\nThis calculation was a simple combinatoric — under the null hypothesis that Bristol was guessing by blind luck, she would have chosen any of the \\(8_C4=70\\) distinct sets of four teacups at random. So under the null, there was only a \\(1/70 \\approx 1.4\\%\\) chance that she would pick the right four cups, which she did.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtounivariate.html#how-to-compute-exact-confidence-intervals-for-distributional-parameters",
    "href": "howtounivariate.html#how-to-compute-exact-confidence-intervals-for-distributional-parameters",
    "title": "Coding it up in R",
    "section": "How to compute ‘exact’ confidence intervals for distributional parameters",
    "text": "How to compute ‘exact’ confidence intervals for distributional parameters\n\nFor binomial probabilities p\nStatisticians debate the merits of several different “exact” solutions for this problem.3 In this document we’ll outline the Clopper-Pearson interval, which is very conservative (meaning that it is more likely to contain the true parameter than its nominal confidence level.)\nLet’s say that 1000 likely United States voters are polled and 65 of them claim they will vote for a third-party candidate in the upcoming presidential election. What would be a 90% “exact” two-sided confidence interval for the true proportion of voters supporting the third-party candidate?4\nThe good news: this is a simple one-line calculation using stats::binom.test!\nThe better news: you could replicate this using the theory you were taught, and the root-finding helper function stats::uniroot!\n\nhelp(binom.test)\nbinom.test(x=65,n=1000,alternative='two.sided',conf.level=0.90)\n\n\n    Exact binomial test\n\ndata:  65 and 1000\nnumber of successes = 65, number of trials = 1000, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n90 percent confidence interval:\n 0.05264301 0.07930468\nsample estimates:\nprobability of success \n                 0.065 \n\nhelp(uniroot)\nuniroot(function(z){pbinom(64,1000,z)-0.95},c(0.0,0.065),tol=1e-8)$root\n\n[1] 0.05264301\n\nuniroot(function(z){pbinom(65,1000,z)-0.05},c(0.065,0.1),tol=1e-8)$root\n\n[1] 0.07930468\n\n\nBoth methods would also work for one-sided intervals. For example, say that you sample 500 “penny stocks” (companies with very low share prices; typically very speculative or risky investments) and find that 89 of those companies file for bankruptcy within six months. What’s a one-sided 80% confidence upper bound on the proportion of penny stocks which quickly go bankrupt?\n\nbinom.test(x=89,n=500,alternative='less',conf.level=0.80)\n\n\n    Exact binomial test\n\ndata:  89 and 500\nnumber of successes = 89, number of trials = 500, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is less than 0.5\n80 percent confidence interval:\n 0.0000000 0.1939393\nsample estimates:\nprobability of success \n                 0.178 \n\nuniroot(function(z){pbinom(89,500,z)-0.20},c(0.178,0.3),tol=1e-8)$root\n\n[1] 0.1939393\n\n\n\n\nFor other parameters\nThis scenario does not often happen. Exact confidence intervals are rarely seen by working data scientists outside of binomial proportions. With modern sample sizes, the approximate methods discussed in Handy rules of thumb, The Central Limit Theorem, and especially The Student’s t distribution are much more common.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtounivariate.html#how-to-identify-a-distribution-and-its-likely-parameter-values",
    "href": "howtounivariate.html#how-to-identify-a-distribution-and-its-likely-parameter-values",
    "title": "Coding it up in R",
    "section": "How to identify a distribution and its likely parameter values",
    "text": "How to identify a distribution and its likely parameter values\n\nTesting whether a sample could come from a specific distribution with known parameters\nThis is a canonical use of the Kolmogorov-Smirnov test. The function stats::ks.test comes pre-loaded with every distribution of R. Although a version of the test exists for discrete-valued samples (where “ties” may occur within the ranks of \\(\\boldsymbol{x}\\)), the base R implementation does not perform this correction. Still, for large sample sizes, you may ignore the warnings in peace.\nSuppose we sell “25 pound” bags of dog food which historically have had a mean weight of 25.6 pounds and a standard deviation of 0.3 pounds (we don’t want to shortchange the customer.) After a change to the assembly line, the most recent ten bag weights were:\n\\[\\boldsymbol{x}=\\{25.3, 25.1, 25.1, 25.5, 25.2, 24.7, 24.8, 24.9, 24.8, 25.5\\}\\] Does the new assembly line preserve our old bag weights?\n\nhelp(ks.test)\nx &lt;- c(25.3, 25.1, 25.1, 25.5, 25.2,\n       24.7, 24.8, 24.9, 24.8, 25.5)\nks.test(x,pnorm,mean=25.8,sd=0.4)\n\nWarning in ks.test.default(x, pnorm, mean = 25.8, sd = 0.4): ties should not be\npresent for the one-sample Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  x\nD = 0.77337, p-value = 1.276e-05\nalternative hypothesis: two-sided\n\n\nNo, with \\(p \\approx 0.05\\%\\), we reject the null at any conventional significance level and conclude that our bags no longer fit the historical distribution. Notice that we have not made a specific claim that the bags weight more or less, or that their spread is wider or narrower, simply that they do not belong to the Normal(25.6,0.32) distribution.\n\n\nTesting whether two samples could have come from the same distribution\nThis is the second canonical use of the Kolmogorov-Smirnov test. We don’t even need to know the distribution of either sample. We can still very easily run a test to see if it’s possible they share the same CDF.\nLet’s say we tweak the assembly line again and get a new sample of ten bags.\n\\[\\boldsymbol{y}=\\{25.4, 25.7, 26.2, 25.8, 25.6, 26.4, 25.4, 24.9, 26.0, 26.0\\}\\]\nWe can ask (i) if the new sample is different from the old sample, and (ii) whether the new sample conforms to the historical standards:\n\ny &lt;- c(25.4, 25.7, 26.2, 25.8, 25.6, \n       26.4, 25.4, 24.9, 26.0, 26.0)\nks.test(x,y)\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  x and y\nD = 0.7, p-value = 0.01234\nalternative hypothesis: two-sided\n\nks.test(y,pnorm,mean=25.6,sd=0.3)\n\nWarning in ks.test.default(y, pnorm, mean = 25.6, sd = 0.3): ties should not be\npresent for the one-sample Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  y\nD = 0.30879, p-value = 0.2961\nalternative hypothesis: two-sided\n\n\nUsing a 5% significance threshold, we may say that the latest tweak has changed our assembly line from the first sample, and that we have no evidence against the idea that we have returned to the old standards of Normal(25.6,0.32).5\n\n\nIdentifying and estimating an unknown sample\nThis is not a canonical use of the Kolmogorov-Smirnov test. Above, we tested whether a sample of data belonged to a known, fully-specified distribution. If we instead are estimating the distribution parameters from the data itself, the test is no longer fully accurate.\nHowever… this remains common in exploratory data analysis (EDA). WE don’t need to convince an outside audience, we just need to convince ourselves. For example, let’s take the second sample of dog food bag weights:\n\\[\\boldsymbol{y}=\\{25.4, 25.7, 26.2, 25.8, 25.6, 26.4, 25.4, 24.9, 26.0, 26.0\\}\\]\nSay we had no prior idea which distribution these came from, and we wanted a guess. The data seem continuous and don’t seem to have natural bounds. Two candidate distributions might be the Cauchy and the Normal. We’ll find the best parameters for both (using MASS::fitdistr) and see if they’re any good:\n\nlibrary(MASS)\ny &lt;- c(25.4, 25.7, 26.2, 25.8, 25.6, \n       26.4, 25.4, 24.9, 26.0, 26.0)\n\n(cauchy.pars &lt;- fitdistr(y,dcauchy,start=list(location=25,scale=0.5))$estimate)\n\n  location      scale \n25.7668966  0.2682156 \n\nks.test(y,pcauchy,location=cauchy.pars[1],scale=cauchy.pars[2])\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  y\nD = 0.12774, p-value = 0.9968\nalternative hypothesis: two-sided\n\n(normal.pars &lt;- fitdistr(y,dnorm,start=list(mean=25,sd=0.5))$estimate)\n\n      mean         sd \n25.7400002  0.4176117 \n\nks.test(y,pnorm,mean=normal.pars[1],sd=normal.pars[2])\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  y\nD = 0.13322, p-value = 0.9943\nalternative hypothesis: two-sided\n\n\nFrom this we conclude that the small sample of ten dog food weights could easily be from a Cauchy distribution or from a Normal distribution. More observations would help us to determine whether one of these distributions is a better fit to the dog food bag weights.",
    "crumbs": [
      "1. Univariate Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoapproximate.html",
    "href": "howtoapproximate.html",
    "title": "Coding it up in R",
    "section": "",
    "text": "How to iteratively optimize a likelihood function",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoapproximate.html#how-to-iteratively-optimize-a-likelihood-function",
    "href": "howtoapproximate.html#how-to-iteratively-optimize-a-likelihood-function",
    "title": "Coding it up in R",
    "section": "",
    "text": "Using automated routines\nWe have already examined R’s MLE-fitting procedures in Coding it up in R, specifically the functions MASS::fitdistr and fitdistrplus::fitdist, along with the more general workhorse function stats::optim. These functions replicate (or even improve upon) the gradient descent method described previously.\nYou can also use the function stats::uniroot to find a one-dimensional root of an equation, as opposed to hand-coding a Newton-Raphson algorithm. As we know from Univariate likelihood, the parameter which maximizes the likelihood function usually is a root of the derivative of the log-likelihood function.\nFor example, let’s imagine that we are loosing arrows at a target and counting the number of misses until we score a bullseye. After five trials the five counts for the missed shots are: \\(\\boldsymbol{x}=\\{4,6,10,5,5\\}\\). If we assume these are drawn from a geometric distribution with unknown probability \\(p\\), we can use uniroot() to find the zero of the log-likelihood:\nThe geometric PMF is: \\(P(X=k\\;|p) = (1-p)^kp\\)\nThe likelihood function would be: \\(\\mathcal{L}(p|\\boldsymbol{x}) = p^n \\prod_i(1-p)^{x_i}\\)\nThe log-likelihood would be \\(n \\log p + \\sum_i x_i \\log (1-p)\\)\nIts partial derivative with respect to \\(p\\) would be \\(\\frac{n}{p}-\\frac{\\sum_i x_i}{1-p}\\)\nSwitching from equations to R, we solve for the MLE of \\(p\\):\n\nX &lt;- c(4,6,10,5,5)\ndll.geom &lt;- function(p) length(X)/p - sum(X)/(1-p)\nuniroot(dll.geom,c(0.01,0.99))[[1]]\n\n[1] 0.1428571\n\n1/7\n\n[1] 0.1428571\n\n\nWhich should seem intuitively correct: if on average we miss six times \\((\\bar{x}=6)\\) before making a bullseye on the seventh shot, then our probability of making bullseyes should be 1/7.\n\n\n‘Manual’ iterative optimization routines\nI do not really recommend this strategy in an industry or production context. The automated routines found in R (or any other language) converge faster, better, and more surely than what you are likely to buil on your own. Optimization algorithms are an art and science of their own, and take years of training to perform at an enterprise-grade level. However, it can be quite helpful to prove to yourself that you know what you’re doing. The sections The Newton-Raphson method and Gradient descent contain illustrations of simple manual routines you can implement.\nFor example, we can use the Newton-Raphson method instead of optim() to find the root of the derivative of the log-likelihood function in the archery example above:\n\n# helper function to approximate derivatives\nddx &lt;- function(value, func, delta=1e-6){\n  (func(value+delta)-func(value-delta))/(2*delta)}      \n\n# core newton-raphson algorithm, plus some output\nnewton &lt;- function(f, x0, tol=1e-6, max.iter=20){         \n  x &lt;- x0\n  i &lt;- 0\n  results &lt;- data.frame(iter=i,x=x,fx=f(x))              \n  while (abs(f(x))&gt;tol & i&lt;max.iter) {                    \n    i &lt;- i + 1\n    x &lt;- x - f(x)/ddx(x,f)\n    results &lt;- rbind(results,data.frame(iter=i,x=x,fx=f(x)))}\n  return(results)}\n\nnewton(f=dll.geom,x0=0.05,tol=1e-10)\n\n  iter          x           fx\n1    0 0.05000000 6.842105e+01\n2    1 0.08365123 2.703336e+01\n3    2 0.11968296 7.698410e+00\n4    3 0.13953568 9.682306e-01\n5    4 0.14279220 1.857091e-02\n6    5 0.14285712 7.034290e-06\n7    6 0.14285714 1.023182e-12\n\n\nOr we could use gradient descent to maximize the likelihood function more directly:\n\n# gradient descent function with backtracking line search\ngraddesc &lt;- function(f, g, x0, b=0.8, tol=1e-2, max.iter=40){\n  i &lt;- 0\n  x &lt;- x0\n  results &lt;- t(c(0,x,-f(x)))\n  while (sqrt(g(x)%*%g(x))&gt;tol & i&lt;max.iter) {  \n    i &lt;- i + 1\n    t &lt;- 0.01\n    while ((f(x+t*g(x))&lt;f(x)) & t&gt;tol) {  \n      t &lt;- t*b}  \n    x &lt;- x + t*g(x)                                           \n    results &lt;- rbind(results,t(c(i,x,-f(x))))} \n  colnames(results) &lt;- c('iter',paste0('x',1:length(x)),'f') \n  return(results)}\n\nll.geom &lt;- function(p) length(X)*log(p) + sum(X)*log(1-p)\n\ngraddesc(f=ll.geom, g=dll.geom, x0=0.1, tol=1e-10)\n\n      iter        x1        f\n [1,]    0 0.1000000 14.67374\n [2,]    1 0.1853333 14.57728\n [3,]    2 0.1065617 14.57550\n [4,]    3 0.1748781 14.48505\n [5,]    4 0.1251698 14.40218\n [6,]    5 0.1613513 14.39976\n [7,]    6 0.1307366 14.37613\n [8,]    7 0.1546269 14.37302\n [9,]    8 0.1344580 14.36450\n[10,]    9 0.1506242 14.36244\n[11,]   10 0.1370251 14.35905\n[12,]   11 0.1480728 14.35788\n[13,]   12 0.1388114 14.35645\n[14,]   13 0.1463924 14.35583\n[15,]   14 0.1400553 14.35521\n[16,]   15 0.1452663 14.35489\n[17,]   16 0.1409200 14.35461\n[18,]   17 0.1445043 14.35446\n[19,]   18 0.1415197 14.35433\n[20,]   19 0.1439856 14.35425\n[21,]   20 0.1419348 14.35419\n[22,]   21 0.1436312 14.35416\n[23,]   22 0.1422215 14.35413\n[24,]   23 0.1433886 14.35411\n[25,]   24 0.1424194 14.35410\n[26,]   25 0.1432222 14.35409\n[27,]   26 0.1425558 14.35408\n[28,]   27 0.1431081 14.35408\n[29,]   28 0.1426497 14.35408\n[30,]   29 0.1430296 14.35408\n[31,]   30 0.1427144 14.35407\n[32,]   31 0.1429757 14.35407\n[33,]   32 0.1427589 14.35407\n[34,]   33 0.1429387 14.35407\n[35,]   34 0.1427896 14.35407\n[36,]   35 0.1429132 14.35407\n[37,]   36 0.1428107 14.35407\n[38,]   37 0.1428957 14.35407\n[39,]   38 0.1428252 14.35407\n[40,]   39 0.1428837 14.35407\n[41,]   40 0.1428352 14.35407\n\n\nNotice that the Newton-Raphson algorithm converged to the eighth decimal place in seven iterations, while the gradient descent method had only found the first four decimal places after 40 iterations. Better programmers with more experience could wring much more power from gradient descent than I did.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoapproximate.html#how-to-perform-univariate-hypothesis-tests-and-confidence-intervals-using-the-clt-and-the-t-distribution",
    "href": "howtoapproximate.html#how-to-perform-univariate-hypothesis-tests-and-confidence-intervals-using-the-clt-and-the-t-distribution",
    "title": "Coding it up in R",
    "section": "How to perform univariate hypothesis tests and confidence intervals using the CLT and the t-distribution",
    "text": "How to perform univariate hypothesis tests and confidence intervals using the CLT and the t-distribution\n\nt-tests for means and differences of means\nThe MASS::anorexia dataset will help to illustrate some basic t-testing use cases. This dataset describes an experimental trial in which 72 anorexic young women were assigned to three groups: a control group receiving no treatment (“Cont”), a group receiving cognitive behavioral therapy (“CBT”) and a a group receiving family therapy (“FT”).\n\nlibrary(MASS)\nhead(anorexia)\n\n  Treat Prewt Postwt\n1  Cont  80.7   80.2\n2  Cont  89.4   80.1\n3  Cont  91.8   86.4\n4  Cont  74.0   86.3\n5  Cont  78.1   76.1\n6  Cont  88.3   78.1\n\n\n\n90% confidence interval for the mean weight of incoming patients (automated and manual solutions):\n\nt.test(x=anorexia$Prewt,conf.level=0.90)$conf.int\n\n[1] 81.39044 83.42622\nattr(,\"conf.level\")\n[1] 0.9\n\nn.pre &lt;- length(anorexia$Prewt)\nmuhat.pre &lt;- mean(anorexia$Prewt)\nse.pre &lt;- sd(anorexia$Prewt)/sqrt(n.pre)\nmuhat.pre + c(-1,1)*qt(p=0.95,df=n.pre-1)*se.pre\n\n[1] 81.39044 83.42622\n\n\nOne-sided test of whether the mean weight of the incoming patients could be as high as 84 pounds:\n\nt.test(x=anorexia$Prewt,alternative='less',mu=84)$p.value\n\n[1] 0.005576362\n\npt(q=(muhat.pre - 84)/se.pre,df=n.pre-1)\n\n[1] 0.005576362\n\n\nTwo-sample test of whether the mean post-treatment weight of the CBT group was different than the post-treatment weight of the control group (assuming equal variances):\n\nt.test(x=anorexia$Postwt[anorexia$Treat=='CBT'],\n   y=anorexia$Postwt[anorexia$Treat=='Cont'],\n   alternative='two.sided',var.equal=TRUE)$p.value\n\n[1] 0.01692976\n\nn.cont &lt;- sum(anorexia$Treat=='Cont')\nmuhat.cont &lt;- mean(anorexia$Postwt[anorexia$Treat=='Cont'])\nn.cbt &lt;- sum(anorexia$Treat=='CBT')\nmuhat.cbt &lt;- mean(anorexia$Postwt[anorexia$Treat=='CBT'])\nvar.pooled &lt;- (var(anorexia$Postwt[anorexia$Treat=='Cont'])*(n.cont-1) +\n           var(anorexia$Postwt[anorexia$Treat=='CBT'])*(n.cbt-1)) /\n          (n.cont + n.cbt - 2)\nse.pooled &lt;- sqrt(var.pooled/n.cont + var.pooled/n.cbt)\n2*(1-pt(q=(muhat.cbt - muhat.cont)/se.pooled,df=n.cont + n.cbt - 2))\n\n[1] 0.01692976\n\n\nTwo-sample 90% confidence interval for how much more the CBT group weighed after treatment than then control group (assuming equal variances):\n\nt.test(x=anorexia$Postwt[anorexia$Treat=='CBT'],\n   y=anorexia$Postwt[anorexia$Treat=='Cont'],\n   conf.level=0.90,var.equal=TRUE)$conf.int\n\n[1] 1.473675 7.704044\nattr(,\"conf.level\")\n[1] 0.9\n\n(muhat.cbt - muhat.cont) + c(-1,1)*qt(p=0.95,df=n.cbt+n.cont-2)*se.pooled\n\n[1] 1.473675 7.704044\n\n\nMatched-pairs 90% confidence interal measuring how much the CBT group gained during the treatment process. Note that this test requires each observation in the first group to be matched to a corresponding observation in the second group (as is true in a before-and-after study):\n\nt.test(x=anorexia$Postwt[anorexia$Treat=='CBT'],\n   y=anorexia$Prewt[anorexia$Treat=='CBT'],\n   paired=TRUE,conf.level=0.90,var.equal=TRUE)$conf.int\n\n[1] 0.6981979 5.3155952\nattr(,\"conf.level\")\n[1] 0.9\n\nmuhat.paired &lt;- mean(anorexia$Postwt[anorexia$Treat=='CBT'] -\n                 anorexia$Prewt[anorexia$Treat=='CBT'])\nse.paired &lt;- sd(anorexia$Postwt[anorexia$Treat=='CBT'] - \n            anorexia$Prewt[anorexia$Treat=='CBT'])/sqrt(n.cbt)\nmuhat.paired + c(-1,1)*qt(p=0.95,df=n.cbt-1)*se.paired\n\n[1] 0.6981979 5.3155952\n\n\n\n\n\nz-tests for proportions and differences of proportions\nSticking with the MASS::anorexia dataset, let’s define a “success” as a patient who weighed more post-treatment than pre-treatment.1\n\nanorexia$Success &lt;- 1*(anorexia$Postwt &gt; anorexia$Prewt)\n\nNow we can examine the proportions of success within and between the different treatment groups. For example:\n\nTwo-sided 80% confidence interval for the one-sample proportion of successes in the family therapy group.\n\nx.ft &lt;- sum(anorexia$Success[anorexia$Treat=='FT'])\nn.ft &lt;- length(anorexia$Success[anorexia$Treat=='FT'])\nprop.test(x=x.ft,n=n.ft,conf.level=0.80)$conf.int\n\n[1] 0.5819867 0.8909870\nattr(,\"conf.level\")\n[1] 0.8\n\n\nOne-sided, one-sample proportion test of whether the family therapy grpoup achieved at least a 60% success rate. Note that R by default uses a “continuity correction” which improves upon the CLT approximation for small sample sizes. By disabling this continuity correction we can manually match R’s results using the CLT:\n\nprop.test(x=x.ft,n=n.ft,p=0.60,alternative='greater')$p.value\n\n[1] 0.1274205\n\nprop.test(x=x.ft,n=n.ft,p=0.60,alternative='greater',correct=FALSE)$p.value\n\n[1] 0.08284192\n\n1-pnorm((x.ft/n.ft - 0.6)/sqrt(0.6*0.4/n.ft))\n\n[1] 0.08284192\n\n\nTwo-sided, two-proportion test of whether the family therapy group and the CBT group have different success rates:\n\nx.cbt &lt;- sum(anorexia$Success[anorexia$Treat=='CBT'])\nprop.test(x=c(x.ft,x.cbt),n=c(n.ft,n.cbt),\n      alternative='two.sided')$p.value\n\n[1] 0.4965428\n\nprop.test(x=c(x.ft,x.cbt),n=c(n.ft,n.cbt),\n      alternative='two.sided',correct=FALSE)$p.value\n\n[1] 0.3145389\n\np.combined &lt;- (x.ft + x.cbt)/(n.ft + n.cbt)\nse.combined &lt;- sqrt(p.combined*(1-p.combined)*(1/n.ft+1/n.cbt))\n2*(1-pnorm((x.ft/n.ft - x.cbt/n.cbt)/se.combined))\n\n[1] 0.3145389",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoapproximate.html#footnotes",
    "href": "howtoapproximate.html#footnotes",
    "title": "Coding it up in R",
    "section": "",
    "text": "This is a low bar; we might normally want to set a minimum threshold for weight gain.↩︎",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "rulesofthumb.html#extremely-quick-and-dirty-tests-and-intervals",
    "href": "rulesofthumb.html#extremely-quick-and-dirty-tests-and-intervals",
    "title": "Handy rules of thumb",
    "section": "Extremely quick and dirty tests and intervals",
    "text": "Extremely quick and dirty tests and intervals\nBuilding off the above tip, I want to give you a mental math tool that would never be rigorous enough for a formal result, but which can help you make decisions during EDA and answer some stakeholder questions in real time.\nUnder the null hypothesis, sample statistics will be observed within two standard errors of the hypothesized mean roughly 95% of the time.\nThis basic concept allows us to approximate some basic hypothesis tests and create some rough confidence intervals very quickly. The results are not always very accurate, but most results in life are not borderline cases: they point pretty clearly in one direction or another. So to speed our work, we can rely upon this rule to move forward and follow up with more precise calculations when needed.\nLet’s apply this rule to several different distributions. No R code or calculators were needed for these examples!\n\nWhen estimating a proportion from a sample, what sample size would you need for the estimate to likely be within 5 percentage points of the truth?\n\nThe mean of a Bernoulli variable is \\(\\mu=p\\)\nThe variance of a Bernoulli variable is \\(p(1-p)\\)\nThe standard error of its mean would be \\(s.e.\\!(\\hat{\\mu})=\\sqrt{p(1-p)/n}\\)\nThis error is at its maximum when \\(p=0.5\\) and \\(s.e.\\!(\\hat{\\mu})=0.5/\\sqrt{n}\\)\nTwo standard errors would be a radius of \\(1/\\sqrt{n}\\)\n\\(1/\\sqrt{n}=10\\%\\) when \\(n=100\\) and \\(1/\\sqrt{n}=5\\%\\) when \\(n=400\\)\nTherefore we need a sample of 400 if we want our sample proportion to likely be within 5 percentage points of the mean. If we’re okay with a possible 10pp error, then our sample size could be 100.\n\nA hospital in a rural area sees 100 births over a four-year period. If births are Poisson-distributed, what’s a 95% confidence interval for the yearly rate parameter \\(\\lambda\\)?\n\nIn the Poisson distribution, \\(\\lambda\\) is both the mean and the variance\nSo we estimate \\(\\hat{\\lambda}=\\hat{\\mu}=\\hat{\\sigma}^2=\\bar{x}=25\\)\nThe standard error would be \\(\\hat{\\sigma}/\\sqrt{n}=5/2=2.5\\)\nTwo standard errors would be 5\nOur confidence interval for \\(\\lambda\\) is \\(25 \\pm 5 = [20,30]\\)\n\nWe conduct a test producing a \\(\\chi^2\\) (chi-squared) statistic of 118 with 72 degrees of freedom. Can we reject the null at the 5% significance level?\n\nIn the \\(\\chi^2\\) distribution, the mean is \\(df\\) and the variance is \\(2\\cdot df\\)\nUnder the null, sample statistics will mostly be observed with the range \\(\\mu \\pm 2\\sigma\\)\n\\(\\mu = df = 72\\)\n\\(\\sigma = \\sqrt{2 df} = \\sqrt{144} = 12\\)\nWe would only expect to observe statistics as high as \\(72 + 2 \\cdot 12 = 96\\)\nA sample statistic of 118 therefore gives us enough evidence to reject the null hypothesis.",
    "crumbs": [
      "2. Useful Approximations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Handy rules of thumb</span>"
    ]
  },
  {
    "objectID": "variancedecomposition.html#motivating-the-problem",
    "href": "variancedecomposition.html#motivating-the-problem",
    "title": "Variance decomposition",
    "section": "",
    "text": "Unitless\nNot affected by the scale of the response or the predictors\nMeasures how “good” a model is independent of other models, that is, the metric should mean something even when applied to a single model",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Variance decomposition</span>"
    ]
  },
  {
    "objectID": "howtoregress.html",
    "href": "howtoregress.html",
    "title": "Coding it up in R",
    "section": "",
    "text": "How to specify a linear regression in R\nLet’s run multiple (&gt;1 predictor) OLS regression in R, and learn how to read its summaries and use its output objects.\nThe penguins dataset describes measuresments of three species of Antarctic penguins, taken from three islands from 2007–2009. Let’s see if we can predict body mass (in grams) from bill length and flipper length (in millimeters).\npeng.lm &lt;- lm(body_mass ~ bill_len + flipper_len, data=penguins)\nAlready I need to stop and tell you about several important details in this simple line of code.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoregress.html#how-to-specify-a-linear-regression-in-r",
    "href": "howtoregress.html#how-to-specify-a-linear-regression-in-r",
    "title": "Coding it up in R",
    "section": "",
    "text": "The formula: The syntax I supplied was Y ~ X1 + X2, which R understands as “\\(Y\\) is a linear function of the predictors \\(X_1\\) and \\(X_2\\) and also an intercept.\n\nI don’t need to specify an intercept (that is, using the column of ones as a predictor), since it’s the default. If I wish to make the intercept explicit, I would write Y ~ 1 + X1 + X2 (the results would be identical.)\nIf I don’t want an intercept, I could use the syntax Y ~ 0 + X1 + X2, or alternatively Y ~ -1 + X1 + X2 (no mathematical difference). These formulae are not literal equations. They simply help R to understand what model to run. We will discuss the implications of a no-intercept model later.\nTechnically, R doesn’t recognize the terms body_mass, bill_len, or flipper_len. These aren’t defined objects that R is “aware of”. By using the data= argument, we are telling R “it’s okay that you don’t recognize these names; you’ll find objects with these names in this dataset.”\nWe can also write the formula without using the data= argument, and simply provide the objects directly to R. This method works well when your predictors and response variables are in different data frames, or not in any data frame at all (e.g. stored as separate vectors): lm(penguins$body_mass ~ penguins$bill_len + penguins$flipper_len)\n\nThe assignment: Notice that I didn’t ask to see the results of the lm() call, but instead I stored the results as an R object. This allows me to modify it later, and to retrieve objects like the fitted values or the residuals whenever I like. Running R modesl without storing them into memory is very rare.\nThe function call: I used lm() here since I am running a linear model. In the future we will use functions like glm(), glm.nb(), or zeroinfl() to run non-linear models, and their syntax will be very similar.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoregress.html#how-to-interpret-the-basic-summary-of-a-linear-regression-in-r",
    "href": "howtoregress.html#how-to-interpret-the-basic-summary-of-a-linear-regression-in-r",
    "title": "Coding it up in R",
    "section": "How to interpret the basic summary of a linear regression in R",
    "text": "How to interpret the basic summary of a linear regression in R\nThe R function summary() will provide basic information about whatever you put inside of it: datasets, models, lists, etc. When we apply it to an lm() object, we see the following output:\n\nsummary(peng.lm)\n\n\nCall:\nlm(formula = body_mass ~ bill_len + flipper_len, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1090.5  -285.7   -32.1   244.2  1287.5 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5736.897    307.959 -18.629   &lt;2e-16 ***\nbill_len        6.047      5.180   1.168    0.244    \nflipper_len    48.145      2.011  23.939   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 394.1 on 339 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7585 \nF-statistic: 536.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s learn how to read this from top to bottom:\n\nThe call: Repeats its instructions back to the user. Helps to remind you which model you’re looking at, especially if your models are being auto-generated by a function.\nThe residuals: Not very useful. Can help you to notice a large outlier or a skewed distribution (e.g. if \\(|\\mathrm{Q1}| \\ll \\mathrm{Q3}\\)), but those should be examined separately.\nThe table of coefficients: Each row of this table refers to a different \\(\\hat{\\beta}\\) estimated by the model. (Intercept) refers to \\(\\hat{\\beta}_0\\) and each slope after the intercept is labeled using the name of its corresponding predictor.\nThe estimates themselves are in the first column, Estimate. These are our best guesses, not the truth (which is still unknown). The second column, Std. Error, provides standard errors for our estimates which can be used to conduct hypothesis tests and to construct confidence intervals. The third and fourth columns, t value and Pr(&gt;|t|), conduct a two-sided hypothesis test that the true value of the parameter is actually 0.\nIf we can reject this null hypothesis, then we can say with some confidence that the predictor “belongs” in the model.1 On the other hand, if we can’t reject the null that \\(\\beta_j = 0\\), then the predictor has no place in our model. After all, the following two equations are equivalent:\n\\[\\begin{aligned} Y &= \\beta_0 + \\beta_1 X_1 + 0 X_2 + \\beta_3 X_3 + \\varepsilon \\\\ Y &= \\beta_0 + \\beta_1 X_1 + \\beta_3 X_3 + \\varepsilon \\end{aligned}\\]\nHere we estimate that for every additional millimeter of bill length, penguins weigh an additional 6 grams, and that for every additional millimeter of flipper length, penguins weigh an additional 48 grams. Furthermore, a penguin with no bill or flippers would be expected to weigh -5736 grams.2\nThe model fit statistics: At the very bottom we learn more about how well the regression model describes the data. Importantly, we see that two rows of the original dataset were not used due to missing values in the response or the predictors — regressions cannot use a row in which either \\(Y\\) or any of the \\(X\\) predictors are missing.\nWe also see that R^2 is about 76%, meaning that 76% of the variation in penguin body mass can be explained by variation in penguin bill and flipper lengths. The remaining model fit metrics must wait until we learn a little more theory.",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoregress.html#footnotes",
    "href": "howtoregress.html#footnotes",
    "title": "Coding it up in R",
    "section": "",
    "text": "In future sections we will add more nuance to this idea.↩︎\nLater, we shall learn why these estimates are flawed and unrealistic.↩︎\nAs a general rule, any information that R prints in an output can be accessed through the code as well.↩︎\nNotice that the fourth fitted value is labeled 5 — a clear sign that the fourth row of the original data could not be used in the regression.↩︎\nOr, for the first row, a model with no other predictors.↩︎\nFor our simple linear predictors this will always be 1; later, we shall consider categorical predictors which use more degrees of freedom.↩︎",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoregress.html#how-to-retrieve-and-use-regression-model-attributes-in-r",
    "href": "howtoregress.html#how-to-retrieve-and-use-regression-model-attributes-in-r",
    "title": "Coding it up in R",
    "section": "How to retrieve and use regression model attributes in R",
    "text": "How to retrieve and use regression model attributes in R\nSometimes all we need to move forward can be found in the model summary. More usually, we will want to use objects from the regression. These can be found in two places: (i) in the lm() object, and (ii) in the summary() object.3\nRecall that we named the penguin regression peng.lm. This object now has many attributes we can select by name:\n\nnames(peng.lm)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n\n\n\nThe fitted values (also called \\(\\hat{\\boldsymbol{y}}\\) or the predictions) are stored as a vector in fitted.values:4\n\n\nhead(peng.lm$fitted.values)\n\n       1        2        3        5        6        7 \n3213.779 3456.922 3895.064 3777.003 3648.292 3212.570 \n\n\n\nThe residuals (also called \\(\\boldsymbol{e}\\) or the errors) are stored as a vector in residuals.\n\n\nhead(peng.lm$residuals)\n\n          1           2           3           5           6           7 \n 536.220898  343.077607 -645.064115 -327.003441    1.707668  412.430396 \n\n\n\nThe estimated coefficients (also called \\(\\hat{\\boldsymbol{\\beta}}\\) or the parameter estimates) are stored as a vector in coefficients.\n\n\npeng.lm$coefficients\n\n (Intercept)     bill_len  flipper_len \n-5736.897161     6.047488    48.144859 \n\n\n\nIf you need more information about the coefficients, you can retrieve the matrix of standard errors, t-statistics, and p-values from the summmary object as coefficients. Notice that the estimates are the same as the lm() coefficients, this method simply extracts more information.\n\n\nsummary(peng.lm)$coefficients\n\n                Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) -5736.897161 307.959128 -18.628762 7.796205e-54\nbill_len        6.047488   5.179831   1.167507 2.438263e-01\nflipper_len    48.144859   2.011115  23.939388 7.564660e-75\n\nsummary(peng.lm)$coefficients[1,4] #p-value for intercept\n\n[1] 7.796205e-54\n\n\n\nThe estimated error variance, \\(\\hat{\\sigma}^2\\) can also be found in the summary information. In R, the value which is stored is simply \\(\\hat{\\sigma}\\), the standard deviation, stored as sigma.\n\n\nsummary(peng.lm)$sigma\n\n[1] 394.0678\n\n\n\nAnd the last item for now will the R2 model metric, which can be found in the summary information under r.squared:\n\n\nsummary(peng.lm)$r.squared\n\n[1] 0.7599577",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoregress.html#how-to-make-predictions-for-new-values-using-a-regression-object",
    "href": "howtoregress.html#how-to-make-predictions-for-new-values-using-a-regression-object",
    "title": "Coding it up in R",
    "section": "How to make predictions for new values using a regression object",
    "text": "How to make predictions for new values using a regression object\nThe fitted values provide our model’s guess as to where the training data “should” have been observed. However, we often want to learn where new \\(X\\)-values would be observed. As described in Multiple regression, predictions can take two forms:\n\nPredictions for where new values will be observed, or\nPredictions for the mean of the new observations.\n\nBoth of these predictions share the same point estimates, they simply differ in their standard errors, i.e. in the width of their confidence intervals.\nThe point estimates for new observations can be computed automatically using the predict() function or manually from the estimated coefficients as \\(\\mathbf{X}_{\\textrm{new}} \\hat{\\boldsymbol{\\beta}}\\). Let’s build five new penguins and find their predicted body mass both ways:\n\nnewpengs &lt;- data.frame(ones=c(1,1,1,1,1),\n                       bill_len=c(35,40,45,50,55),\n                       flipper_len=c(175,185,195,205,215))\npredict(peng.lm,newdata=newpengs)\n\n       1        2        3        4        5 \n2900.115 3411.801 3923.487 4435.173 4946.859 \n\nas.matrix(newpengs)%*%peng.lm$coefficients\n\n         [,1]\n[1,] 2900.115\n[2,] 3411.801\n[3,] 3923.487\n[4,] 4435.173\n[5,] 4946.859\n\n\nNote a few details here: we had to change the data frame (which R regards as a type of list) into a matrix so that we could matrix multiply it by the vector of beta-hats using the %*% operator. We also built in a vector of ones into the new data — totally unnecessary for the automatic calculation using predict(), but helpful for the manual solution. Finally, note that we had to build a data frame with the same names as the original dataset, so that the regression object peng.lm could know which vectors to use in which “roles”.\nThe wider confidence intervals for the possible location of new values can be found as follows:\n\npredict.lm(peng.lm,newdata=newpengs,level=0.9,interval='prediction')\n\n       fit      lwr      upr\n1 2900.115 2245.681 3554.549\n2 3411.801 2759.673 4063.930\n3 3923.487 3272.035 4574.940\n4 4435.173 3782.762 5087.584\n5 4946.859 4291.863 5601.856\n\n\nAnd the narrower confidence intervals for the location of the mean for new values can be found with this change:\n\npredict.lm(peng.lm,newdata=newpengs,level=0.9,interval='confidence')\n\n       fit      lwr      upr\n1 2900.115 2823.723 2976.507\n2 3411.801 3358.665 3464.938\n3 3923.487 3879.417 3967.558\n4 4435.173 4378.676 4491.670\n5 4946.859 4865.788 5027.931",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "howtoregress.html#recovering-anova-information-from-a-regression-using-r",
    "href": "howtoregress.html#recovering-anova-information-from-a-regression-using-r",
    "title": "Coding it up in R",
    "section": "Recovering ANOVA information from a regression using R",
    "text": "Recovering ANOVA information from a regression using R\nOnce we fit a regression in R, it’s easy to decompose the total variance of \\(\\boldsymbol{y}\\) into (i) variance explained by the model and (ii) the residual variance. That information comes to us from the anova() command:\n\nanova(peng.lm)\n\nAnalysis of Variance Table\n\nResponse: body_mass\n             Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \nbill_len      1 77669072 77669072  500.16 &lt; 2.2e-16 ***\nflipper_len   1 88995501 88995501  573.09 &lt; 2.2e-16 ***\nResiduals   339 52643125   155289                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s review this table piece by piece:\n\nThe first two rows describe variance explained by each individual predictor, and the last row marked Residuals describes the residual variance.\nThe ANOVA tables in R are order-dependent – the variance explained by each individual predictor is only the additional variance explained by adding that predictor to a model with all previously-listed variables in the model.5 ANOVA tables from other software tools may work differently.\nFor each predictor, the first column describes the model degrees of freedom used up by that predictor.6 In the last Residuals row, the first column lists the total residual degrees of freedom \\(n - k - 1\\).\nThe second column lists the sum of squares contribution from each predictor toward the SSM, as well as the residual sum of squares/sum of squared errors (SSE).\nThe remaining columns are used to conduct an \\(F\\) test which we will study in later sections.\n\nWe can access all of this information from the anova() object, and we can even compare it to our manual calculations of SST, SSM, and SSE. Starting with the model sum of squares, SSM, notice that the two contributions from bill_len and flipper_len total about 166 million g2. We can recreate total directly from the fitted values:\n\nanova(peng.lm)[1,2] + anova(peng.lm)[2,2]\n\n[1] 166664572\n\n(SSM &lt;- sum((peng.lm$fitted.values-mean(penguins$body_mass,na.rm=TRUE))^2))\n\n[1] 166664572\n\n\nThe SSE or sum of squared errors can be found using the bottom row of the table:\n\nanova(peng.lm)[3,2]\n\n[1] 52643125\n\n(SSE &lt;- sum(peng.lm$residuals^2,na.rm=TRUE))\n\n[1] 52643125\n\n\nAnd the total sum of squares can be found either by totaling the Sum Sq column of the ANOVA table, or directly from \\(\\boldsymbol{y}\\), or by adding SSM and SSE:\n\nsum(anova(peng.lm)[,2])\n\n[1] 219307697\n\n(SST &lt;- sum((penguins$body_mass-mean(penguins$body_mass,na.rm=TRUE))^2,na.rm=TRUE))\n\n[1] 219307697\n\nSSM + SSE\n\n[1] 219307697",
    "crumbs": [
      "3. Linear Regression",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "proofs.html",
    "href": "proofs.html",
    "title": "Appendix C — Proofs",
    "section": "",
    "text": "Arithmetic solution for simple OLS regression\nWe can readily find the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimize the residual sum of squares by writing out the RSS function, taking its first partial derivatives, setting them equal to zero, and solving. We require two preliminary results which are purely arithmetic and not a matter of statistical theory:\nNote that,\n\\[\\begin{aligned} \\sum_i(x_i - \\bar{x}) x_i &= \\sum_i(x_i^2 - \\bar{x} x_i) = \\sum_i x_i^2 - \\bar{x}\\sum_i x_i \\\\ &= \\sum_i x_i^2 - n\\bar{x}^2\\end{aligned}\\]\nAnd also that,\n\\[\\begin{aligned} \\sum_i(x_i - \\bar{x})^2 &= \\sum_i(x_i^2 - 2x_i\\bar{x}  + \\bar{x}^2) = \\sum_i x_i^2 - 2\\bar{x}\\sum_i x_i + n\\bar{x}^2 \\\\ &= \\sum_i x_i^2 - n\\bar{x}^2 \\end{aligned}\\] And so by transitivity,\n\\[(1) \\quad \\sum_i(x_i - \\bar{x}) x_i = \\sum_i(x_i - \\bar{x})^2\\]\nLikewise note that,\n\\[\\begin{aligned} \\sum_i(y_i - \\bar{y}) x_i &= \\sum_i(y_i x_i - \\bar{y} x_i) = \\sum_i y_i x_i - \\bar{y}\\sum_i x_i \\\\ &= \\sum_i y_i x_i - n\\bar{y}\\bar{x} \\end{aligned}\\]\nAnd also that,\n\\[\\begin{aligned} \\sum_i(y_i - \\bar{y})(x_i - \\bar{x}) &= \\sum_i(y_i x_i - y_i\\bar{x} - \\bar{y}x_i + \\bar{y}\\bar{x}) = \\sum_i y_i x_i - \\bar{x}\\sum_i y_i - \\bar{y}\\sum_i x_i + \\bar{y}\\bar{x} \\\\ &= \\sum_i y_i x_i - n\\bar{y}\\bar{x} \\end{aligned}\\] And so by transitivity,\n\\[(2) \\quad \\sum_i(y_i - \\bar{y}) x_i = \\sum_i(y_i - \\bar{y})(x_i - \\bar{x})\\]\nNow for the actual Least Squares solution. First, we write out the RSS function which we need to minimize:\n\\[\\mathrm{RSS} = \\sum_i e_i^2 = \\sum_i (y_i - \\hat{y}_i)^2 = \\sum_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\\]\nNext we take the derivative with respect to \\(\\hat{\\beta}_0\\), set it equal to zero, and solve:\n\\[\\begin{aligned} \\frac{\\delta\\mathrm{RSS}}{\\delta\\hat{\\beta}_0} = 2 \\sum_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)(-1) &= 0 \\\\ \\longrightarrow  \\sum_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) &= 0 \\\\ \\longrightarrow \\sum_i y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1 \\sum_i x_i &= 0 \\\\ \\longrightarrow n\\bar{y} - n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\longrightarrow n\\hat{\\beta}_0 &= n\\bar{y} - n\\hat{\\beta}_1 \\bar{x} \\\\ \\longrightarrow \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\end{aligned}\\] ̅ Last we take the derivative with respect to \\(\\hat{\\beta}_1\\), set it equal to zero, and solve as well. Notice that we use the substitution for \\(\\hat{\\beta}_0\\) derived above, and that the final lines rely upon preliminary results (1) and (2):\n\\[\\begin{aligned} \\frac{\\delta\\mathrm{RSS}}{\\delta\\hat{\\beta}_1} = 2 \\sum_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)(-x_i) &= 0 \\\\ \\longrightarrow \\sum_i (y_i - \\bar{y} + \\hat{\\beta}_1\\bar{x} - \\hat{\\beta}_1x_i)(-x_i) &= 0 \\\\ \\longrightarrow \\sum_i (y_i - \\bar{y})x_i - \\hat{\\beta}_1 \\sum_i (x_i - \\bar{x}) x_i &= 0 \\\\ \\longrightarrow \\hat{\\beta}_1 &= \\frac{\\sum_i (y_i - \\bar{y})x_i}{\\sum_i (x_i - \\bar{x}) x_i} \\\\ \\longrightarrow \\hat{\\beta}_1 &= \\frac{\\sum_i (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_i (x_i - \\bar{x})^2} \\\\ \\longrightarrow \\hat{\\beta}_1 &= \\frac{\\mathrm{Cov}(\\boldsymbol{y},\\boldsymbol{x})}{\\mathbb{V}[\\boldsymbol{x}]} \\end{aligned}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Proofs</span>"
    ]
  },
  {
    "objectID": "proofs.html#arithmetic-solution-for-simple-ols-regression",
    "href": "proofs.html#arithmetic-solution-for-simple-ols-regression",
    "title": "Appendix C — Proofs",
    "section": "",
    "text": "Finding 1\n\n\n\n\n\n\n\nFinding 2",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Proofs</span>"
    ]
  },
  {
    "objectID": "proofs.html#linear-algebraic-solution-for-multiple-ols-regression",
    "href": "proofs.html#linear-algebraic-solution-for-multiple-ols-regression",
    "title": "Appendix C — Proofs",
    "section": "Linear algebraic solution for multiple OLS regression",
    "text": "Linear algebraic solution for multiple OLS regression\nThe simple linear regression case above conveys the concept of the least squares solution, but readers may wish to see a full solution for the more general case with multiple predictors. This proof will require you to know (or feel your way through) some linear algebra and vector calculus. Let us begin with two findings which help set up the main finding:\n\nFinding 1\n\n\\[(1) \\quad (\\boldsymbol{a} - \\boldsymbol{b}) \\circ (\\boldsymbol{a} - \\boldsymbol{b}) = \\boldsymbol{a} \\circ \\boldsymbol{a} + \\boldsymbol{b} \\circ \\boldsymbol{b} - 2\\boldsymbol{a} \\circ \\boldsymbol{b}\\]\nThe finding above is really just algebra, but we shall be glad of it soon. Take note that the bolded letters are vectors, not scalars, though the result matches our scalar expectations.\n\nFinding 2\n\n\\[\\begin{aligned} \\mathrm{RSS}(\\hat{\\boldsymbol{\\beta}}) &= \\sum_i e_i^2 = \\boldsymbol{e} \\circ \\boldsymbol{e} = (\\boldsymbol{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\circ (\\boldsymbol{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\\\ (2) \\quad &= \\boldsymbol{y} \\circ \\boldsymbol{y} + \\hat{\\boldsymbol{\\beta}}^T(\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}} - 2\\boldsymbol{y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\end{aligned}\\]\nThis second finding relies on the first, and it establishes that the residual sum of squares for a given choice of betas can be condensed into three summary statistics of the data: \\(\\boldsymbol{y} \\circ \\boldsymbol{y}\\), \\(\\mathbf{X}^T\\mathbf{X}\\), and \\(\\boldsymbol{y}^T \\mathbf{X}\\). You can think of these in turn as functions of the means and variances of \\(\\boldsymbol{y}\\) and \\(\\mathbf{X}\\) along with the covariance of \\(\\boldsymbol{y}\\) and \\(\\mathbf{X}\\), which matches the simple regression case above.\nWith these findings, we proceed to the main event: finding the vector \\(\\hat{\\boldsymbol{\\beta}}\\) which minimizes the residual sum of squares. We take the gradient vector of RSS with respect to \\(\\hat{\\boldsymbol{\\beta}}\\), set equal to zero, and solve:\n\\[\\begin{aligned} \\nabla \\mathrm{RSS}(\\hat{\\boldsymbol{\\beta}}) = \\frac{\\delta}{\\delta\\hat{\\boldsymbol{\\beta}}}\\left( \\boldsymbol{y} \\circ \\boldsymbol{y} + \\hat{\\boldsymbol{\\beta}}^T(\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}} - 2\\boldsymbol{y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\right) & \\\\ = 2(\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}} - 2\\boldsymbol{y}^T \\mathbf{X} &= 0 \\\\ \\longrightarrow (\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}} &= \\boldsymbol{y}^T \\mathbf{X} \\\\ \\longrightarrow \\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{y} \\end{aligned}\\]\nBecause \\(\\hat{\\boldsymbol{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}=\\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{y}\\), we can think of the fitted values \\(\\hat{\\boldsymbol{y}}\\) as a projection of the original values \\(\\boldsymbol{y}\\) onto the model space of \\(\\mathbf{X}\\). The projection matrix for this transformation (sometimes also called the “hat matrix”) would be \\(\\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Proofs</span>"
    ]
  },
  {
    "objectID": "heteroskedasticity.html",
    "href": "heteroskedasticity.html",
    "title": "Heteroskedasticity",
    "section": "",
    "text": "Categories and illustrations of heteroskedasticity\nOLS regression models require homoskedasticity, meaning that each error \\(\\varepsilon_i\\) is drawn from the same normal distribution with mean zero and a constant variance \\(\\sigma^2\\). (Homoskedasticity comes from the Greek for “equally scattered”). Of course, each error will be a different size — some positive, some negative, some small, some large — but the probability of an error being a certain size will be the same for each observation. The troubling alternative to homoskedasticity is heteroskedasticity, in which different observations have differing error variances.1\nHeteroskedasticity takes a few different forms:\nCode\nset.seed(1789)\nhetX &lt;- sort(runif(100,0,7))\nhetE &lt;- rnorm(100)\nhetY &lt;- 1000 + 20*hetX + hetE*rep(c(30,10),times=c(70,30))\nplot(hetX,hetY,main='X-Y plot of original data',xlab='Total GPU time (hours)',\n     ylab='Weekly power costs ($)',pch=19)\n\n\n\n\n\nHeteroskedasticity from a mixed distribution\n\n\n\n\nCode\nplot(lm(hetY~hetX)$residuals,main='Ordered plot of OLS residuals',\n     xlab='Week',ylab='Residual power costs ($)',pch=19,)\nabline(h=0,col='grey50',lty=3)\nabline(v=70.5,lwd=2,lty=2,col='#0000ff')\ntext(88,50,labels='Solar battery',col='#0000ff')\n\n\n\n\n\nHeteroskedasticity from a mixed distribution\nset.seed(1776)\nhetE &lt;- sample(c(1,1,1,1,5),500,TRUE)*rnorm(500,0,0.5)\nplot(hetE,main='Daily unexplained market movement',xlab='Trading Day',\n     ylab='Residual S&P 500 Return',pch=19)\n\n\n\n\n\n\n\nhist(hetE,breaks=seq(-7.25,7.25,0.25),main='Histogram of Residual Returns',\n     xlab='Daily Residual Return (%)')\nlines(seq(-7,7,0.1),dnorm(seq(-7,7,0.1),0,2.5)*0.1*500,lwd=2,col='#0000ff')\nlines(seq(-7,7,0.1),dnorm(seq(-7,7,0.1),0,0.5)*0.2*500,lwd=2,col='#0000ff',lty=2)\nlegend(x='topright',col='#0000ff',lty=c(1,2),legend=c('News Day','Quiet Day'),bty='n')\nCode\nset.seed(1814)\nhetE &lt;- rnorm(1000)*arima.sim(list(ar=0.993),n=1000)\nplot(hetE,main='Volatility of a Fictitious Cryptocurrency Over Time',\n     xlab='Days since ICO',ylab='% Change in Value')\n\n\n\n\n\nHeteroskedasticity not associated with the predictors\nCode\nset.seed(1803)\nhetX &lt;- runif(50,0.3,3)\nhetE &lt;- runif(50,0.75,1.25)\nhetY &lt;- -200+1100*hetX*hetE\nplot(hetX,hetY,main='Diamond Value by Carat Size',\n     xlab='Carats',ylab='Retail Value ($)',pch=19)\nabline(reg=lm(hetY~hetX),lty=2,lwd=2,col='grey50')\n\n\n\n\n\nHeteroskedasticity associated with the predictors\n\n\n\n\nCode\nplot(hetX,lm(hetY~hetX)$residuals,main='Residuals of Diamond Value by Carat Size',\n     xlab='Carats',ylab='Residual of Retail Value ($)',pch=19)\nabline(h=0,lty=2,lwd=2,col='grey50')\n\n\n\n\n\nHeteroskedasticity associated with the predictors",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Heteroskedasticity</span>"
    ]
  },
  {
    "objectID": "listofsymbols.html",
    "href": "listofsymbols.html",
    "title": "Appendix D — List of Symbols",
    "section": "",
    "text": "Lowercase Greek letters",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>List of Symbols</span>"
    ]
  },
  {
    "objectID": "listofsymbols.html#lowercase-greek-letters",
    "href": "listofsymbols.html#lowercase-greek-letters",
    "title": "Appendix D — List of Symbols",
    "section": "",
    "text": "Letter\nSymbol\nUses\n\n\n\n\nalpha\n\\(\\alpha\\)\nA generic constant, or a significance threshold\n\n\nbeta\n\\(\\beta\\)\nAn intercept, slope, or other model coefficient\n\n\ngamma\n\\(\\gamma\\)\nA step size\n\n\ndelta\n\\(\\delta\\)\nA partial derivative, or a tiny amount\n\n\nepsilon\n\\(\\varepsilon\\)\nAn error term, or a tolerance factor\n\n\neta\n\\(\\eta\\)\nThe linear predictor for a GLM\n\n\ntheta\n\\(\\theta\\)\nA generic parameter or set of parameters\n\n\nlambda\n\\(\\beta\\)\nThe rate of a Poisson or exponential distribution\n\n\nmu\n\\(\\mu\\)\nThe mean of a random variable\n\n\npi\n\\(\\pi\\)\nThe constant 3.14159…, or a probability\n\n\nrho\n\\(\\rho\\)\nCorrelation\n\n\nsigma\n\\(\\sigma\\)\nThe standard deviation of a random variable\n\n\n\n\\(\\sigma^2\\)\nThe variance of a random variable\n\n\nphi\n\\(\\phi\\)\nDispersion parameter for quasi-likelihood models\n\n\nchi\n\\(\\chi\\)\nSymbol used to name the chi-squared distribution",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>List of Symbols</span>"
    ]
  },
  {
    "objectID": "listofsymbols.html#uppercase-greek-letters",
    "href": "listofsymbols.html#uppercase-greek-letters",
    "title": "Appendix D — List of Symbols",
    "section": "Uppercase Greek letters",
    "text": "Uppercase Greek letters\n\n\n\nLetter\nSymbol\nUses\n\n\n\n\nDelta\n\\(\\Delta\\)\nChange\n\n\nPi\n\\(\\Pi\\)\nProduct\n\n\nSigma\n\\(\\Sigma\\)\nSum\n\n\nPhi\n\\(\\Phi\\)\nCDF of the standard normal distribution\n\n\nOmega\n\\(\\Omega\\)\nUniversal set (all possible outcomes)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>List of Symbols</span>"
    ]
  },
  {
    "objectID": "listofsymbols.html#set-theory",
    "href": "listofsymbols.html#set-theory",
    "title": "Appendix D — List of Symbols",
    "section": "Set theory",
    "text": "Set theory\n\n\n\nName\nSymbol\nMeaning\n\n\n\n\n\n\\(\\in\\)\nIs an element of\n\n\n\n\\(\\subseteq\\)\nIs a subset of (and can be equal to)\n\n\n\n\\(\\cup\\)\nUnion (“or”)\n\n\n\n\\(\\cap\\)\nIntersection (“and”)\n\n\n\n\\(\\mathcal{A}^c\\)\nA-complement (“everything but A”)\n\n\nnull\n\\(\\emptyset\\)\nThe empty set (or null set)\n\n\nOmega\n\\(\\Omega\\)\nUniversal set (all possible outcomes)\n\n\n\n\\(\\mathbb{R}\\)\nThe real numbers\n\n\n\n\\(\\mathbb{Z}\\)\nThe integers\n\n\n\n\\(\\mathbb{Z}^+,\\mathbb{Z}^-\\)\nThe positive or negative integers\n\n\n\n\\(\\mathbb{N}\\)\nThe natural numbers (0, 1, 2, …)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>List of Symbols</span>"
    ]
  },
  {
    "objectID": "listofsymbols.html#other-mathematical-symbols",
    "href": "listofsymbols.html#other-mathematical-symbols",
    "title": "Appendix D — List of Symbols",
    "section": "Other mathematical symbols",
    "text": "Other mathematical symbols\n\n\n\nName\nSymbol\nMeaning\n\n\n\n\nnabla\n\\(\\nabla\\)\nGradient (vector of first derivatives)\n\n\nfactorial\n\\(!\\)\nFactorial operator, e.g. 5! = 5 x 4 x 3 x 2 x 1 = 120\n\n\n\n\\(\\lfloor\\;\\rfloor\\)\nFloor operator, e.g. \\(\\lfloor3.14159\\rfloor=3\\)\n\n\n\n\\(\\sim\\)\nIs distributed according to, or depends upon\n\n\n\n\\(\\approx\\)\nIs approximately equal to\n\n\n\n\\(\\ll,\\gg\\)\nIs much less than, is much greater than\n\n\n\n\\(\\perp\\)\nPerpendicular or orthogonal to, independent of\n\n\ndot\n\\(\\circ\\)\nDot product (a vector operator)\n\n\n\n\\(|x|\\)\nAbsolute value of a scalar x\n\n\n\n\\(||\\boldsymbol{x}||\\)\nMagnitude (length) of a vector x\n\n\n\n\\(F^{-1},\\mathbf{A}^{-1}\\)\nInverse of a function F or matrix A\n\n\n\n\\(\\boldsymbol{x}^T,\\mathbf{A}^T\\)\nTranspose of a vector x or matrix A\n\n\n\n\\(1\\!\\!1\\)\nIndicator function, or a vector of ones\n\n\n\n\\(\\mathcal{L}\\)\nLikelihood\n\n\n\n\\(\\mathcal{l}\\)\nLog-likelihood",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>List of Symbols</span>"
    ]
  },
  {
    "objectID": "dist_bernoulli.html",
    "href": "dist_bernoulli.html",
    "title": "Appendix A — Bernoulli distribution",
    "section": "",
    "text": "Assumptions\nThe Bernoulli distribution is one of the simplest distributions, used primarily as a “building block” to define more complicated distributions such as the Binomial. However, it’s worth treating on its own.\nLet \\(X\\) be a discrete random variable that can only take on values of 0 or 1. \\(X\\) will be a Bernoulli variable with parameter \\(p\\) if and only if,\nThat’s it! A Bernoulli variable is a simple 1/0 switch with fixed probability of being 1. A coin flip is a Bernoulli variable, including flips of a weighted coin. Any single attempt with two outcomes (success or failure, life or death, profit or loss) is a Bernoulli variable.",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Bernoulli distribution</span>"
    ]
  },
  {
    "objectID": "dist_bernoulli.html#assumptions",
    "href": "dist_bernoulli.html#assumptions",
    "title": "Appendix A — Bernoulli distribution",
    "section": "",
    "text": "\\(P(X=1) = p\\)\n\\(P(X=0) = 1 - p\\)1",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Bernoulli distribution</span>"
    ]
  },
  {
    "objectID": "dist_bernoulli.html#definition",
    "href": "dist_bernoulli.html#definition",
    "title": "Appendix A — Bernoulli distribution",
    "section": "Definition",
    "text": "Definition\n\\[\\begin{array}{ll}\n  \\text{Support:} & \\{0,1\\} \\\\\n  \\text{Parameter(s):} & p,\\text{ the probability of success }(p \\in [0,1]) \\\\\n  \\text{PMF:} & P(X=k) = \\left\\{\\begin{array}{cl} 1-p, & \\quad k=0 \\\\ p, & \\quad k=1 \\end{array}\\right\\} \\\\\n  \\text{CDF:} & F_X(x) = \\left\\{\\begin{array}{cl} 0, & \\quad x \\lt 0 \\\\ 1-p, & \\quad 0 \\le x \\lt 1 \\\\ 1, & \\quad x \\ge 1 \\end{array}\\right\\} \\\\\n  \\text{Mean:} & \\mathbb{E}[X]=p \\\\\n  \\text{Variance:} & \\mathbb{V}[X]=p(1-p) \\\\\n\\end{array}\\]",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Bernoulli distribution</span>"
    ]
  },
  {
    "objectID": "dist_bernoulli.html#visualizer",
    "href": "dist_bernoulli.html#visualizer",
    "title": "Appendix A — Bernoulli distribution",
    "section": "Visualizer",
    "text": "Visualizer\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n      tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Bernoulli distribution PMF\",\n  fluidRow(plotOutput(\"distPlot\")),\n  fluidRow(sliderInput(\"p\", \"Probability (p)\", min=0, max=1, step=0.01, value=0)))\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    plot(x=c(0,1),y=c(1-input$p,input$p),main=NULL,\n         xlim=c(0,1),ylim=c(0,1),\n         xlab='x',ylab='Probability',type='h',lwd=3)})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Bernoulli distribution</span>"
    ]
  },
  {
    "objectID": "dist_bernoulli.html#relations-to-other-distributions",
    "href": "dist_bernoulli.html#relations-to-other-distributions",
    "title": "Appendix A — Bernoulli distribution",
    "section": "Relations to other distributions",
    "text": "Relations to other distributions\n\nThe sum of \\(n\\) identically and independently-distributed Binomial variates is distributed Binomial(\\(n,p\\)).\n\nAs a corollary, for \\(n=1\\), a Binomial variable is simply a Bernoulli variable.\n\nThe number of identical and independently-distributed Bernoulli trials required until the first success (\\(X=1\\)) is a Geometric variable with the same parameter \\(p\\).\nThe number of identical and independently-distributed Bernoulli trials required until the \\(r\\)th success is Negative Binomial-distributed with parameters \\(r\\) and \\(p\\).",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Bernoulli distribution</span>"
    ]
  },
  {
    "objectID": "dist_bernoulli.html#footnotes",
    "href": "dist_bernoulli.html#footnotes",
    "title": "Appendix A — Bernoulli distribution",
    "section": "",
    "text": "Sometimes statisticians use the convention \\(q = 1 - p\\).↩︎",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Bernoulli distribution</span>"
    ]
  },
  {
    "objectID": "proofs.html#proofs-about-the-average-fitted-value-and-the-average-residual",
    "href": "proofs.html#proofs-about-the-average-fitted-value-and-the-average-residual",
    "title": "Appendix C — Proofs",
    "section": "Proofs about the average fitted value and the average residual",
    "text": "Proofs about the average fitted value and the average residual\nThese proofs were referenced in Variance decomposition, and rely upon the findings above. The case for simple regression is easily proven; the multiple regression version requires some familiarity with linear algebra.\n\nSimple regression case: To claim that the average fitted value \\(\\hat{\\boldsymbol{y}}\\) is equal to the average response value \\(\\boldsymbol{y}\\) we can note that:\n\n\\[\\begin{aligned} \\frac{1}{n}\\sum_i \\hat{y}_i &= \\frac{1}{n}\\sum_i (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i) \\\\ &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\frac{1}{n}\\sum_i x_i \\\\ &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}_i \\\\ &= (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) + \\hat{\\beta}_1 \\bar{x}_i = \\bar{y}\\end{aligned}\\]\nAnd since \\(e_i = y_i - \\hat{y}_i\\), we can show that the average residual must be 0:\n\\[\\frac{1}{n}\\sum_i e_i = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i) = \\bar{y} - \\frac{1}{n}\\sum_i \\hat{y}_i = \\bar{y} - \\bar{y} = 0\\]\nI would be happy to supply a proof for the more general multiple regression case upon student request.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Proofs</span>"
    ]
  },
  {
    "objectID": "dist_binomial.html",
    "href": "dist_binomial.html",
    "title": "Appendix B — Binomial distribution",
    "section": "",
    "text": "Assumptions\nThe Binomial distribution counts the number of successes in a fixed series of Bernoulli trials.\nLet \\(B_1, \\ldots, B_n\\) be a series of \\(n\\) Bernoulli variables each identically and independently distributed with parameter \\(p\\). Define their sum as,\n\\[X =\\sum_{i=1}^n B_n\\]\nThen \\(X \\sim \\mathrm{Binomial}(n,p)\\).\nTwo key premises of the Binomial distribution are that (i) \\(n\\), the number of trials, is fixed beforehand,1 and (ii) the probability of success never changes over time or in response to the previous trials.",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "dist_binomial.html#definition",
    "href": "dist_binomial.html#definition",
    "title": "Appendix B — Binomial distribution",
    "section": "Definition",
    "text": "Definition\n\\[\\begin{array}{ll}\n  \\text{Support:} & \\{0,1,2,\\ldots,n\\} \\\\\n  \\text{Parameter(s):} & p,\\text{ the probability of success }(p \\in [0,1]) \\\\ & n,\\text{ the number of trials }(n \\in \\mathbb{Z}^+) \\\\\n  \\text{PMF:} & P(X=k) = \\left(\\begin{array}{c} n \\\\ k \\end{array} \\right) p^k (1-p)^{n-k} \\\\\n  \\text{CDF:} & F_X(x) = \\left\\{\\begin{array}{cl} 0, & \\quad x \\lt 0 \\\\ \\sum_{i=0}^{\\lfloor x \\rfloor} \\left(\\begin{array}{c} n \\\\ i \\end{array}\\right) p^i (1-p)^{n-i}, & \\quad 0 \\le x \\lt n \\\\ 1, & \\quad x \\ge n \\end{array}\\right\\} \\\\\n  \\text{Mean:} & \\mathbb{E}[X] = np \\\\\n  \\text{Variance:} & \\mathbb{V}[X] = np(1-p) \\\\\n\\end{array}\\]",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "dist_binomial.html#visualizer",
    "href": "dist_binomial.html#visualizer",
    "title": "Appendix B — Binomial distribution",
    "section": "Visualizer",
    "text": "Visualizer\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n      tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Bernoulli distribution PMF\",\n  fluidRow(plotOutput(\"distPlot\")),\n  fluidRow(column(width=6,sliderInput(\"n\", \"Trials (n)\", min=5, max=25, step=1, value=15)),\n           column(width=6,sliderInput(\"p\", \"Probability (p)\", min=0, max=1, step=0.01,value=0.5))))\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    plot(x=0:input$n,y=dbinom(x=0:input$n,input$n,input$p),main=NULL,\n         xlab='x (Successes)',ylab='Probability',type='h',lwd=3)})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "dist_binomial.html#relations-to-other-distributions",
    "href": "dist_binomial.html#relations-to-other-distributions",
    "title": "Appendix B — Binomial distribution",
    "section": "Relations to other distributions",
    "text": "Relations to other distributions\n\nA Binomial(\\(n,p\\)) variable is equivalent to the sum of \\(n\\) i.i.d. Bernoulli(\\(p\\)) variables\n\nAs a corollary, when \\(n=1\\), a Binomial(\\(n,p\\)) variable is simply a Bernoulli(\\(p\\)) variable.\n\nThe Binomial distribution forms a discrete approximation to the Normal distribution with \\(\\mu = np\\) and \\(\\sigma^2 = np(1-p)\\), and the quality of this approximation grows when \\(np\\) is very large and \\(p\\) is not near 0 or 1. The accuracy of this approximation increases with the use of a “continuity correction”, such as adding or subtracting 0.5 to the Normal quantile. For example, compare:\n\n\npbinom(q=155,size=200,prob=0.8)\n\n[1] 0.2112617\n\npnorm(q=155,mean=200*0.8,sd=sqrt(200*0.8*0.2))\n\n[1] 0.1883796\n\npnorm(q=155.5,mean=200*0.8,sd=sqrt(200*0.8*0.2))\n\n[1] 0.2131628\n\n\n\nThe Poisson distribution can be seen as a limit case of the Binomial distribution, as \\(n\\) grows toward infinity and \\(p\\) shrinks to zero, with \\(\\lambda = np\\). When the usual counts are nowhere near the theoretical upper bound, we can often use the Poisson instead of the Binomial.\nFor example, say that a town has 20,000 homes, and that any home has a 0.1% chance to have a termite infestation. The current number of termite infestations is technically a Binomial(20000,0.001) variable, but well-approximated by a Poisson(20) variable:\n\n\ndbinom(x=6:10,size=20000,prob=0.001)\n\n[1] 0.0001823448 0.0005213501 0.0013042233 0.0029000148 0.0058032228\n\ndpois(x=6:10,lambda=20)\n\n[1] 0.0001832137 0.0005234676 0.0013086690 0.0029081533 0.0058163065",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "dist_binomial.html#footnotes",
    "href": "dist_binomial.html#footnotes",
    "title": "Appendix B — Binomial distribution",
    "section": "",
    "text": "Unlike, say, the Geometric distribution in which trials are continued until the first success occurs.↩︎",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Binomial distribution</span>"
    ]
  },
  {
    "objectID": "dist_poisson.html",
    "href": "dist_poisson.html",
    "title": "Appendix F — Poisson distribution",
    "section": "",
    "text": "Assumptions\nThe Poisson distribution describes the number of events that occur during a fixed observation window of a Poisson process. These processes often occur when there are many independent chances of an event happening, yet the probability of any one is very low:\nA Poisson process is a type of number-generating process in which “events” of some kind happen irregularly, but with a long-term average rate.\nPoisson processes are most often observed over time (such as the number of large meteor strikes observed over 500 years), but they can also be observed over space (such as the number of craters observed across 500 square kilometers of the moon’s surface), or combinations of the two, or even more exotic concepts (such as observations across the electromagnetic spectrum). In these notes, we will continue to assume the observation windows are time-based.\nThe formal definition of a Poisson process requires some time to unpack. For now, let’s say the following:\nThen we say that \\(N\\) is a Poisson process, that \\(P(N(t) \\le k)\\) has a Poisson CDF with rate \\(\\lambda\\), and that if \\(T\\) is the arrival time of the next event (i.e., \\(T = \\min(t: \\,N(t)=1)\\)) then \\(P(T \\le t)\\) has an exponential distribtuion with rate \\(\\lambda\\).\nA key concept to the Poisson and the Exponential distributions is that they describe the same counting process. The Poisson (discrete) counts the number of events for a fixed window of time, and the Exponential (continuous) counts the time elapsed until the next event.",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Poisson distribution</span>"
    ]
  },
  {
    "objectID": "dist_poisson.html#assumptions",
    "href": "dist_poisson.html#assumptions",
    "title": "Appendix F — Poisson distribution",
    "section": "",
    "text": "Let \\(N(t)\\) be a counting process, that is, a non-negative, integer-valued, non-decreasing function with the property that for all \\(t \\ge s\\) we can say that \\(N(t) - N(s)\\) represents the number of events which happen in the window \\([s,t]\\)\nLet \\(\\lambda\\) represent the long-term average rate of events which occur per unit of observation: \\(\\mathbb{E}[N(1)] = \\lambda\\) and \\(\\mathbb{E}[N(t)] = t\\lambda\\)\n\nAs a corollary, notice that \\(N(0) = 0\\), meaning that events must always arrive one at a time, and never two or more simultaneously\n\nLet the event counts in mutually exclusive observation windows be independent of each other, which we could write several ways, e.g. for all \\(t \\ge s\\), then \\(N(t - s) \\perp N(s)\\)",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Poisson distribution</span>"
    ]
  },
  {
    "objectID": "dist_poisson.html#definition",
    "href": "dist_poisson.html#definition",
    "title": "Appendix F — Poisson distribution",
    "section": "Definition",
    "text": "Definition\n\\[\\begin{array}{ll}\n  \\text{Support:} & \\mathbb{R}^+ \\\\\n  \\text{Parameter(s):} & \\lambda,\\text{ the rate in events per unit of time }(\\lambda \\gt 0) \\\\\n  \\text{PMF:} & f_X(x) = \\lambda e^{-\\lambda x} \\\\\n  \\text{CDF:} & F_X(x) = \\left\\{\\begin{array}{cl} 0, & \\quad x \\lt 0 \\\\ 1 - \\lambda e^{-\\lambda x}, & \\quad x \\ge 0 \\end{array}\\right\\} \\\\\n  \\text{Mean:} & \\mathbb{E}[X] = \\frac{1}{\\lambda} \\\\\n  \\text{Variance:} & \\mathbb{V}[X] = \\frac{1}{\\lambda^2} \\\\\n\\end{array}\\]",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Poisson distribution</span>"
    ]
  },
  {
    "objectID": "dist_poisson.html#visualizer",
    "href": "dist_poisson.html#visualizer",
    "title": "Appendix F — Poisson distribution",
    "section": "Visualizer",
    "text": "Visualizer\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n      tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Poisson distribution PMF\",\n  fluidRow(plotOutput(\"distPlot\")),\n  fluidRow(sliderInput(\"lambda\", \"Arrival rate (lambda)\", min=0.2, max=10, step=0.2,value=2))))\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    plot(x=0:input$n,y=dpois(x=0:(4*input$lambda),input$lambda),main=NULL,\n         xlab='x (Count of events)',ylab='Probability',type='h',lwd=3)})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Poisson distribution</span>"
    ]
  },
  {
    "objectID": "dist_poisson.html#relations-to-other-distributions",
    "href": "dist_poisson.html#relations-to-other-distributions",
    "title": "Appendix F — Poisson distribution",
    "section": "Relations to other distributions",
    "text": "Relations to other distributions\n\nThe Poisson distribution can be seen as a limit case of the Binomial distribution, as \\(n\\) grows toward infinity and \\(p\\) shrinks to zero, with \\(\\lambda = np\\). When the usual counts are nowhere near the theoretical upper bound, we can often use the Poisson instead of the Binomial.\nFor example, say that a town has 20,000 homes, and that any home has a 0.1% chance to have a termite infestation. The current number of termite infestations is technically a Binomial(20000,0.001) variable, but well-approximated by a Poisson(20) variable:\n\n\ndbinom(x=6:10,size=20000,prob=0.001)\n\n[1] 0.0001823448 0.0005213501 0.0013042233 0.0029000148 0.0058032228\n\ndpois(x=6:10,lambda=20)\n\n[1] 0.0001832137 0.0005234676 0.0013086690 0.0029081533 0.0058163065",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Poisson distribution</span>"
    ]
  },
  {
    "objectID": "heteroskedasticity.html#categories-and-illustrations-of-heteroskedasticity",
    "href": "heteroskedasticity.html#categories-and-illustrations-of-heteroskedasticity",
    "title": "Heteroskedasticity",
    "section": "",
    "text": "Regime change. In a regime change, also called a structural break, the parameters of a model observed over time suddenly shift to new values, often as a result of a strong system shock. This can affect both the betas \\(\\boldsymbol{\\beta}\\) and the error variance \\(\\sigma^2\\). For example, a company’s monthly electrical bills might be dependent on how much GPU processing time they use, but the errors might become permanently less volatile after they install solar battery storage systems.\n\n\n* *Mixed distributions.*  In a regime change, the break point clearly separates the data and the distributions into a \"before\" and \"after\" period. But in other datasets, we see two (or more) error distributions which are mixed together such that each new observation can come from either (or any) error source. For example, a company’s stock might have one error variance on slow news days and a larger error variance on days where the market learns important information about the company.\n\n* *Unconditional heteroskedasticity.*  Error variances can grow and shrink in proportion to an unobserved variable, particularly in time series data.  For example, speculative assets such as cryptocurrencies might enter periods of high and low volatility over the span of weeks (if examining daily data) or even seconds/minutes (if examining intraday data), and these differences in the error distribution would present as a smooth continuüm rather than a clean structural break.\n\nConditional heteroskedasticity.  Sometimes, the predictors themselves not only predict Y but also predict the size of the error variance σ^2.  One common case would be datasets in which the errors are proportional, that is multiplicative and not additive.  For example, the value of a diamond is usually based not just on the size of the stone, but on metrics such as the color, the brilliancy of the cut, and the lack of visible flaws.  A small 0.5 carat diamond might be worth $400 ± $100 depending on the other factors, while a larger 2.0 carat diamond might be worth $2000 ± $500.  The qualitative factors adjust the base value on a proportional basis, creating larger errors for larger stones.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Heteroskedasticity</span>"
    ]
  },
  {
    "objectID": "heteroskedasticity.html#consequences-of-heteroskedasticity",
    "href": "heteroskedasticity.html#consequences-of-heteroskedasticity",
    "title": "Heteroskedasticity",
    "section": "Consequences of heteroskedasticity",
    "text": "Consequences of heteroskedasticity\nThe chief danger of using OLS regression in the presence of heteroskedasticity is simply that we will mislead ourselves about the error variance.\n* We may be more confident or less confident than we should be that our betas are statistically significant and belong in the model\n\n* All our confidence intervals (for predictions, mean responses, and the true values of the betas) will be narrower or wider than they should be\n\n* We may view the errors as arithmetic/additive when they are really geometric/multiplicative\nThese are important hazards, but they need not be fatal. Heteroskedastic data will not bias our betas (just their standard errors), so we may remain reasonably confident in our point estimates, so long as we see no regime changes.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Heteroskedasticity</span>"
    ]
  },
  {
    "objectID": "heteroskedasticity.html#identifying-heteroskedasticity",
    "href": "heteroskedasticity.html#identifying-heteroskedasticity",
    "title": "Heteroskedasticity",
    "section": "Identifying heteroskedasticity",
    "text": "Identifying heteroskedasticity\nThere are usually two ways to identify violations of your modeling assumptions: (1) finding the right plots to highlight any potential problems, and (2) hypothesis tests specifically built to test the assumption in question. These two groups of methods pair well with each other, complementing the strengths and weaknesses of the other:\n* Hypothesis tests are best at noticing subtle, aggregate trends. The human eye cannot easily tell when the errors are slightly heteroskedastic, but with even a moderate sample size a test can confirm such findings without a doubt. \n\n* The human eye is best at noticing the presence of outliers and meaningful patterns. Tests aggregate the entire dataset into a single number without telling you why the data scored well or poorly. The human eye will notice individual data points that tell a story, or speak to unmodeled features in the data.\n\nDiagnostic plots\nThere are several plots which help to reveal heteroskedasticity, and two of them were shown in the figures above:\n\nSimply plotting the residuals in order might reveal structural breaks or periods of unconditional volatility\nA histogram or density of the residuals might suggest the presence of a mixed distribution\nPlotting the residuals as a function of the fitted values (or as a function of the predictor in simple regression) is the classic way to reveal conditional heteroskedasticity. Look for a “trumpet” shape to the residuals: a flared or triangular pattern.\n\n\n\nHypothesis tests\nThere are many tests which offer evidence on heteroskedasticity; I will provide details on one such test, and sketch out another.\nThe Breusch-Pagan test (1979) takes a very straightforward, two-stage approach to identifying heteroskedasticity associated with the predictors. (This test does not detect other types of heteroskedasticity!) First, we run the multiple regression model as usual, producing a set of residuals and a maximum likelihood estimate of the error variance:\n\\[\\begin{aligned} \\mathrm{M1:} \\quad y_i &= \\boldsymbol{x}_i \\boldsymbol{\\beta} + \\varepsilon_i \\\\ e_i &= y_i - \\hat{y}_i = y_i - \\boldsymbol{x}_i \\hat{\\boldsymbol{\\beta}} \\\\ \\hat{\\sigma}^2_{ML} &= \\sum_i e_i^2 / n \\end{aligned}\\]\nThen we run a second-stage regression to see if the error sizes (represented by the squared, normalized residuals) can be meaningfully predicted by the original predictors.2 If we write the second regression equation using alpha \\((\\alpha)\\) for the slopes and delta \\((\\delta)\\) for the error term, it would be:\n\\[\\mathrm{M2:} \\quad \\frac{e_i^2}{\\hat{\\sigma}^2_{ML}} = \\boldsymbol{x}_i \\boldsymbol{\\alpha} + \\delta_i\\]\nThe Breusch-Pagan test uses likelihood-based methods to show that half the model sum-of-squares (SSM) of the second-stage model is distributed chi-squared, with degrees of freedom equal to the predictors in the second-stage model (\\(k\\)), under the null assumption of no heteroskedasticity.\n\\[\\frac{1}{2}\\mathrm{SSM}_\\mathrm{M2} \\sim \\chi^2_{k}\\]\nThe Breusch-Pagan test only examines heteroskedasticity which increases (or decreases) linearly with the predictors. It’s simple to implement, and available in all common statistical software, but may not catch more subtle forms of heteroskedasticity.\nA related, slightly more comprehensive test is White’s test (1980). White’s test includes both all the original regression predictors, and their squares, and their cross-products in the second-stage regression. In this way, White’s test can measure both non-linear heteroskedasticity as well as possible misspecification of the model (i.e. cases where adding an additional predictor would help the model match OLS regression assumptions). Software packages can usually implement White’s test as well.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Heteroskedasticity</span>"
    ]
  },
  {
    "objectID": "heteroskedasticity.html#correcting-or-mitigating-heteroskedasticity",
    "href": "heteroskedasticity.html#correcting-or-mitigating-heteroskedasticity",
    "title": "Heteroskedasticity",
    "section": "Correcting or mitigating heteroskedasticity",
    "text": "Correcting or mitigating heteroskedasticity\nThe right approach to mitigating heteroskedasticity depends heavily on what type of heteroskedasticity has been observed.\n\nIn the presence of a structural break, the easiest solution is to model the “before” and “after” periods separately (or drop one of them entirely), as these will now presumably show homoskedastic errors. In cases where a single model is necessary (either because of a phenomenon bridging both periods, or simply to preserve as large a sample as possible), more advanced modeling such as WLS or fGLS methods will be needed, both of which are beyond the scope of this document.\nIn the presence of a mixed distribution, the solutions are more difficult. If the two distributions can be easily disentangled, perhaps through a feature already in the data, then the subsamples can be modeled separately. But if not, we may have to accept some degree of misspecification. Standard errors and confidence intervals will be less reliable.\nIn the presence of heteroskedasticity not associated with the predictors, if we believe the heteroskedasticity to have time-series properties, then ARCH (auto-regressive conditionally heteroskedastic) and GARCH (generalized ARCH) models are a popular remedy. When the volatility is not time-based, we may need to consider WLS and fGLS models, including the use of Huber-White errors, sometimes called heteroskedasticity-consistent errors. All of these remedies are beyond the scope of this document.\nIn the presence of heteroskedasticity associated with the predictors, we may choose between both simple and complex solutions. In addition to the advanced WLS and fGLS methods mentioned above, we can often use a simple transformation to significantly align our residuals with the OLS assumptions. For example, if our errors are proportional rather than additive, then a log-transformation of the response will lead to a suitable OLS target.\n\nIn practice, we may wish to try a few different ways of controlling the variance. Log-transformations and square root-transformations are the most common, but we may also look at reciprocals (modeling \\(1⁄\\boldsymbol{y}\\) instead of \\(\\boldsymbol{y}\\)), rates, or other transformations and features.\nOnce we apply a potential solution, we can re-examine the new residuals using either the graphical methods or the hypothesis tests mentioned above. The new residuals may still fail a hypothesis test at our desired significance threshold, but we often can achieve a significant reduction in heteroskedasticity which should increase our confidence in the regression estimates, predictions, and confidence intervals.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Heteroskedasticity</span>"
    ]
  },
  {
    "objectID": "heteroskedasticity.html#visualizer",
    "href": "heteroskedasticity.html#visualizer",
    "title": "Heteroskedasticity",
    "section": "Visualizer",
    "text": "Visualizer\nWhile heteroskedasticity doesn’t necessarily bias the estimated betas, it can still pose severe challenges for regression. The data below show conditional heteroskedasticity: larger predictors are linked to larger errors (both positive and negative). Consider how accurate or inaccurate the stated “80% prediction interval” would be at different locations:\n\nFor small predictor values, the 80% prediction interval becomes too conservative, i.e. wider than needed\nFor large predictor values, much more than 20% of new data points would be expected to fall outside the 80% prediction interval\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 960\n\nlibrary(shiny)\nlibrary(bslib)\n\nset.seed(1628)\nhetdat &lt;- data.frame(X=runif(200,min=0,max=10))\nhetdat$Y &lt;- 10 + 1.5*hetX + 0.5*hetX*rnorm(200)\nhetLM &lt;- lm(Y~X,hetdat)\nhetint &lt;- predict(hetLM,newdata=data.frame(X=seq(0,10,0.1)),\n                  interval='prediction',level=0.8)\n  \nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"80% CIs for predicted new observations\",\n  fluidRow(sliderInput(\"x\", \"x-level of new observations\", min=0, max=10, value=5, step=0.1)),\n  fluidRow(plotOutput(\"distPlot1\")))\n\nserver &lt;- function(input, output) {\n  index &lt;- reactive({input$x*10+1})\n  output$distPlot1 &lt;- renderPlot({\n    plot(NA,NA,xlab='X',ylab='Y',xlim=c(-0.1,10.1),ylim=c(0,40))\n    rect(xleft=input$x-0.1,ybottom=hetint[index(),2],\n         xright=input$x+0.1,ytop=hetint[index(),3],\n         border=NA,density=NA,col='#0000ff50')\n    points(hetdat$X,hetdat$Y,)})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Heteroskedasticity</span>"
    ]
  },
  {
    "objectID": "heteroskedasticity.html#footnotes",
    "href": "heteroskedasticity.html#footnotes",
    "title": "Heteroskedasticity",
    "section": "",
    "text": "You will also see the spellings “homoscedasticity” and “heteroscedasticity” (with a ‘c’ instead of a ‘k’). It’s a matter of transliteration, without a clear right or wrong choice.↩︎\nIn fact, we need not use the original predictors. If you can hypothesize a functional form for your heteroskedasticity, you can use whichever variables you like in the second-stage regression.↩︎",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Heteroskedasticity</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html",
    "href": "autocorrelation.html",
    "title": "Autocorrelation",
    "section": "",
    "text": "Categories and illustrations of serial correlation\nOLS regression assumes that our errors are all independent and thus uncorrelated with each other, i.e.\n\\[\\mathbb{E}[\\varepsilon_i \\cdot \\varepsilon_j] = 0 \\quad \\forall i,j:i \\ne j\\]\nThis assumption is often presented as demanding complete independence between the errors, and from time to time I will use the concept of independence as a convenient shorthand, but we may relax this assumption to simply require the errors be uncorrelated, and all of our useful OLS results will still hold.\nWhen the errors are correlated, we say that the data show serial correlation, which is also called autocorrelation.\nMost of the literature on serial correlation describes data with an obvious time series nature, such as daily market prices or hourly transaction totals. However, many datasets will show serial correlation even if they are completely cross-sectional, that is, gathered all at the same time:\nYou will note that the serial correlation in these examples has no real time series component but results instead from how we order the observations in our sample. As we shall see, this type of serial correlation does not always pose a problem for our analyses.\nThe more classic type of serial correlation results from data gathered regularly across a time interval, such as daily stock market prices. Using \\(t\\) to subscript the data instead of our customary \\(i\\), consider an error process which is itself a combination of the previous observation and a separate IID-normal error term:\n\\[\\varepsilon_t = \\rho \\varepsilon_{t-1} + u_t, \\quad |\\rho| \\le 1, u_t \\sim \\mathrm{Normal}(0,\\sigma^2)\\]\nNow imagine these correlated errors become the errors of a multiple regression model:\n\\[y_t = \\boldsymbol{x}_t \\boldsymbol{\\beta} + \\varepsilon_t\\]\nTogether, these two models become a regression with an “AR1” (first-order autocorrelation) error process, and when the model is estimated from a sample of data, we would expect to find serial correlation among the residuals, which are estimators for \\(\\varepsilon_t\\). When the correlation parameter \\(\\rho\\) is large and positive, the residuals resemble a random walk; when \\(\\rho\\) is large and negative, the residuals will swing wildly from one residual to the next:\nCode\nset.seed(1860)\npar(mfrow=c(1,3))\nplot(arima.sim(model=list(ar=0.8),n=50),\n     main=expression(paste('Autocorrelation of ',rho == 0.8)),\n     ylim=c(-3,3),ylab='Residual')\nabline(h=0,lwd=2,lty=2,col='grey50')\nplot(arima.sim(model=list(ar=0.0),n=50),\n     ylim=c(-3,3),main='No autocorrelation',ylab='Residual')\n\n\nWarning in min(Mod(polyroot(c(1, -model$ar)))): no non-missing arguments to\nmin; returning Inf\n\n\nCode\nabline(h=0,lwd=2,lty=2,col='grey50')\nplot(arima.sim(model=list(ar=-0.8),n=50),\n     main=expression(paste('Autocorrelation of ',rho == -0.8)),\n     ylim=c(-3,3),ylab='Residual')\nabline(h=0,lwd=2,lty=2,col='grey50')\n\n\n\n\n\nIllustrations of positive and negative autocorrelation\nNotice that each residual, if uncorrelated, should have an independent 50-50 chance of being either positive or negative. The probability of observing roughly 25 positive residuals in a row (in the leftmost plot above) would normally approach zero. Likewise, the probability of always observing a sign change from one residual to the next (in the rightmost plot above) should be near zero, just as it would be unusual to observe a sequence of coin flips HTHTHTHTHTHTHTH… By contrast, in the middle plot above, note that the next residual in a sequence has a roughly 50% chance of sharing the sign of the previous residual. In uncorrelated residuals, the past should not predict the present!",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#categories-and-illustrations-of-serial-correlation",
    "href": "autocorrelation.html#categories-and-illustrations-of-serial-correlation",
    "title": "Autocorrelation",
    "section": "",
    "text": "* Say that I administer a test to college students in five different classrooms. When I collate and enter the data, I will likely enter all the grades from one section, followed by a second, and a third, etc. Within each classroom, there might be unique factors (noise and light levels, teacher attributes, reasons why that time slot worked best for the students, etc.) which correlate errors within the same classroom.\n\nSay that I reorder the dataset by student ID number. However, the student ID number might be a function of when they were first entered into the school’s IT systems. This in turn would be correlated with factors such cohort/graduation year, or whether a student was accepted through early admission or accepted late from the waitlist, which may also be correlated with educational outcomes.\nSay that I reorder the dataset by student’s last name. However, last names starting with certain letters are more common in some countries than others, so once again we might observe some correlation between nearby observations, corresponding to linguistic and cultural similarities between the students.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#consequences-of-serial-correlation",
    "href": "autocorrelation.html#consequences-of-serial-correlation",
    "title": "Autocorrelation",
    "section": "Consequences of serial correlation",
    "text": "Consequences of serial correlation\nLet’s consider three separate causes of serial correlation:\n\nIn some cases, serial correlation can be ignored without much consequence. When the data aren’t actually from a time series, and the serial correlation comes instead from an arbitrary ordering of the rows, and especially if the model will not be used for out-of-sample prediction, then mild or moderate serial correlation may be disregarded. In fact, for all but the most sensitive applications, a mild amount of serial correlation (say, \\(|\\rho| \\le 0.1\\)) may be disregarded, since the regression model almost surely has larger challenges elsewhere.\nWhen the data instead shows true time series behavior, the presence of any amount of serial correlation begins to affect the model conclusions. Although “true” serial correlation does not bias the betas, it does distort many of the other model characteristics: the significance and standard errors of the betas, the estimated error variance, confidence intervals for predictions and mean responses, etc. Serial correlation also means the OLS model is no longer consistent, an estimation term meaning that the model estimators smoothly converge on the true parameters with larger and larger sample sizes.\nFinally, when the serial correlation comes from omitted variables, we might say instead that the model is misspecified and that the serial correlation is simply a symptom of the misspecification, rather than a property of the data’s generating process. In such circumstances, the estimates of the betas may no longer be reliable, as the omitted variables could easily bias the regression results!",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#identifying-serial-correlation",
    "href": "autocorrelation.html#identifying-serial-correlation",
    "title": "Autocorrelation",
    "section": "Identifying serial correlation",
    "text": "Identifying serial correlation\nAs with the discussion of Heteroskedasticity, there are two principal ways to identify serial correlation: examining plots of the data, and conducting hypothesis tests.\n\nDiagnostic plots\nA simple ordered plot of the residuals (like those above) will sometimes provide enough evidence to spark suspicion. However, the human eye is not very adept at distinguishing mild to moderate amounts of serial correlation.\nAnother way to notice autocorrelation is to plot the residuals against their own lagged values. For example, if your regression contains 100 residuals, plot the paired points \\((e_1,e_2), (e_2,e_3), \\ldots, (e_{99},e_{100})\\). In cases of strong serial correlation you will see a linear trend to the data. However, even here the human eye can easily mislead us:\n\n\nCode\nset.seed(1833)\npar(mfrow=c(1,2))\ntmp &lt;- arima.sim(model=list(ar=0.8),n=100)\nplot(tmp[1:99],tmp[2:100],\n     main=expression(paste('Autocorrelation of ',rho == 0.8)),\n     xlab='Previous Residual',ylab='Current Residual')\ntmp &lt;- arima.sim(model=list(ar=-.15),n=100)\nplot(tmp[1:99],tmp[2:100],\n     main=expression(paste('Autocorrelation of ',rho == -0.15)),\n     xlab='Previous Residual',ylab='Current Residual')\n\n\n\n\n\nPlots of the residuals vs their own lagged values\n\n\n\n\nAs you can see above, strong positive autocorrelation leaves a clear trend in the lagged residuals, while weaker (but still substantial) negative correlation shows almost no evidence of a linear trend.\nThe best way to spot serial correlation is not to stare at the raw data, but to specifically plot the correlations of the residuals with its past values. This type of plot is known as an autocorrelation function (ACF) plot, and it can reveal not just first-order correlation, but also seasonality in the data beyond the first lag:1\n\n\nCode\nset.seed(1933)\npar(mfrow=c(2,1))\ntmp &lt;- arima.sim(model=list(ar=c(0.4,-0.8),order=c(2,0,0)),n=120)\nplot(tmp,main='Residuals over time',ylab='Residual')\nplot(acf(tmp,plot=FALSE),main='ACF plot of residuals',ylab='Correlation')\n\n\n\n\n\nSimulated AR2 residuals and their ACF plot\n\n\n\n\nThe figure above shows a set of residuals with complicated serial correlation (positive first-order correlation and negative second-order correlation). The ACF plot highlights that several sets of the lagged residuals show a stronger correlation with the current residuals than would be predicted by chance alone.\n\n\nHypothesis tests\nAs mentioned earlier, plots are helpful but the human eye can easily fail us or lead to subjective differences in opinion between different analysts. Hypothesis tests can be a reliable partner to diagnostic plots and vice versa.\nTwo widely used tests for serial correlation are the Durbin-Watson test and the Ljung-Box test. Both\n&lt;TO BE CONTINUED?&gt;",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#correcting-or-mitigating-serial-correlation",
    "href": "autocorrelation.html#correcting-or-mitigating-serial-correlation",
    "title": "Autocorrelation",
    "section": "Correcting or mitigating serial correlation",
    "text": "Correcting or mitigating serial correlation\nAs with Heteroskedasticity, the appropriate correction for serially correlated residuals will depend upon why they are serially correlated.\n\nTrue time series behavior should usually be addressed with an explicit time series model, which is beyond the scope of this course (though XARIMA models and dynamic regression models with AR errors are two natural extensions of the models presented here.)\nAutocorrelated errors caused by omitted variables are best remedied either by finding those variables (or their proxies), or by using clustered standard errors. The Newey-West estimator is a procedure for approximating the variance-covariance matrix of the betas in the presence of heteroskedastic and serially-correlated data, using weighted least squares (WLS). It generalizes the Huber-White standard errors mentioned in Heteroskedasticity.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#visualizer",
    "href": "autocorrelation.html#visualizer",
    "title": "Autocorrelation",
    "section": "Visualizer",
    "text": "Visualizer\nWhile serial correlation doesn’t necessarily bias the estimated betas, it can still pose challenges for regression. Adjust the first-order error correlation between -1 and +1 and see how the estimated betas and predicted new values start to become untrustworthy:\n\nWith positive autocorrelation, the estimated intercept becomes less reliable\nWith negative autocorrelation, the estimated slope becomes less reliable\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 960\n\nlibrary(shiny)\nlibrary(bslib)\n\nset.seed(1621)\narX &lt;- runif(60,min=0,max=10)\n  \nui &lt;- page_fluid(\n  tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"100 Estimates of the true relationship, N = 60 obs\",\n  fluidRow(sliderInput(\"rho\", \"Error correlation (rho)\", min=-0.99, max=0.99, value=0.2, step=0.01)),\n  fluidRow(plotOutput(\"distPlot1\")))\n\nserver &lt;- function(input, output) {\n  arE &lt;- reactive({})\n  arY &lt;- reactive({replicate(n=100,expr={30 - 2*arX + 3*arima.sim(list(ar=input$rho),n=60)})})\n  output$distPlot1 &lt;- renderPlot({\n    plot(NA,NA,xlab='X',ylab='Y',xlim=c(0,10),ylim=c(0,40))\n    apply(arY(),2,function(z) abline(reg=lm(z~arX),lwd=2,col='#000000'))\n    abline(30,-2,lwd=3,col='#0000ff')\n    legend(x='topright',legend=c('True betas','Estimated betas'),\n           col=c('#0000ff','#000000'),lwd=c(3,2),cex=0.8)})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#footnotes",
    "href": "autocorrelation.html#footnotes",
    "title": "Autocorrelation",
    "section": "",
    "text": "Another plot, the partial autocorrelation function (or PACF), measures a different type of time series behavior. The full interpretation of these functions and plots must be left to a dedicated time series course.↩︎",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Autocorrelation</span>"
    ]
  },
  {
    "objectID": "nonnormality.html",
    "href": "nonnormality.html",
    "title": "Non-normal errors",
    "section": "",
    "text": "Categories and illustrations of non-normal errors\nOur prior work in univariate estimation will serve us well, here. We have already built the understanding and toolkit to examine the sample distribution of the residuals and test them against candidate distributions including the normal distribution.\nIn truth, the residuals are rarely normally distributed. With small or moderate sample sizes, we may fail to reject the null hypothesis that the residuals are normal, but these “good” results are more often due to low statistical power than to precise conformity to the OLS assumptions.\nReaders of this section can use the techniques discussed in #sec-identification to spot Poisson errors, uniform errors, etc. One less-familiar distribution is the pathological Cauchy distribution, which is roughly bell-shaped yet has no mean or variance. The “tails” of the Cauchy distribution are so thick that extreme values of literally any magnitude are guaranteed to happen with sufficient sample size. These huge outliers immediately change the sample characteristics so strongly that the sample will never consistently estimate a single mean or variance.\nCode\nset.seed(2007)\npar(mfrow=c(2,1))\ntmp &lt;- rcauchy(n=500)\nhist(tmp,breaks=seq(-500,500,0.2),xlim=c(-3,3),\n     main='Histogram of Cauchy residuals',xlab='Residual')\nplot(tmp,main='Ordered plot of Cauchy residuals',ylab='Residual')\n\n\n\n\n\nHistogram and ordered plot of simulated Cauchy residuals\nThe graphs above show that while the center of the Cauchy distribution resembles a wide normal distribution, with most values observed between -3 and +3, the full sample contains many values far from this middle point. In another sample from the same distribution, we could easily observe values in the 1000s.\nCode\npar(mfrow=c(2,1))\nplot(cumsum(tmp)/(1:length(tmp)),main='Cumulative Mean from Cauchy Sample',\n     xlab='Sample size',ylab='Sample mean')\nplot(cumsum(tmp^2)/((1:length(tmp))-1),main='Cumulative Variance of Cauchy Sample',\n     xlab='Sample size',ylab='Sample variance')\n\n\n\n\n\nMean and variance estimates from simulated Cauchy residuals\nThese large and relatively common extreme values pose a challenge for estimation. As you can see from the plots above, our sample estimates of mean and variance have no stability, and do not converge on any “true” parameters with increased sample size. And yet, any small subsample of this distribution might seem roughly normally distributed, and many subsamples will pass a Kolmogorov-Smirnov test of their normality.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Non-normal errors</span>"
    ]
  },
  {
    "objectID": "nonnormality.html#consequences-of-non-normal-errors",
    "href": "nonnormality.html#consequences-of-non-normal-errors",
    "title": "Non-normal errors",
    "section": "Consequences of non-normal errors",
    "text": "Consequences of non-normal errors\nA small degree of non-normality will normally not concern us. Least squares methods will work to estimate the betas even if we cannot use maximum likelihood methods, and the least squares estimators for the betas will be unbiased so long as the errors are symmetric and with finite variance. The standard errors of the regression and the confidence intervals for the parameters will remain reasonably robust.\nThere are three situations which create more serious problems for any regression analysis: * Sometimes, moderate non-normality of the residuals is a consequence of a structural break, a mixed distribution, or slowly drifting parameters. In these cases, the estimators for the betas may be biased.\n* At other times, we may be using a regression analysis as a way to describe the *tails* of a prediction, that is, we may want to know the 95th percentile of a predicted point, or the 99th percentile, etc. These tail probabilities are very sensitive to the shape of the error distribution, and even moderate non-normality will result in misleading predictions.^[The 2007--08 financial crisis was worsened when large financial institutions failed to realize that the forecasting errors for their mortgage-backed securities did not have normally-distributed tail probabilities.] \n\n* As shown above, in small or moderate samples we may falsely believe that a truly pathological error distribution is instead normal. Although this happens rarely, it can completely invalidate every aspect of our model. Models of new scenarios, trained on small datasets, should always be treated with a skepticism and wariness.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Non-normal errors</span>"
    ]
  },
  {
    "objectID": "nonnormality.html#identifying-non-normal-errors",
    "href": "nonnormality.html#identifying-non-normal-errors",
    "title": "Non-normal errors",
    "section": "Identifying non-normal errors",
    "text": "Identifying non-normal errors\nThe Kolmogorov-Smirnov test discussed in Identifying distributions is a fine starting point for testing the residuals for normality. Remember to match the error variances for your test: either (i) test your residuals against a Normal distribution with variance \\(\\hat{\\sigma}^2_{\\varepsilon}\\) or (ii) test your normalized residuals \\(\\boldsymbol{e}/\\hat{\\sigma}^2_{\\varepsilon}\\) against the Standard Normal distribution.\nThe Shapiro-Wilk test for normality is also commonly used to assess regression residuals, and can be found in most statistical software environments. Because the Shapiro-Wilk test specifically examines normality, it’s somewhat more powerful than the Kolmogorov-Smirnov test, meaning that it’s better able to correctly reject the null hypothesis and confirm small differences between the data and our expectations. The test relies on a statistic with a novel and unnamed distribution which I will not cover here, as it must be calculated through Monte Carlo simulation, but most statistical software packages implement the test with minimal user input.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Non-normal errors</span>"
    ]
  },
  {
    "objectID": "nonnormality.html#correcting-or-mitigating-non-normal-errors",
    "href": "nonnormality.html#correcting-or-mitigating-non-normal-errors",
    "title": "Non-normal errors",
    "section": "Correcting or mitigating non-normal errors",
    "text": "Correcting or mitigating non-normal errors\nWe may not be able to easily mitigate non-normal errors without making even worse problems for ourselves.\n* If the non-normal errors are connected with the need to transform the data, then we may solve two problems at once (non-normality and non-linearity). However, if the relationship is already linear, then transforming the data to help with *non-normality* will usually result in creating new *non-linearity*.\n\n* If the non-normal errors are due to large outliers, then with careful study we may determine that some observations should be removed, or that new covariates should be introduced which help explain the outliers and reduce their residuals. However, outliers are often a genuine and legitimate part of the data generating process and should not be removed simply because they worsen our model fit.\n\n* As before, if the non-normality is due to a mixed distribution or a regime change, we can try to disentangle or separately model the different distributions or regimes.\nMore techniques for applying linear models to seemingly non-linear relationships are discussed in later sections.",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Non-normal errors</span>"
    ]
  },
  {
    "objectID": "dist_exponential.html",
    "href": "dist_exponential.html",
    "title": "Appendix C — Exponential distribution",
    "section": "",
    "text": "Assumptions\nThe Exponential distribution describes the interarrival times between events from a Poisson process. The most common examples are the time it takes to be seen by a teller at a bank, or the time spent waiting for a bus at a bus stop.1\nA Poisson process is a type of number-generating process in which “events” of some kind happen irregularly, but with a long-term average rate.\nPoisson processes are most often observed over time (such as the number of large meteor strikes observed over 500 years), but they can also be observed over space (such as the number of craters observed across 500 square kilometers of the moon’s surface), or combinations of the two, or even more exotic concepts (such as observations across the electromagnetic spectrum). In these notes, we will continue to assume the observation windows are time-based.\nThe formal definition of a Poisson process requires some time to unpack. For now, let’s say the following:\nThen we say that \\(N\\) is a Poisson process, that \\(P(N(t) \\le k)\\) has a Poisson CDF with rate \\(\\lambda\\), and that if \\(T\\) is the arrival time of the next event (i.e., \\(T = \\min(t: \\,N(t)=1)\\)) then \\(P(T \\le t)\\) has an exponential distribtuion with rate \\(\\lambda\\).\nA key concept to the Poisson and the Exponential distributions is that they describe the same counting process. The Poisson (discrete) counts the number of events for a fixed window of time, and the Exponential (continuous) counts the time elapsed until the next event.",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exponential distribution</span>"
    ]
  },
  {
    "objectID": "dist_exponential.html#assumptions",
    "href": "dist_exponential.html#assumptions",
    "title": "Appendix C — Exponential distribution",
    "section": "",
    "text": "Let \\(N(t)\\) be a counting process, that is, a non-negative, integer-valued, non-decreasing function with the property that for all \\(t \\ge s\\) we can say that \\(N(t) - N(s)\\) represents the number of events which happen in the window \\([s,t]\\)\nLet \\(\\lambda\\) represent the long-term average rate of events which occur per unit of observation: \\(\\mathbb{E}[N(1)] = \\lambda\\) and \\(\\mathbb{E}[N(t)] = t\\lambda\\)\n\nAs a corollary, notice that \\(N(0) = 0\\), meaning that events must always arrive one at a time, and never two or more simultaneously\n\nLet the event counts in mutually exclusive observation windows be independent of each other, which we could write several ways, e.g. for all \\(t \\ge s\\), then \\(N(t - s) \\perp N(s)\\)",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exponential distribution</span>"
    ]
  },
  {
    "objectID": "dist_exponential.html#definition",
    "href": "dist_exponential.html#definition",
    "title": "Appendix C — Exponential distribution",
    "section": "Definition",
    "text": "Definition\n\\[\\begin{array}{ll}\n  \\text{Support:} & \\mathbb{R}^+ \\\\\n  \\text{Parameter(s):} & \\lambda,\\text{ the rate in events per unit of time }(\\lambda \\gt 0) \\\\\n  \\text{PMF:} & f_X(x) = \\lambda e^{-\\lambda x} \\\\\n  \\text{CDF:} & F_X(x) = \\left\\{\\begin{array}{cl} 0, & \\quad x \\lt 0 \\\\ 1 - \\lambda e^{-\\lambda x}, & \\quad x \\ge 0 \\end{array}\\right\\} \\\\\n  \\text{Mean:} & \\mathbb{E}[X] = \\frac{1}{\\lambda} \\\\\n  \\text{Variance:} & \\mathbb{V}[X] = \\frac{1}{\\lambda^2} \\\\\n\\end{array}\\]",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exponential distribution</span>"
    ]
  },
  {
    "objectID": "dist_exponential.html#visualizer",
    "href": "dist_exponential.html#visualizer",
    "title": "Appendix C — Exponential distribution",
    "section": "Visualizer",
    "text": "Visualizer\nIn the visualizer below, notice that the Exponential distribution is self-similar — it displays a fractal-like property of keeping the same curve for every choice of \\(\\lambda\\). If you do not fix the x-axis, then only the scale of the axes will change, not the shape of the PDF. However, once we fix the x-axis, you’ll better notice how the shape changes along a specific stretch from \\(x=0\\) to \\(x=2\\).\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n      tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Exponential distribution PDF\",\n  fluidRow(plotOutput(\"distPlot\")),\n  fluidRow(column(width=8,sliderInput(\"lambda\", \"Rate (lambda)\", min=0, max=20, step=0.1, value=2)),\n           column(width=4,checkboxInput(\"fax\", \"Fix x-axis\"))))\n\nserver &lt;- function(input, output) {\n  xseq &lt;- reactive(seq(0,list(4/input$lambda,2)[[input$fax+1]],length.out=200))\n  output$distPlot &lt;- renderPlot({\n    plot(x=xseq(),y=dexp(xseq(),input$lambda),\n         xlim=list(NULL,c(0,2))[[input$fax+1]],ylim=NULL,\n         type='l',main=NULL,xlab='x (Arrival time)',ylab='Density',lwd=3)})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exponential distribution</span>"
    ]
  },
  {
    "objectID": "dist_exponential.html#relations-to-other-distributions",
    "href": "dist_exponential.html#relations-to-other-distributions",
    "title": "Appendix C — Exponential distribution",
    "section": "Relations to other distributions",
    "text": "Relations to other distributions\n\nThe Geometric distribution (specifically type (ii) mentioned above which also counts the last success) forms a discrete approximation to the Exponential distribution with \\(\\lambda = p\\), and the quality of this approximation grows for \\(p\\) near 0. For example, compare:\n\n\npgeom(q=20,prob=0.05)\n\n[1] 0.6594384\n\npexp(q=21,rate=0.05)\n\n[1] 0.6500623",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exponential distribution</span>"
    ]
  },
  {
    "objectID": "dist_exponential.html#footnotes",
    "href": "dist_exponential.html#footnotes",
    "title": "Appendix C — Exponential distribution",
    "section": "",
    "text": "Though in real life, neither of these are well-approximated by the Exponential distribution!↩︎",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Exponential distribution</span>"
    ]
  },
  {
    "objectID": "dist_geometric.html",
    "href": "dist_geometric.html",
    "title": "Appendix D — Geometric distribution",
    "section": "",
    "text": "Assumptions\nThe Geometric distribution counts the number of trials needed before the first success (or alternatively the number of successes needed before the first failure).\nDifferent textbooks will use one of two conventions: either (i) the Geometric distribution counts the number of failures before the first success, or (ii) the Geometric distribution counts the number of total trials including both the failures and the last trial ending in success. You should always be sure to stay consistent and not to mix these two cases in your work or when searching for reference material!\nWe will use the first definition on this page, counting only the failures, which is the convention used by R. So if the very first trial ends in success, then we would write \\(X=0\\) failures.\nLet \\(B_1, \\ldots, B_n\\) be a series of \\(n\\) Bernoulli variables each identically and independently distributed with parameter \\(p\\). Let \\(X\\) denote the index of the last failure before the first success, i.e.\n\\[B_{X+1} = 1; \\;B_i = 0 \\; \\forall i \\le X\\]\nThen \\(X \\sim \\mathrm{Geometric}(p)\\).\nTwo key premises of the Geometric distribution are that (i) \\(n\\), the number of trials, is allowed to vary until it reaches a natural stopping point,1 and (ii) the probability of success never changes over time or in response to the previous trials.",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Geometric distribution</span>"
    ]
  },
  {
    "objectID": "dist_geometric.html#definition",
    "href": "dist_geometric.html#definition",
    "title": "Appendix D — Geometric distribution",
    "section": "Definition",
    "text": "Definition\n\\[\\begin{array}{ll}\n  \\text{Support:} & \\{0,1,2,\\ldots,n\\} \\\\\n  \\text{Parameter(s):} & p,\\text{ the probability of success }(p \\in [0,1]) \\\\\n  \\text{PMF:} & P(X=k) = p(1 - p)^k \\\\\n  \\text{CDF:} & F_X(x) = \\left\\{\\begin{array}{cl} 0, & \\quad x \\lt 0 \\\\ 1 - (1-p)^{\\lfloor x \\rfloor + 1}, & \\quad x \\ge 0 \\end{array}\\right\\} \\\\\n  \\text{Mean:} & \\mathbb{E}[X] = \\frac{1-p}{p} \\\\\n  \\text{Variance:} & \\mathbb{V}[X] = \\frac{1-p}{p^2} \\\\\n\\end{array}\\]",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Geometric distribution</span>"
    ]
  },
  {
    "objectID": "dist_geometric.html#visualizer",
    "href": "dist_geometric.html#visualizer",
    "title": "Appendix D — Geometric distribution",
    "section": "Visualizer",
    "text": "Visualizer\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\nlibrary(bslib)\n\nui &lt;- page_fluid(\n      tags$head(tags$style(HTML(\"body {overflow-x: hidden;}\"))),\n  title = \"Geometric distribution PMF\",\n  fluidRow(plotOutput(\"distPlot\")),\n  fluidRow(sliderInput(\"p\", \"Probability (p)\", min=0.01, max=0.99, step=0.01, value=0.5)))\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    plot(x=0:20,y=dgeom(x=0:20,input$p),main=NULL,\n         xlab='x (Prior failures)',ylab='Probability',type='h',lwd=3)})\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Geometric distribution</span>"
    ]
  },
  {
    "objectID": "dist_geometric.html#relations-to-other-distributions",
    "href": "dist_geometric.html#relations-to-other-distributions",
    "title": "Appendix D — Geometric distribution",
    "section": "Relations to other distributions",
    "text": "Relations to other distributions\n\nThe Geometric distribution (specifically type (ii) mentioned above which also counts the last success) forms a discrete approximation to the Exponential distribution with \\(\\lambda = p\\), and the quality of this approximation grows for \\(p\\) near 0. For example, compare:\n\n\npgeom(q=20,prob=0.05)\n\n[1] 0.6594384\n\npexp(q=21,rate=0.05)\n\n[1] 0.6500623",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Geometric distribution</span>"
    ]
  },
  {
    "objectID": "dist_geometric.html#footnotes",
    "href": "dist_geometric.html#footnotes",
    "title": "Appendix D — Geometric distribution",
    "section": "",
    "text": "Unlike, say, the Binomial distribution in which the number of trials are fixed ahead of time↩︎",
    "crumbs": [
      "Appendices",
      "Distributions",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Geometric distribution</span>"
    ]
  },
  {
    "objectID": "robustness.html",
    "href": "robustness.html",
    "title": "Regression robustness",
    "section": "",
    "text": "OLS regression offers impressive analytical tools in a very resource-light framework which can be implemented and precisely replicated in every technology stack, even a simple spreadsheet app. In exchange, OLS regression requires that (1) the response varies linearly with the predictors, and (2) the errors are IID-normally distributed.1 The datasets we study do not often meet these strong assumptions, which can limit the occasions we use OLS or limit our trust in the conclusions.\nHowever, most practitioners agree that OLS regression is impressively “robust” in the face of misspecification or violations of its assumptions, meaning that the model remains useful and reasonably accurate — sometimes! In this section we will discuss how to identify when the data do not meet the assumptions of linear regression, what problems that can cause for our analysis, corrective techniques which might remove or mitigate the problem, and in which situations we may remain comfortable with a moderate degree of misspecification.\nWe will consider five types of misspecification and their effect on our analyses:2",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Regression robustness</span>"
    ]
  },
  {
    "objectID": "robustness.html#footnotes",
    "href": "robustness.html#footnotes",
    "title": "Regression robustness",
    "section": "",
    "text": "There are other assumptions, such as that the model is not overdetermined or that the predictors are not fully collinear, but violations of those assumptions tend to be “immediately fatal” rather than “misleading”.↩︎\nWe will leave a sixth type of misspecification, multicollinearity, for later sections↩︎",
    "crumbs": [
      "4. Regression Robustness",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Regression robustness</span>"
    ]
  }
]