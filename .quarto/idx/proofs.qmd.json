{"title":"Proofs","markdown":{"headingText":"Proofs","headingAttr":{"id":"sec-proofs","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Arithmetic solution for simple OLS regression\n\nWe can readily find the values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ which minimize the residual sum of squares by writing out the RSS function, taking its first partial derivatives, setting them equal to zero, and solving. We require two preliminary results which are purely arithmetic and not a matter of statistical theory:\n\n  * **Finding 1**\n  \n  Note that,\n  \n  $$\\begin{aligned} \\sum_i(x_i - \\bar{x}) x_i &= \\sum_i(x_i^2 - \\bar{x} x_i) = \\sum_i x_i^2 - \\bar{x}\\sum_i x_i \\\\ &= \\sum_i x_i^2 - n\\bar{x}^2\\end{aligned}$$\n  \n  And also that,\n  \n  $$\\begin{aligned} \\sum_i(x_i - \\bar{x})^2 &= \\sum_i(x_i^2 - 2x_i\\bar{x}  + \\bar{x}^2) = \\sum_i x_i^2 - 2\\bar{x}\\sum_i x_i + n\\bar{x}^2 \\\\ &= \\sum_i x_i^2 - n\\bar{x}^2 \\end{aligned}$$\n  And so by transitivity,\n    \n  $$(1) \\quad \\sum_i(x_i - \\bar{x}) x_i = \\sum_i(x_i - \\bar{x})^2$$\n    \n  * **Finding 2**\n  \n  Likewise note that,\n  \n  $$\\begin{aligned} \\sum_i(y_i - \\bar{y}) x_i &= \\sum_i(y_i x_i - \\bar{y} x_i) = \\sum_i y_i x_i - \\bar{y}\\sum_i x_i \\\\ &= \\sum_i y_i x_i - n\\bar{y}\\bar{x} \\end{aligned}$$\n  \n  And also that,\n  \n  $$\\begin{aligned} \\sum_i(y_i - \\bar{y})(x_i - \\bar{x}) &= \\sum_i(y_i x_i - y_i\\bar{x} - \\bar{y}x_i + \\bar{y}\\bar{x}) = \\sum_i y_i x_i - \\bar{x}\\sum_i y_i - \\bar{y}\\sum_i x_i + \\bar{y}\\bar{x} \\\\ &= \\sum_i y_i x_i - n\\bar{y}\\bar{x} \\end{aligned}$$\n  And so by transitivity,\n    \n  $$(2) \\quad \\sum_i(y_i - \\bar{y}) x_i = \\sum_i(y_i - \\bar{y})(x_i - \\bar{x})$$\n  \nNow for the actual Least Squares solution. First, we write out the RSS function which we need to minimize:\n\n$$\\mathrm{RSS} = \\sum_i e_i^2 = \\sum_i (y_i - \\hat{y}_i)^2 = \\sum_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2$$\n \nNext we take the derivative with respect to $\\hat{\\beta}_0$, set it equal to zero, and solve:\n\n$$\\begin{aligned} \\frac{\\delta\\mathrm{RSS}}{\\delta\\hat{\\beta}_0} = 2 \\sum_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)(-1) &= 0 \\\\ \\longrightarrow  \\sum_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) &= 0 \\\\ \\longrightarrow \\sum_i y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1 \\sum_i x_i &= 0 \\\\ \\longrightarrow n\\bar{y} - n\\hat{\\beta}_0 - n\\hat{\\beta}_1 \\bar{x} &= 0 \\\\ \\longrightarrow n\\hat{\\beta}_0 &= n\\bar{y} - n\\hat{\\beta}_1 \\bar{x} \\\\ \\longrightarrow \\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\end{aligned}$$\n̅\nLast we take the derivative with respect to $\\hat{\\beta}_1$, set it equal to zero, and solve as well. Notice that we use the substitution for $\\hat{\\beta}_0$ derived above, and that the final lines rely upon preliminary results (1) and (2):\n\n$$\\begin{aligned} \\frac{\\delta\\mathrm{RSS}}{\\delta\\hat{\\beta}_1} = 2 \\sum_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)(-x_i) &= 0 \\\\ \\longrightarrow \\sum_i (y_i - \\bar{y} + \\hat{\\beta}_1\\bar{x} - \\hat{\\beta}_1x_i)(-x_i) &= 0 \\\\ \\longrightarrow \\sum_i (y_i - \\bar{y})x_i - \\hat{\\beta}_1 \\sum_i (x_i - \\bar{x}) x_i &= 0 \\\\ \\longrightarrow \\hat{\\beta}_1 &= \\frac{\\sum_i (y_i - \\bar{y})x_i}{\\sum_i (x_i - \\bar{x}) x_i} \\\\ \\longrightarrow \\hat{\\beta}_1 &= \\frac{\\sum_i (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_i (x_i - \\bar{x})^2} \\\\ \\longrightarrow \\hat{\\beta}_1 &= \\frac{\\mathrm{Cov}(\\boldsymbol{y},\\boldsymbol{x})}{\\mathbb{V}[\\boldsymbol{x}]} \\end{aligned}$$\n\n## Linear algebraic solution for multiple OLS regression\n\nThe simple linear regression case above conveys the concept of the least squares solution, but readers may wish to see a full solution for the more general case with multiple predictors. This proof will require you to know (or feel your way through) some linear algebra and vector calculus. Let us begin with two findings which help set up the main finding:\n\n  * **Finding 1**\n  \n  $$(1) \\quad (\\boldsymbol{a} - \\boldsymbol{b}) \\circ (\\boldsymbol{a} - \\boldsymbol{b}) = \\boldsymbol{a} \\circ \\boldsymbol{a} + \\boldsymbol{b} \\circ \\boldsymbol{b} - 2\\boldsymbol{a} \\circ \\boldsymbol{b}$$\n  \n  The finding above is really just algebra, but we shall be glad of it soon.  Take note that the bolded letters are vectors, not scalars, though the result matches our scalar expectations.\n  \n  * **Finding 2**\n  \n  $$\\begin{aligned} \\mathrm{RSS}(\\hat{\\boldsymbol{\\beta}}) &= \\sum_i e_i^2 = \\boldsymbol{e} \\circ \\boldsymbol{e} = (\\boldsymbol{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\circ (\\boldsymbol{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\\\ (2) \\quad &= \\boldsymbol{y} \\circ \\boldsymbol{y} + \\hat{\\boldsymbol{\\beta}}^T(\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}} - 2\\boldsymbol{y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\end{aligned}$$\n  \n  This second finding relies on the first, and it establishes that the residual sum of squares for a given choice of betas can be condensed into three summary statistics of the data: $\\boldsymbol{y} \\circ \\boldsymbol{y}$, $\\mathbf{X}^T\\mathbf{X}$, and $\\boldsymbol{y}^T \\mathbf{X}$. You can think of these in turn as functions of the means and variances of $\\boldsymbol{y}$ and $\\mathbf{X}$ along with the covariance of $\\boldsymbol{y}$ and $\\mathbf{X}$, which matches the simple regression case above.\n  \nWith these findings, we proceed to the main event: finding the vector $\\hat{\\boldsymbol{\\beta}}$ which minimizes the residual sum of squares. We take the gradient vector of RSS with respect to $\\hat{\\boldsymbol{\\beta}}$, set equal to zero, and solve:\n\n$$\\begin{aligned} \\nabla \\mathrm{RSS}(\\hat{\\boldsymbol{\\beta}}) = \\frac{\\delta}{\\delta\\hat{\\boldsymbol{\\beta}}}\\left( \\boldsymbol{y} \\circ \\boldsymbol{y} + \\hat{\\boldsymbol{\\beta}}^T(\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}} - 2\\boldsymbol{y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\right) & \\\\ = 2(\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}} - 2\\boldsymbol{y}^T \\mathbf{X} &= 0 \\\\ \\longrightarrow (\\mathbf{X}^T\\mathbf{X}) \\hat{\\boldsymbol{\\beta}} &= \\boldsymbol{y}^T \\mathbf{X} \\\\ \\longrightarrow \\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{y} \\end{aligned}$$\n\nBecause $\\hat{\\boldsymbol{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}=\\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{y}$, we can think of the fitted values $\\hat{\\boldsymbol{y}}$ as a projection of the original values $\\boldsymbol{y}$ onto the model space of $\\mathbf{X}$. The projection matrix for this transformation (sometimes also called the “hat matrix”) would be $\\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T$.\n\n## Proofs about the average fitted value and the average residual\n\nThese proofs were referenced in @sec-variancedecomposition, and rely upon the findings above. The case for simple regression is easily proven; the multiple regression version requires some familiarity with linear algebra.\n\n* **Simple regression case**: To claim that the average fitted value $\\hat{\\boldsymbol{y}}$ is equal to the average response value $\\boldsymbol{y}$ we can note that:\n\n$$\\begin{aligned} \\frac{1}{n}\\sum_i \\hat{y}_i &= \\frac{1}{n}\\sum_i (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i) \\\\ &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\frac{1}{n}\\sum_i x_i \\\\ &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}_i \\\\ &= (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) + \\hat{\\beta}_1 \\bar{x}_i = \\bar{y}\\end{aligned}$$\n\nAnd since $e_i = y_i - \\hat{y}_i$, we can show that the average residual must be 0:\n\n$$\\frac{1}{n}\\sum_i e_i = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i) = \\bar{y} - \\frac{1}{n}\\sum_i \\hat{y}_i = \\bar{y} - \\bar{y} = 0$$\n\nI would be happy to supply a proof for the more general multiple regression case upon student request.","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["shinylive"],"toc":true,"toc-depth":3,"number-sections":false,"output-file":"proofs.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","code-copy":true,"page-layout":"full","smooth-scroll":true,"code-annotations":"hover"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}